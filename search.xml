<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Nginx学习笔记-跨站访问]]></title>
    <url>%2F2018%2F09%2F10%2FNginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E8%B7%A8%E7%AB%99%E8%AE%BF%E9%97%AE%2F</url>
    <content type="text"><![CDATA[Nginx解决跨站访问跨站访问首先了解一下什么是跨域跨域，指的是浏览器不能执行其他网站的脚本。它是由浏览器的同源策略造成的，是浏览器施加的安全限制。 所谓同源是指，域名，协议，端口均相同，不明白没关系，举个栗子： 12345http://www.123.com/index.html 调用 http://www.123.com/server.php （非跨域）http://www.123.com/index.html 调用 http://www.456.com/server.php （主域名不同:123/456，跨域）http://abc.123.com/index.html 调用 http://def.123.com/server.php （子域名不同:abc/def，跨域）http://www.123.com:8080/index.html 调用 http://www.123.com:8081/server.php （端口不同:8080/8081，跨域）http://www.123.com/index.html 调用 https://www.123.com/server.php （协议不同:http/https，跨域） 请注意：localhost和127.0.0.1虽然都指向本机，但也属于跨域。 浏览器执行javascript脚本时，会检查这个脚本属于哪个页面，如果不是同源页面，就不会被执行。 浏览器为了安全，限制了跨域的访问，实际上如果服务端在返回的头信息中添加了 Access-Control-Allow-Origin参数的话，表明服务 端允许跨站访问，那么浏览器则不会在阻止跨域访问了，因此可以巧妙地利用nginx方向代理来实现跨域访问。 案例例如：在开发前后端完全分离的系统中，服务端代码属于一个工程，前端代码属于另一个工程，前端开发人员在进行接口对接时，可能会在webstorm等工具进行编码，并用webstorm的内置服务器进行调试，这就会有跨域问题，因为，webstorm内置服务器默认前缀部分是http://localhost:63342/，而服务端接口的路径前缀部分一定不会是这样，这样便产生了跨域访问的问题。 案例代码例如如下这一段代码，在webstorm中调试这个ajax所在的页面，页面路径是http://localhost:63342/untitled/across-domain.html，而要访问的接口路径是http://localhost:8888/ssm/interfaces/test/m006 123456789101112131415161718192021222324$(function()&#123; $.ajax(&#123; url: "http://localhost:8888/ssm/interfaces/test/m006", type: "POST", async: false, dataType: "json", contentType: "application/json", cache: false, data: JSON.stringify(&#123; "id": "bgdsdgs", "name": "name", "email": "429661318@qq.com", "voModel2": [&#123; "id": "429661318@qq.com", "v3": &#123;"id":"sdfghjkl"&#125; &#125;] &#125;), success: function(data) &#123; $("#text").html(JSON.stringify(data)); &#125;, error: function(text) &#123; $("text").html(JSON.stringify(text)) &#125; &#125;) 通过js请求非本站网址的地址会提示跨域问题，如下内容： Failed to load &lt;跨站地址&gt;: No ‘Access-Control-Allow-Origin’ header is present on the requested resource. Origin ‘&lt;请求地址&gt;’ is therefore not allowed access。 解决方案一反向代理。 123456789location / &#123; root C:\Users\shengmengqi\WebstormProjects\angularJsFrame; index across-domain.html;&#125;# 作用：访问的http://localhost/ssm/interfaces/相当于一个代理url，实际访问的# 是http://localhost:8888/ssm/interfaces/location /ssm/interfaces/&#123; proxy_pass http://localhost:8888/ssm/interfaces/;&#125; 解决方案二添加Access-Control-Allow-Origin头信息。 不做任何限制的跨域请求配置（不建议） 123456789101112131415server &#123; ... ... add_header Access-Control-Allow-Origin *; location / &#123; if ($request_method = 'OPTIONS') &#123; add_header Access-Control-Allow-Origin *; add_header Access-Control-Allow-Methods GET,POST,PUT,DELETE,OPTIONS; return 204; &#125; ... ... &#125;&#125; 上述配置做法，虽然做到了去除跨域请求控制，但是由于对任何请求来源都不做控制，看起来并不安全，所以不建议使用。 指定一个域名白名单跨域请求配置（具有局限性） 123456789101112131415server &#123; ... ... add_header Access-Control-Allow-Origin http://127.0.0.1; location / &#123; if ($request_method = 'OPTIONS') &#123; add_header Access-Control-Allow-Origin http://127.0.0.1; add_header Access-Control-Allow-Methods GET,POST,PUT,DELETE,OPTIONS; return 204; &#125; ... ... &#125;&#125; 上述配置做法，仅局限于 http://127.0.0.1 域名进行跨域访问，假设我使用 http://localhost 请求，尽管都是同一台机器，同一个浏览器，它依旧会提示 No ‘Access-Control-Allow-Origin’ header is present on the requested resource 。 如果没有特殊要求，使用此种方式已经足够。 白名单只能配置一个，不能配置多个。 通过设置变量值解决指定多个域名白名单跨域请求配置（建议使用） 12345678910111213141516171819202122server &#123; ... ... set $cors_origin ""; if ($http_origin ~* "^http://127.0.0.1$") &#123; set $cors_origin $http_origin; &#125; if ($http_origin ~* "^http://localhost$") &#123; set $cors_origin $http_origin; &#125; add_header Access-Control-Allow-Origin $cors_origin; location / &#123; if ($request_method = 'OPTIONS') &#123; add_header Access-Control-Allow-Origin $cors_origin; add_header Access-Control-Allow-Methods GET,POST,PUT,DELETE,OPTIONS; return 204; &#125; ... ... &#125;&#125; 设置一个变量 $cors_origin 来存储需要跨域的白名单，通过正则判断，若是白名单列表，则设置 $cors_origin 值为对应的域名，则做到了多域名跨域白名单功能。 解决方案三从客户端的角度来说，基本上都是用 jsonp 解决，这里就不多做分析了，有需要的朋友可以自行查询资料，本文重点讲究的从服务端角度解决跨域问题。 参考资料:globe_with_meridians:[nginx 指定多个域名跨域请求配置] :globe_with_meridians:[ Nginx作为静态资源web服务之跨域访问] :globe_with_meridians:[jQuery jsonp跨域请求]]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx学习笔记-缓存]]></title>
    <url>%2F2018%2F09%2F10%2FNginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[Nginx缓存浏览器缓存机制web缓存分为很多种，比如数据库缓存、代理服务器缓存、CDN缓存，以及浏览器缓存。浏览器通过代理服务器向源服务器发起请求的原理如图。 浏览器先向代理服务器发起Web请求，再将请求发到源服务器。它属于共享缓存，所以很多地方都可以使用其缓存资源，所以节省了很多流量。 浏览器缓存浏览器缓存是将文件保存在客户端。在同一个回话过程中检查缓存的副本是否足够新，在后退网页时，访问过的资源可以从浏览器缓存中拿出来使用。 浏览器一般都是通过http缓存的，但是也可以通过其他方式，如在html页面中的meta标签增加属性，但不是所有浏览器都支持的，因为有些浏览器不能理解HTML内容本身。 1&lt;meta HTTP-EQUIV="Pragma" CONTENT="no-cache" /&gt; header控制的缓存大多数的浏览器缓存都是有http的header所决定的。 校验机制： 校验头 说明 Expires、Cache-control 校验是否过期。 ETag 校验资源的唯一标识。 Last-Modified 校验资源的最后修改时间。 ExpiresWeb服务器响应消息头字段，在响应http请求时告诉浏览器在过期时间内浏览器可以直接从浏览器缓存读取数据，无需再次请求。但是Expires是HTTP1.0的东西，在HTTP1.1中，他的作用基本上可以忽略。 1Expires: Sun, 09 Sep 2018 14:38:15 GMT Cache-control与Expires的作用是一致的，都是指当前资源的有效期，控制浏览器是直接从浏览器缓存中存取数据，还是重新发送请求到服务器取数据。它的优先级高于Expires,而且它的选择更多，设置也更细致。 Cache-Control 为单向指令，即请求中存在的指令，并不意味着响应中存在同样的指令。 Request Headers中Cache-Control指令： 指令 说明 max-stale= 缓存可随意提供过期文件，如果指定了参数 在这段时间内，文档不能过期。 max-age= 缓存无法返回缓存时间大于秒的文件，这条指令使得缓存更加严格。 no-cache 需要回源验证才能决定是否缓存。 no-store 禁止缓存。 only-if-cached 当缓存中有副本文件存在时，客户端才会获取副本。 Response Headers中Cache-Control指令： 指令 说明 public 可共享缓存（客户端和代理服务器都可以缓存），响应可以被缓存。 private 可私有缓存（客户端能缓存，代理服务器不能缓存）。比如用户私有内容，不能共享。 no-cache 允许缓存者缓存响应。 no-store 禁止缓存。 must-revalidation/proxy-revalidation 如果缓存内容失效，请求必须发送服务器/代理进行验证。 max-age= 缓存内容在s秒后失效。 Last-ModifiedLast-Modified/If-Modified-Since要配合Cache-Control使用。 Last-Modified: 表示这个资源的最后修改时间。Web服务器在响应请求时，告诉浏览器资源的最后修改时间。当浏览器再次进行请求时，会向服务器传送If-Modified-Since包头，询问Last-Modified时间点之后资源是否被修改。 If-Modified-Since: 当资源过期时（用Cache-Control标识的max-age）,发现资源具有Last-Modified声明，则再次向Web服务器请求头带上If-Modified-Since,表示请求时间。Web服务器收到请求后发现有头信息If-Modified-Since，则与被请求资源的最后修改时间进行比较。若最后修改时间比较新，说明资源修改过，则返回相应整片资源内容（卸载响应消息包体内），HTTP Code为200; 若最后修改时间比较旧，说明资源没有更新，则相应的HTTP Code为304，告诉浏览器使用所保存的缓存。 ETagETag/If-None-Match: 也要配合Cache-Control使用。 ETag：web服务器相应请求时，告诉浏览器当前资源在服务器的唯一标识（生成规则由服务器产生）。Apache中，ETag的值，默认是对文件的索引节（INode），大小（Size）和最后修改时间（MTime）进行Hash后得到的。 If-None-Match:当资源过期时（使用Cache-Control标识的max-age），发现资源具有ETag声明，则再次向Web服务器请求时带上头信息If-None-Match（ETag的值）。Web服务器收到请求后，头信息If-None-Cache与被请求的资源进行校验串比较，然后返回200或者304。 你可能觉得Last-Modified已经足够让浏览器知道本地缓存是否是最新的，为什么还是要增加ETag呢？ HTTP1.1中的ETag的出现主要为了解决几个Last-Modified中比较难解决的问题： Last-Modifed标注的最后修改只能精确到秒，如果某些文件在1秒以内，被修改多次的话，它将不能准确标注文件的修改时间 某些文件会被定期生成，当时内容并没有发生任何变化，但Modified却变了，导致文件没法使用缓存。 有可能存在服务器没有准确获取文件修改时间，或者与代理服务器的时间不一致等情形。 ETag是服务器自动生成，或者由开发者对生成的对应资源在服务器端的唯一标识符，能够更加准确的控制缓存。Last-Modified与ETag可以一起使用，服务器会优先验证ETag，一致的情况下，才会继续对Last-Modifed，最后才决定是否返回304。 用户行为 用户操作 Expires/Cache-Control Last_Modified/Etag 地址栏回车 有效 有效 页面链接跳转 有效 有效 新开窗口 有效 有效 前进后退 有效 有效 F5刷新 无效 有效 Ctrl+f5刷新 无效 无效 关于Etag指令的行为： 指令 页面链接跳转 地址栏回车 F5刷新 点击返回按钮 public 缓存页面 缓存页面 重新请求页面 缓存页面 private 重新请求 第一次请求，随后缓存 重新请求页面 缓存页面 no-cache/no-store 重新请求 重新请求 重新请求页面 缓存页面 must-revalidation/proxy-revalidation 浏览器重新请求 第一次请求，随后缓存 重新请求页面 缓存页面 max-age=60s 在60秒后重新请求 在60秒后重新请求 重新请求页面 在60秒后重新请求 缓存流程 Nginx设置缓存Nginx提供了expires、etag、if-modified-since指令来实现浏览器缓存控制。 expires如果我们使用Nginx作为静态资源服务器，那么可以使用expires进行缓存控制。 1234location / &#123; autoindex on; expires 3600s;&#125; 这时会得到类似以下的响应头。 对于静态资源会自动添加ETag。 如果是静态资源，那么Last-Modified值为文件的最后修改时间。 Expires是根据当前服务器系统时间算出来的。 etag对于静态资源会自动添加ETag，可以通过配置etag off指令禁止生成ETag。 1234location / &#123; autoindex on; etag off;&#125; if_modified_since此指令用于指定Nginx如何对服务器端的Last-Moditied时间和浏览器端的if-modified-since时间进行比较。 默认的“if_modified_since exact”表示精准匹配。 也可以使用”if_modified_since _before“表示只要文件的最后修改时间早于或等于浏览器的if-modified-since时间，就返回304。 Nginx代理层缓存使用Nginx作为反向代理时，请求会先进入Nginx，然后Nginx将请求转发给后端应用。 配置如下： 1234567891011# 配置upstreamupstream backend_tomcat &#123; server emmin-1:8080 max_fails=10 fail_timeout=5s weight=1; server emmin-2:8080 max_fails=10 fail_timeout=5s weight=5;&#125;# 配置locationlocation = /cache &#123; proxy_pass http://backend_tomcat/ expires 5s；&#125; 然后再请求相关URL时，过期时间相关响应头被Expires指令更改了，但是last-modified是没有变的。 即使我们更改了缓存过期头，但Nginx自己没有对这些内容做代理层缓存，每次请求还是要到后端验证的。 假设在过期时间内，这些验证在Nginx这一层进行就可以了，不需要到后端验证，这样可以减少后端很大的压力。 具体流程如下： 浏览器发起请求，首先到Nginx，Nginx根据URL在Nginx本地查找是否有代理层本地缓存。 Nginx没有找到本地缓存，则访问后端获取最新的文档，并放入Nginx本地缓存，返回200状态码和最新的文档给浏览器。 Nginx找到本地缓存，首先验证文档是否过期（Cache-Control:mas-age=）。如果过期，则访问后端获取最新的文档，并放入Nginx本地缓存，返回200状态码和最新的文档给浏览器；如果文档没有过期，即if-modified-since与缓存文档的last-moditied匹配，则返回305状态码给浏览器。 内容不需要访问后端，即不需要后端动态计算/渲染等，直接Nginx代理层就把内容返回，速度更快。像Apache Traffic Server、Squid、Varnish、Nginx等技术都可以用来进行内容缓存。 还有CDN技术就是用来加速用户访问的。 用户首先访问全国各地的CDN节点（使用ATS、Squid实现），如果CDN没命中，则会回源到中央Nginx集群，该集群做二级缓存，如果没有命中缓存（该集群的缓存不是必须的，要根据实际命中情况等决定），则最后回源到后端应用集群。 Nginx代理层缓存配置 HTTP模块配置 123456789proxy_buffering on;proxy_buffer_size 4k;proxy_buffers 512 4k;proxy_busy_buffers_size 64k;proxy_cache_path /export/cache/proxy_cache levels=1:2 keys_zone=cache:512m inactive=5m max_size=8g use_temp_path=off;# proxy timeoutproxy_connect_timeout 3s;proxy_read_timeout 5s;proxy_send_timeout 5s; proxy_cache_path指令配置： levels=1:2：表示创建两级目录结构，缓存目录的第一级是1个字符，第二季目录是2个字符，比如/export/cache/proxy_cache/7/3c。如果将所有文件放在一级目录下的话，文件量会很大，会导致文件访问变慢。 keys_zone=cache:512m：设置存储所有缓存key和相关信息的共享内存区，1M大约能存储8000个key。 inactive=5m：inactive指定被缓存的内容多久不被访问将从缓存中移除，以保证内容的新鲜，默认为10分钟。 max_size=8g：最大缓存阈值，”cache manager”进程会监控最大缓存大小，当缓存达到该阈值时，该进程将从缓存中移除最近最少访问的内容。 use_temp_path：如果为on，则内容首先被写入临时文件（proxy_temp_path），然后重命名到proxy_cache_path指令的目录；如果设置为off，则内容直接被写入到proxy_cache_path指令的目录，如果需要cache建议off。 proxy_cache配置 1234567location = /cache &#123; proxy_cache cache; proxy_cache_key $scheme$proxy_host$request_uri; proxy_cache_valid 200 5s; proxy_pass http://backend_tomcat/cache$is_args$args; add_header cache-status $upstream_cache_status;&#125; 缓存相关配置： peoxy_cache：指定使用哪个共享内存区存储缓存信息。 proxy_cache_key：设置缓存使用的key，默认为完整的访问URL，根据实际情况设置缓存key。 proxy_cache_valid：为不同的响应状态码设置缓存时间。如果是proxy_cache_valid 5s，则200、301、302响应都将被缓存。 proxy_cache_valid不是唯一设置缓存时间的，还可以通过以下方式（优先级从上到下）实现。 以秒为单位的“X-Accel-Expires”响应头来设置响应缓存时间。 如果没有“X-Accel-Expires”，则可以根据“Cache-Control”、“Expires”来设置响应缓存时间。 否则，使用proxy_cache_valid设置缓存时间。 如果响应头包含Cache-Control：private/no-cache/no-store、Set-Cookie，或者只有一个Vary响应头并且其值为*，则响应内容将不会被缓存。可以使用proxy_ignore_headers来忽略这些响应头。 add_header cache-status $upstream_cache_status：在响应头中添加缓存命中的状态。 HIT：缓存命中，直接返回缓存中内容，不回源到后端。 MISS：缓存未命中，回源到后端获取最新的内容。 EXPIRED：缓存命中但过期了，回源到后端获取最新的内容。 UPDATING：缓存已过期但正在被别的Nginx Worker进程更新。配置了proxy_cache_use_stale updating指令时会存在该状态。 STALE：缓存已过期，但因后端服务出现了问题（比如后端服务挂了）返回过期的响应。配置了proxy_cache_use_stale updating指令时会存在该状态。 REVALIDATED：启用proxy_cache_revalidate指令后，当缓存内容过期时，Nginx通过一次if-modified-since的请求头去验证缓存内容是否过期，此时会返回该状态。 BYPASS：proxy_cache_bypass指令有效时，强制回源到后端获取内容，即使已经缓存了。 proxy_cache_use_stale：当对缓存的过期时间不敏感，或者后端服务出问题时，即使缓存的内容不新鲜也总比返回错误给用户强（类似于托底），此时可以配置该参数，如“proxy_cache_use_stale error timeout http_500 http_502 http_503 http_504”，即如果出现超时、后端链接出错、500/502/503等错误时，则即使缓存内容已过期也先返回给用户，此时$upstream_cache_status为STALE。 还有一个updating表示缓存已过期但是正在被别的Nginx Worker进程更新，只是返回了过期内容，此时$upstream_cache_status为UPDATING。 proxy_cache_revalidate：当缓存过期后，如果开启了proxy_cache_revalidate，则会发出一次if-modified-since或if-none-match条件请求，如果后端返回304，则此时$upstream_cache_status为REVALIDATED。我们将会得到两个好处，节省带宽和减少写磁盘的次数。 proxy_cache_lock：当多个客户端同事请求同一份内容时，如果开启proxy_cache_lock（默认off），则只有一个请求被发送至后端。其他请求将等待该请求的返回。当第一个请求返回后，其他相同请求将从缓存中获取内容返回。当第一个请求超过了proxy_cache_lock_timeout超时时间（默认为5s），则其他请求将同时请求到后端来获取响应，且响应不会被缓存。启用proxy_cache_lock可以应对Dog_pile effect（当某个缓存失效时，同时有大量相同的请求没命中缓存，而同时请求到后端，从而导致后端压力太大，此时限制一个请求去获取即可）。 proxy_cache_lock_age：如果在proxy_cache_lock_age指定的时间内（默认为5s），最后一个发送到后端进行新缓存构建的请求还没有完成，则下一个请求将被发送到后端来构建缓存（因为1.7.8版本之后，proxy_cache_lock_timeout超时之后返回的内容是不缓存的，需要下一次请求来构建响应缓存）。 清理缓存有时缓存的内容是错误的，需要手工清理。Nginx商业版提供了purger功能，对于社区版Nginx，则可以考虑使用ngx_cache_purge模块进行缓存清理。 12345location ~ /purge(/.*)&#123; allow 127.0.0.1; deny all; proxy_cache_purge cache$1$is_args$args;&#125; 该方法要限制访问权限，如只循序内容可以访问或者需要密码才能访问。 一些经验 只缓存200状态码的响应。像302等，要根据实际场景决定。比如，当系统出错时，自动302到错误页面，此时缓存302就不对了。 有些页面不需要强一致，可以进行几秒的缓存。比如商品详情页展示的库存，可以缓存几秒钟。短时间不一致对于用户来说是没有影响的。 JS/CSS/image等一些内容的缓存时间可以设置为很久，比如1个月甚至1年，通过在页面修改版本来控制过期。 假设商品详情页一步加载的一些数据，使用last-modified进行过期控制，而服务器做了逻辑修改，但内容是没有修改的，即内容的最后修改时间没变。如果想过期这些异步加载的数据，则可以考虑在商品详情页添加异步加载数据的版本号，通过版本号来加载最新的数据，或者将last-modified时间加1来解决，但这种情况下使用ETag是更好的选择。 商品详情页异步加载的一些数据，可以考虑更长时间的缓存，比如1个月而不是几分钟。可以通过MQ将修改时间推送到商品详情页，从而实现按需过期数据。 服务器端考虑使用tmpfs内存文件系统缓存、SSD缓存，使用服务器端负载均衡算法一致性哈希来提升缓存命中率。 缓存key要合理设计。比如，去掉某些参数或排序参数，以保证代理层的缓存命中率；要有清理缓存的工具，出问题时能快速清理掉问题key。 AB测试/个性化需求时，要禁用掉浏览器缓存，但要考虑服务端缓存。 为了便于查找问题，一般会在响应头中添加源服务器信息。如访问京东商品详情页会看到ser响应头，此头存储了源服务器IP，以便出现问题时，知道那台服务器有问题。 参考资料:globe_with_meridians:[浏览器的缓存机制] :book:《亿级流量网站架构核心技术》]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx学习笔记-TCP_NODELAY和TCP_NOPUSH的解释]]></title>
    <url>%2F2018%2F09%2F09%2FNginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-TCP_NODELAY%E5%92%8CTCP_NOPUSH%E7%9A%84%E8%A7%A3%E9%87%8A%2F</url>
    <content type="text"><![CDATA[TCP_NODELAY和TCP_NOPUSH的解释配置项123语法: tcp_nodelay on | off;默认值: tcp_nodelay on;上下文: http, server, location 开启或关闭nginx使用TCP_NODELAY选项的功能。 这个选项仅在将连接转变为长连接（keepalive）的时候才被启用。（译者注，在upstream发送响应到客户端时也会启用）。 123语法: tcp_nopush on | off;默认值: tcp_nopush off;上下文: http, server, location 开启或者关闭nginx在FreeBSD上使用TCP_NOPUSH套接字选项， 在Linux上使用TCP_CORK套接字选项。 选项仅在使用sendfile的时候才开启。 开启此选项允许在Linux和FreeBSD 4.*上将响应头和正文的开始部分一起发送，一次性发送整个文件。只有在开启sendfile后才启用。 tcp_nodelay的功能Nagle和DelayedAcknowledgment的延迟问题。 我们先来看看Nagle和DelayedAcknowledgment的含义： 在网络拥塞控制领域，我们知道有一个非常有名的算法叫做Nagle算法（Nagle algorithm），这是使用它的发明人John Nagle的名字来命名的，John Nagle在1984年首次用这个算法来尝试解决福特汽车公司的网络拥塞问题（RFC 896），该问题的具体描述是：如果我们的应用程序一次产生1个字节的数据，而这个1个字节数据又以网络数据包的形式发送到远端服务器，那么就很容易导致网络由于太多的数据包而过载。比如，当用户使用Telnet连接到远程服务器时，每一次击键操作就会产生1个字节数据，进而发送出去一个数据包，所以，在典型情况下，传送一个只拥有1个字节有效数据的数据包，却要发费40个字节长包头（即ip头20字节+tcp头20字节）的额外开销，这种有效载荷（payload）利用率极其低下的情况被统称之为愚蠢窗口症候群（Silly Window Syndrome）。可以看到，这种情况对于轻负载的网络来说，可能还可以接受，但是对于重负载的网络而言，就极有可能承载不了而轻易的发生拥塞瘫痪。 通俗来说 Nagle： 假如需要频繁的发送一些小包数据，比如说1个字节，以IPv4为例的话，则每个包都要附带40字节的头，也就是说，总计41个字节的数据里，其中只有1个字节是我们需要的数据。 为了解决这个问题，出现了Nagle算法。它规定：如果包的大小满足MSS，那么可以立即发送，否则数据会被放到缓冲区，等到已经发送的包被确认了之后才能继续发送。 通过这样的规定，可以降低网络里小包的数量，从而提升网络性能。 DelayedAcknowledgment： 假如需要单独确认每一个包的话，那么网络中将会充斥着无数的ACK，从而降低了网络性能。 为了解决这个问题，DelayedAcknowledgment规定：不再针对单个包发送ACK，而是一次确认两个包，或者在发送响应数据的同时捎带着发送ACK，又或者触发超时时间后再发送ACK。 通过这样的规定，可以降低网络里ACK的数量，从而提升网络性能。 Nagle和DelayedAcknowledgment是如何影响性能的？ Nagle和DelayedAcknowledgment虽然都是好心，但是它们在一起的时候却会办坏事。 如果一个 TCP 连接的一端启用了 Nagle‘s Algorithm，而另一端启用了 TCP Delayed Ack，而发送的数据包又比较小，则可能会出现这样的情况： 发送端在等待接收端对上一个packet 的 Ack 才发送当前的 packet，而接收端则正好延迟了此 Ack 的发送，那么这个正要被发送的 packet 就会同样被延迟。 当然 Delayed Ack 是有个超时机制的，而默认的超时正好就是 40ms。 现代的 TCP/IP 协议栈实现，默认几乎都启用了这两个功能。按上面的说法，当协议报文很小的时候，岂不每次都会触发这个延迟问题？ 事实不是那样的。仅当协议的交互是发送端连续发送两个 packet，然后立刻 read 的 时候才会出现问题。 现在让我们假设某个应用程序发出了一个请求，希望发送小块数据。我们可以选择立即发送数据或者等待产生更多的数据然后再一次发送两种策略。 如果我们马上发送数据，那么交互性的以及客户/服务器型的应用程序将极大地受益。 例如，当我们正在发送一个较短的请求并且等候较大的响应时，相关过载与传输的数据总量相比就会比较低，而且，如果请求立即发出那么响应时间也会快一些。 以上操作可以通过设置套接字的TCP_NODELAY选项来完成，这样就禁用了Nagle 算法。 另外一种情况则需要我们等到数据量达到最大时才通过网络一次发送全部数据，这种数据传输方式有益于大量数据的通信性能，典型的应用就是文件服务器。应用Nagle算法在这种情况下就会产生问题。但是，如果你正在发送大量数据，你可以设置TCP_CORK选项禁用Nagle化，其方式正好同 TCP_NODELAY相反（TCP_CORK 和 TCP_NODELAY 是互相排斥的）。假设客户端的请求发生需要等待服务端的应答后才能继续发生下一包，即串行执行。 好比在用ab性能测试时只有一个并发做10k的压力测试，测试地址返回的内容只有Hello world；ab发出的request需要等待服务器返回response时，才能发生下一个request； 此时ab只会发生一个get请求，请求的相关内容包含在header中；而服务器需要返回两个数据，一个是response头，另一个是html body； 服务器发送端发送的第一个 write 是不会被缓冲起来，而是立刻发送的（response header）， 这时ab接收端收到对应的数据，但它还期待更多数据（html）才进行处理，所以不会往回发送数据，因此也没机会把 Ack 给带回去，根据Delayed Ack 机制， 这个 Ack 会被 Hold 住。 这时服务器发送端发送第二个包，而队列里还有未确认的数据包（response header），这个 packet（html） 会被缓冲起来。 此时，服务器发送端在等待ab接收端的 Ack；ab接收端则在 Delay 这个 Ack，所以都在等待， 直到ab接收端 Deplayed Ack 超时（40ms），此 Ack 被发送回去，发送端缓冲的这个 packet（html） 才会被真正送到接收端， 此时ab才接受到完整的数据，进行对应的应用层处理，处理完成后才继续发生下一个request，因此服务器端才会在read时出现40ms的阻塞。 tcp_nodelay为什么只在keep-alive才启作用？ TCP中的Nagle算法默认是启用的，但是它并不是适合任何情况，对于telnet或rlogin这样的远程登录应用的确比较适合（原本就是为此而设计），但是在某些应用场景下我们却又需要关闭它。 在Apache对HTTP持久连接（Keep-Alive，Prsistent-Connection）处理时凸现的奇数包&amp;结束小包问题（The Odd/Short-Final-Segment Problem）。 这是一个并的关系，即问题是由于已有奇数个包发出，并且还有一个结束小包（在这里，结束小包并不是指带FIN旗标的包，而是指一个HTTP请求或响应的结束包）等待发出而导致的。 我们来看看具体的问题详情，以3个包+1个结束小包为例，可能发生的发包情况： 服务器向客户端发出两个大包；客户端在接受到两个大包时，必须回复ack； 接着服务器向客户端发送一个中包或小包，但服务器由于Delayed Acknowledgment并没有马上ack； 由于发生队列中有未被ack的包，因此最后一个结束的小包被阻塞等待。 最后一个小包包含了整个响应数据的最后一些数据，所以它是结束小包，如果当前HTTP是非持久连接，那么在连接关闭时，最后这个小包会立即发送出去，这不会出现问题； 但是，如果当前HTTP是持久连接（非pipelining处理，pipelining仅HTTP 1.1支持，nginx目前对pipelining的支持很弱，它必须是前一个请求完全处理完后才能处理后一个请求），即进行连续的Request/Response、Request/Response、…，处理，那么由于最后这个小包受到Nagle算法影响无法及时的发送出去（具体是由于客户端在未结束上一个请求前不会发出新的request数据，导致无法携带ACK而延迟确认，进而导致服务器没收到客户端对上一个小包的的确认导致最后一个小包无法发送出来），导致第n次请求/响应未能结束，从而客户端第n+1次的Request请求数据无法发出。 我的理解是这样的： 在http长连接中，服务器的发生类似于：Write-Write-Read，即返回response header、返回html、读取下一个request。 而在http短连接中，服务器的发生类似于：write-read-write-read，即返回处理结果后，就主动关闭连接，短连接中的close之前的小包会立即发生，不会阻塞。 因为第一个 write 不会被缓冲，会立刻到达接收端，如果是 write-read-write-read 模式，此时接收端应该已经得到所有需要的数据以进行下一步处理。 接收端此时处理完后发送结果，同时也就可以把上一个packet 的 Ack 可以和数据一起发送回去，不需要 delay，从而不会导致任何问题。 我做了一个简单的试验，注释掉了 HTTP Body 的发送，仅仅发送 Headers， Content-Length 指定为 0。 这样就不会有第二个 write，变成了 write-read-write-read 模式。此时再用 ab 测试，果然没有 40ms 的延迟了。 因此在短连接中并不存在小包阻塞的问题，而在长连接中需要做tcp_nodelay开启。 tcp_nopush功能TCP_CORK选项的功能类似于在发送数据管道出口处插入一个“塞子”，使得发送数据全部被阻塞，直到取消TCP_CORK选项（即拔去塞子）或被阻塞数据长度已超过MSS才将其发送出去。 选项TCP_NODELAY是禁用Nagle算法，即数据包立即发送出去，而选项TCP_CORK与此相反，可以认为它是Nagle算法的进一步增强，即阻塞数据包发送。 具体点说就是：TCP_CORK选项的功能类似于在发送数据管道出口处插入一个“塞子”，使得发送数据全部被阻塞， 直到取消TCP_CORK选项（即拔去塞子）或被阻塞数据长度已超过MSS才将其发送出去。 举个对比示例，比如收到接收端的ACK确认后，Nagle算法可以让当前待发送数据包发送出去，即便它的当前长度仍然不够一个MSS，但选项TCP_CORK则会要求继续等待，这在前面的tcp_nagle_check()函数分析时已提到这一点，即如果包数据长度小于当前MSS &amp;&amp;（（加塞 || …）|| …），那么缓存数据而不立即发送。 在TCP_NODELAY模式下，假设有3个小包要发送，第一个小包发出后，接下来的小包需要等待之前的小包被ack，在这期间小包会合并，直到接收到之前包的ack后才会发生； 而在TCP_CORK模式下，第一个小包都不会发生成功，因为包太小，发生管道被阻塞，同一目的地的小包彼此合并后组成一个大于mss的包后，才会被发生。 TCP_CORK选项“堵塞”特性的最终目的无非是为了提高网络利用率，既然反正是要发一个数据包（零窗口探测包）。 如果有实际数据等待发送，那么干脆就直接发送一个负载等待发送数据的数据包岂不是更好？ 我们已经知道，TCP_CORK选项的作用主要是阻塞小数据发送，所以在nginx内的用处就在对响应头的发送处理上。 一般而言，处理一个客户端请求之后的响应数据包括有响应头和响应体两部分，那么利用TCP_CORK选项就能让这两部分数据一起发送。 假设我们需要等到数据量达到最大时才通过网络一次发送全部数据，这种数据传输方式有益于大量数据的通信性能，典型的应用就是文件服务器。 应用Nagle算法在这种情况下就会产生问题。因为TCP_NODELAY在发生小包时不再等待之前的包有没有ack，网络中会存在较多的小包，但这会影响网络的传输能力。 但是，如果你正在发送大量数据，你可以设置TCP_CORK选项禁用Nagle化，其方式正好同 TCP_NODELAY相反（TCP_CORK 和 TCP_NODELAY 是互相排斥的）。 下面就让我们仔细分析下其工作原理。 假设应用程序使用sendfile()函数来转移大量数据。应用协议通常要求发送某些信息来预先解释数据，这些信息其实就是报头内容。 典型情况下报头很小，而且套接字上设置了TCP_NODELAY。有报头的包将被立即传输，在某些情况下（取决于内部的包计数器），因为这个包成功地被对方收到后需要请求对方确认。 这样，大量数据的传输就会被推迟而且产生了不必要的网络流量交换。 但是，如果我们在套接字上设置了TCP_CORK（可以比喻为在管道上插入“塞子”）选项，具有报头的包就会填补大量的数据，所有的数据都根据大小自动地通过包传输出去。 当数据传输完成时，最好取消TCP_CORK 选项设置给连接“拔去塞子”以便任一部分的帧都能发送出去。这同“塞住”网络连接同等重要。 总而言之，如果你肯定能一起发送多个数据集合（例如HTTP响应的头和正文），那么我们建议你设置TCP_CORK选项，这样在这些数据之间不存在延迟。 能极大地有益于WWW、FTP以及文件服务器的性能，同时也简化了你的工作。 为什么tcp_nopush只有在开启sendfile后才启用？ 从技术角度来看，sendfile()是磁盘和传输控制协议（TCP）之间的一种系统呼叫，但是sendfile()还能够用来在两个文件夹之间移动数据。 在各种不同的操作系统上实现sendfile()都会有所不同，当然这种不同只是极为细微的差别。通常来说，我们会假定所使用的操作系统是Linux核心2.4版本。 系统呼叫的原型有如下几种： ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count) in_fd 是一种用来读文件的文件描述符。 out_fd 是一种用来写文件的描述符。 Offset 是一种指向被输入文件变量位置的指针，sendfile()将会从它所指向的位置开始数据的读取。 Count 表示的是两个文件描述符之间数据拷贝的字节数。 sendfile()的威力在于，它为大家提供了一种访问当前不断膨胀的Linux网络堆栈的机制。 这种机制叫做“零拷贝(zero-copy)”,这种机制可以把“传输控制协议（TCP）”框架直接的从主机存储器中传送到网卡的缓存块（network card buffers）中去。 为了更好的理解“零拷贝（zero-copy）”以及sendfile()，让我们回忆一下以前我们在传送文件时所需要执行的那些步骤。 首先，一块在用户机器存储器内用于数据缓冲的位置先被确定了下来。 然后，我们必须使用read()这条系统呼叫来把数据从文件中拷贝到前边已经准备好的那个缓冲区中去。 （在通常的情况下，这个操做会把数据从磁盘上拷贝到操作系统的高速缓冲存储器中去，然后才会把数据从高速缓冲存储器中拷贝至用户空间中去，这种过程就是所谓的“上下文切换”。） 在完成了上述的那些步骤之后，我们得使用write()系统呼叫来将缓冲区中的内容发送到网络上去，程序段如下所示： 12345678910111213intout_fd, intin_fd;char buffer[BUFLEN];…/* unsubstantial code skipped for clarity */…read(in_fd, buffer, BUFLEN); /* syscall, make context switch */write(out_fd, buffer, BUFLEN); /* syscall, make context switch */ 操作系统核心不得不把所有的数据至少都拷贝两次：先是从核心空间到用户空间的拷贝，然后还得再从用户空间拷贝回核心空间。 每一次操做都需要上下文切换（context-switch）的这个步骤，其中包含了许多复杂的高度占用CPU的操作。 系统自带的工具vmstat能够用来在绝大多数UNIX以及与其类似的操作系统上显示当前的“上下文切换（context-switch）”速率。 请看叫做“CS”的那一栏，有相当一部分的上下文切换是发生在取样期间的。用不同类型的方式进行装载可以让使用者清楚的看到使用这些参数进行装载时的不同效果。 在有了sendfile()零拷贝（zero-copy）之后，如果可能的话，通过使用直接存储器访问（Direct Memory Access）的硬件设备，数据从磁盘读取到操作系统高速缓冲存储器中会变得非常之迅速。 而TLB高速缓冲存储器则被完整无缺的放在那里，没有充斥任何有关数据传输的文件。 应用软件在使用sendfile() primitive的时候会有很高的性能表现，这是因为系统呼叫没有直接的指向存储器，因此，就提高了传输数据的性能。 通常来说，要被传输的数据都是从系统缓冲存储器中直接读取的，其间并没有进行上下文切换的操作，也没有垃圾数据占据高速缓冲存储器。 因此，在服务器应用程序中使用sendfile()能够显著的减少对CPU的占用。 TCP/IP网络的数据传输通常建立在数据块的基础之上。从程序员的观点来看，发送数据意味着发出（或者提交）一系列“发送数据块”的请求。 在系统级，发送单个数据块可以通过调用系统函数write() 或者sendfile() 来完成。 因为在网络连接中是由程序员来选择最适当的应用协议，所以网络包的长度和顺序都在程序员的控制之下。同样的，程序员还必须选择这个协议在软件中得以实现的方式。 TCP/IP协议自身已经有了多种可互操作的实现，所以在双方通信时，每一方都有它自身的低级行为，这也是程序员所应该知道的情况。 尽管有许多TCP选项可供程序员操作，而我们却最关注如何处置其中的两个选项，它们是TCP_NODELAY 和 TCP_CORK，这两个选项都对网络连接的行为具有重要的作用。 许多UNIX系统都实现了TCP_NODELAY选项，但是，TCP_CORK则是Linux系统所独有的而且相对较新；它首先在内核版本2.4上得以实现。 此外，其他UNIX系统版本也有功能类似的选项，值得注意的是，在某种由BSD派生的系统上的TCP_NOPUSH选项其实就是TCP_CORK的一部分具体实现。 总结你的数据传输并不需要总是准确地遵守某一选项或者其它选择。在那种情况下，你可能想要采取更为灵活的措施来控制网络连接： 在发送一系列当作单一消息的数据之前设置TCP_CORK，而且在发送应立即发出的短消息之前设置TCP_NODELAY。 如果需要提供网络的传输效率，应该减少小包的传输，使用TCP_CORK来做汇总传输，在利用sendfile来提高效率； 但如果是交互性的业务，那应该让任意小包可以快速传输，关闭Nagle算法，提高包的传输效率。 TCP_CORK优化了传输的bits效率，tcp_nodelay优化了传输的packet效率。]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx学习笔记-配置]]></title>
    <url>%2F2018%2F09%2F07%2FNginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Nginx配置Nginx 基本配置格式基本的Nginx配置文件由若干个部分组成，每一个部分都是通过下列方法定义的。 123&lt;section&gt; &#123; &lt;directive&gt; &lt;parameters&gt;;&#125; 需要注意的是，每一个指定行都由分号结束，这标记着一行的结束。 大括号实际上表示一个新配置的上下文（context），但是在大多数情况下，我们把他们作为“节、部分（section）”来读。 Nginx 全局配置参数全局配置部分被用于配置对整个server都有效的参数。 全局部分由两部分组成：main和events部分。 全局参数示例： 123456789101112user root;worker_processes 1;error_log /var/log/nginx/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;pid /run/nginx.pid;events &#123; worker_connections 1024;&#125; 全局配置重要的配置指令： load_module 123Syntax: load_module file;Default: —Context: main 加载动态模块。 User 123Syntax: user user [group];Default: user nobody nobody;Context: main 使用这个参数来配置worker进程的用户和组。如果忽略group，那么group的名字等于该参数指定用户的用户组。 worker_processes 123Syntax: worker_processes number | auto;Default: worker_processes 1;Context: main 指定worker进程启动的数量。这些进程用于处理客户的所有连接。选择 一个正确的数量取决于服务器环境、磁盘子系统和网络基础设施。 一个好的经验法则是设置该参数的值与 CPU 绑定的负载处理器核心的数量相同，并用 1.5～2 之间的数乘以这个数作为I/O密集型负载。 worker_priority 123Syntax: worker_priority number;Default: worker_priority 0;Context: main 指定工作进程的调度优先级，值越小优先级越大。允许范围在-20到20之间变化。 worker_rlimit_core 123Syntax: worker_rlimit_core size;Default: —Context: main 限制工作进程核心文件（rlimit core）最大大小。 pid 123Syntax: pid file;Default: pid logs/nginx.pid;Context: main 设置记录主进程ID的文件，这个配置将会覆盖编译时的默认配置。 use 123Syntax: use method;Default: —Context: events 该指令用于指示使用什么样的连接方法，这个配置将会覆盖编译时的默认配置。如果配置该指令，那么需要一个events区段。通常不需要覆盖，除非是编译时的默认值随着时间的推移产生错误时才需要被覆盖设置。 worker_connections 123Syntax: worker_connections number;Default: worker_connections 512;Context: events 该指令配置一个工作进程能够接受并发连接的最大数。这个连接包括客户连接和向上游服务器的连接，但不限于此。这对于反向代理服务器尤为重要，为了达到这个并发性连接数量，需要在操作系统层面进行一些额外调整。 daemon 123Syntax: daemon on | off;Default: daemon on;Context: main 守护进程是可以脱离终端并且在后台运行的进程。他脱离是为了避免进程执行过程中的信息在终端打印出来，这样一来，进程也不会被任何终端所产生的信息打断。通常为了调试可以关闭。 error_log 123Syntax: error_log file [level];Default: error_log logs/error.log error;Context: main, http, mail, stream, server, location 所有错误写入的文件。如果在其他区段中没有设置其他的error_log，那么这个日志文件将会记录所有的错误。该指定的第二个参数指定了被记录错误的级别（debug、info、notice、warn、error、crit、alert、emerg）。注意，debug级别的错误只有在编译时配置了–with-debug选项才可以使用。 debug_connection 123Syntax: debug_connection address | CIDR | unix:;Default: —Context: events 为选定的客户端连接启用调试日志模式。其他连接将使用error_log指令设置的日志记录级别 。试连接由IPv4或IPv6地址或网络指定。也可以使用主机名指定连接。对于使用UNIX域套接字的连接，调试日志由“ unix:”参数启用。 123456789events &#123; debug_connection 127.0.0.1; debug_connection localhost; debug_connection 192.0.2.0/24; debug_connection ::1; debug_connection 2001:0db8::/32; debug_connection unix:; ...&#125; debug_points 123Syntax: debug_points abort | stop;Default: —Context: main 这个配置用来帮助用户调试Nginx，他接收2个参数：stop和abort。Nginx在一些关键的错误逻辑中设置了调试点。如果设置为stop，那么Nginx的代码执行到这些调试点时，会发出SIGSTOP信号。如果abort，则会产生一个coredump文件。通常不会用这个配置项。 working_directory 123Syntax: working_directory directory;Default: —Context: main 指定工作进程的当前工作目录，应确保工作进程具有指定目录的写入权限。 Nginx 使用include文件在Nginx配置文件中，include文件可以在任何地方，以便增强配置文件的可读性，并且能够使得部分配置文件重新使用。 使用include文件，要确保被包含的文件自身有正确的Nginx语法，即配置指令和块（block），然后指定这些文件的路径。 12# 在路径中出现通配符，表示可以配置多个文件。include /etc/nginx/conf.d/*.conf; Nginx HTTP的server部分客户端相关指令chunked_transfer_encoding 123Syntax: chunked_transfer_encoding on | off;Default: chunked_transfer_encoding on;Context: http, server, location 在发送给客户端的响应中，该指令允许禁用http/1.1标准的块传输编码。 client_body_buffer_size 123Syntax: client_body_buffer_size size;Default: client_body_buffer_size 8k|16k;Context: http, server, location 为了阻止临时文件写到磁盘，可以通过该指令为客户端请求体设置缓存大小。如果请求主体大于缓冲区，则整个主体或部分写入临时文件。默认的缓存大小为两个内存页面。32位系统为8k，64位系统为16k。 client_body_in_file_only 123Syntax: client_body_in_file_only on | clean | off;Default: client_body_in_file_only off;Context: http, server, location 用于调试或者进一步处理客户端请求体。该指令设置为“no”时能够将客户端请求体强制写入到磁盘文件；设置为“clean”时会删除请求处理后留下的临时文件。 client_body_in_single_buffer 123Syntax: client_body_in_file_only on | clean | off;Default: client_body_in_file_only off;Context: http, server, location 为了减少复制的操作，使用该指令强制Nginx将整个客户端请求体保存在单个缓存区中。使用$request_body 变量时，建议使用该指令 ，以节省所涉及的复制操作数。 client_body_temp_path 123Syntax: client_body_temp_path path [level1 [level2 [level3]]];Default: client_body_temp_path client_body_temp;Context: http, server, location 指定存储客户端请求主体的临时文件的目录。在指定目录下最多可以使用三级子目录层次结构。例如在以下配置中： 1client_body_temp_path /spool/nginx/client_temp 1 2; 临时文件的路径为如下所示： 1/spool/nginx/client_temp/7/45/00000123457 client_body_timeout 123Syntax: client_body_timeout time;Default: client_body_timeout 60s;Context: http, server, location 指定读取客户端请求正文的超时时间。超时仅设置为两个连续读取操作之间的时间段，而不是整个请求主体的传输。如果客户端在此时间内未传输任何内容，请求将以408（请求超时）错误终止。 client_header_buffer_size 123Syntax: client_header_buffer_size size;Default: client_header_buffer_size 1k;Context: http, server 为客户端请求头指定一个缓存大小，当请求头大于1kb时会用到这个设置。 client_header_timeout 123Syntax: client_header_timeout time;Default: client_header_timeout 60s;Context: http, server 读取整个客户端头的超时时间。如果客户端在此时间内未传输整个标头，请求将以408（请求超时）错误终止。 client_max_body_size 123Syntax: client_max_body_size size;Default: client_max_body_size 1m;Context: http, server, location 设置客户端请求正文的最大大小，在“Content-Length”请求头字段中指定。如果请求中的大小超过配置的值，则会将413（请求实体太大）错误返回给客户端。设置size为“0”将禁用检查客户端请求正文大小。 keepalive_disable 123Syntax: keepalive_disable none | browser ...;Default: keepalive_disable msie6;Context: http, server, location 对某些类型的客户端禁用keep-alive请求功能。 keepalive_requests 123Syntax: keepalive_requests number;Default: keepalive_requests 100;Context: http, server, location 指定在一个keep-alive连接关闭之前最多可以接受多少个请求。 keepalive_timeout 123Syntax: keepalive_timeout timeout [header_timeout];Default: keepalive_timeout 75s;Context: http, server, location 指定keep-alive连接超时时间，即活动连接能够持续多久。设置为“0”时禁用keep-alive连接。第二个参数用于在响应头中设置“keepalive”头。 msie_padding 123Syntax: msie_padding on | off;Default: msie_padding on;Context: http, server, location 为了填充响应的大小至512字节。对于MSIE客户端，大于400的状态码会被添加注释以便满足512字节，通过启用该指令可以阻止这种行为。 msie_refresh 123Syntax: msie_refresh on | off;Default: msie_refresh off;Context: http, server, location 对于MSIE客户端，该指令可启用发送一个refresh头，而不是redirect。 文件I/O相关指令aio（异步I/O） 123Syntax: aio on | off | threads[=pool];Default: aio off;Context: http, server, location 该指令启用异步文件I/O。该指令对于现代版本的FreeBSD和所有的Linux发行版都有效。启用aio时会自动启用directio，小于directio定义的大小的文件则采用sendfile进行发送，超过或等于directio定义的大小的文件,将采用aio线程池进行发送。也就是说aio和directio适合大文件下载，因为大文件不适合进入操作系统的buffers/cache，这样会浪费内存，而且Linux AIO(异步磁盘IO)也要求使用directio的形式。 例如： 12345678910111213141516171819202122232425262728293031323334353637383940414243# 启用FreeBSD和Linux上使用异步文件I/O(AIO)。location /video/ &#123; aio on; output_buffers 1 64k;&#125;# 在Linux上，可以从内核版本2.6.22开始使用AIO。此外，需要启用 directio，否则读取将被阻止。location /video/ &#123; aio on; directio 512; output_buffers 1 128k;&#125;# 在Linux上， directio 只能用于读取在512字节边界（或XFS为4K）上对齐的块。# 文件的未对齐端以阻塞模式读取，结尾读取未对齐数据将被阻塞。# 当在Linux上启用AIO和sendfile时，AIO用于大于或等于directio指令中指定大小的文件。# 而sendfile用于小于directio指令中指定大小的文件或禁用directio时。location /video/ &#123; sendfile on; aio on; directio 8m;&#125;# 可以使用多线程读取和发送文件，而不会阻止工作进程。location /video/ &#123; sendfile on; aio threads;&#125;# 还可以指定读取和发送文件操作的线程池。# 如果省略池名称，则使用名为"default"的池。# 池的名称可以使用变量设置。# 多线程支持仅在Linux平台上可用，并且只能与处理请求的epoll，kqueue或eventport方法一起使用。aio threads=pool$disk;thread_pool io_pool threads=16; http&#123; ........ location /data&#123; sendfile on; aio threads=io_pool; &#125; &#125; aio_write 123Syntax: aio_write on | off;Default: aio_write off;Context: http, server, location 如果启用了aio，则该指定指定是否写入文件。目前，这种方法只在使用aio线程时起作用，并且仅限于使用从代理服务器接收到的数据来编写临时文件。 directio（直接I/O） 123Syntax: directio size | off;Default: directio off;Context: http, server, location 操作系统内核通常尝试优化和缓存任何读/写请求。 由于数据在内核中缓存，对同一位置的任何后续读取请求将更快，因为不需要再从磁盘读取信息。直接I/O是文件系统的一个功能，其从应用程序到磁盘直接读取和写入，从而绕过所有操作系统缓存。 这使得更好地利用CPU周期和提高缓存效率。在Linux系统下，使用aio时需要要使用该指令。 directio_alignment 123Syntax: directio_alignment size;Default: directio_alignment 512;Context: http, server, location 指定directio的对其方式。在大多数情况下，512字节对其就足够了。但是在Linux使用XFS文件系统时，需要将其增加到4K。 open_file_cache 1234Syntax: open_file_cache off; open_file_cache max=N [inactive=time];Default: open_file_cache off;Context: http, server, location 该指令用于启用缓存静态文件的元信息。可以缓存信息有： 文件的描述符（fd）、文件大小（size）、最后修改时间（last modified time）。 目录存在的信息（path）。 文件查找错误，例如“找不到文件”、“没有读取权限“等。 应该通过open_file_cache_errors 指令单独启用文件查找错误 。 该指令具有以下参数： max：设置缓存中的最大元素数。 在缓存溢出时，删除最近最少使用（LRU）的元素。 inactive：定义一个时间，如果在此期间未访问该元素，则从该缓存中删除该元素; 默认情况下，它是60秒。 off：禁用缓存。 例如： 1234open_file_cache max=1000 inactive=20s;open_file_cache_valid 30s;open_file_cache_min_uses 2;open_file_cache_errors on; open_file_cache_errors 123Syntax: open_file_cache_errors on | off;Default: open_file_cache_errors off;Context: http, server, location 通过open_file_cache启用或禁用文件查找错误的缓存 。 open_file_cache_min_uses 123Syntax: open_file_cache_min_uses number;Default: open_file_cache_min_uses 1;Context: http, server, location open_file_cache指令中inactive参数时间内文件的最少使用次数。如果在设置的inactive时间少于该指令设置的使用次数，则在缓存内删除。如果超过这个数字，文件更改信息一直是在缓存中打开的。例如： 123# 如果在60s内某文件被访问的次数低于8次，那就将它从缓存中删除。open_file_cache max=64 inactive=60s;open_file_cache_min_uses 8; open_file_cache_valid 123Syntax: open_file_cache_valid time;Default: open_file_cache_valid 60s;Context: http, server, location 验证缓存项有效性的时间间隔。如果缓存中的文件元信息不是最新的，则更新它。 postpone_output 123Syntax: postpone_output size;Default: postpone_output 1460;Context: http, server, location 设置从磁盘读取缓冲区响应的数量和大小。如果可能，客户端数据传输将被延迟，直到达到此值。 read_ahead 123Syntax: read_ahead size;Default: read_ahead 0;Context: http, server, location 设置使用文件时内核的预读取量。在Linux上，使用 posix_fadvise(0, 0, 0, POSIX_FADV_SEQUENTIAL) 系统调用，因此size忽略该参数。 sendfile 123Syntax: sendfile on | off;Default: sendfile off;Context: http, server, location, if in location 当应用程序传输文件时，内核首先缓冲数据，然后将数据发送到应用程序缓冲区。 应用程序反过来将数据发送到目的地。sendfile方法是一种改进的数据传输方法，其中数据在操作系统内核空间内的文件描述符之间复制，而不将数据传输到应用程序缓冲区。 这使操作系统资源的利用率提高。 sendfile_max_chunk 123Syntax: sendfile_max_chunk size;Default: sendfile_max_chunk 0;Context: http, server, location 设置为非“0”值时，限制单个senfile()调用中传输的数据量。该指令令可以减少阻塞调用sendfile()所花费的最长时间，因为NGINX不会尝试一次将整个文件发送出去，而是每次发送大小为size的块数据。 HASH相关指令server_names_hash_max_size 123Syntax: server_names_hash_bucket_size size;Default: server_names_hash_bucket_size 32|64|128;Context: http 设置服务器名称散列表的“桶”大小。默认值取决于处理器缓存行的大小。 server_tokens 123Syntax: server_names_hash_max_size size;Default: server_names_hash_max_size 512;Context: http 设置服务器名称散列表的最大大小。 types_hash_bucket_size 123Syntax: types_hash_bucket_size size;Default: types_hash_bucket_size 64;Context: http, server, location 设置类型散列表的“桶”大小。 types_hash_max_size 123Syntax: types_hash_max_size size;Default: types_hash_max_size 1024;Context: http, server, location 设置类型散列表的最大大小。 variables_hash_bucket_size 123Syntax: variables_hash_bucket_size size;Default: variables_hash_bucket_size 64;Context: http 设置变量散列表的“桶”大小。 variables_hash_max_size 123Syntax: variables_hash_max_size size;Default: variables_hash_max_size 1024;Context: http 设置变量散列表的最大大小。 Socketb相关指令lingering_close 123Syntax: lingering_close off | on | always;Default: lingering_close on;Context: http, server, location 控制nginx如何保持客户端连接。 on：nginx 在完全关闭连接之前等待并处理来自客户端的其他数据。先关闭tcp连接的写再关闭连接的读。直接执行关闭若此时nginx的tcp的write_buffer里有数据没有发送到客户端而read_buffer有数据则调用close()使得nginx直接返回客户端RST响应，丢弃了write_buffer里的数据。若write_buffer里存储的为错误信息，则客户端未收到错误信息连接已经被服务端rst了，则客户端会认为服务端太不讲道理了，不告知错误信息就直接关闭连接了。 always：nginx无条件地等待并处理其他客户端数据。 off：立即关闭连接，此行为会破坏协议，不应在正常情况下使用。 lingering_time 123Syntax: lingering_time time;Default: lingering_time 30s;Context: http, server, location 指定nginx处理（读取和忽略）来自客户端的其他数据的最长时间。之后，即使有更多数据，连接也将关闭。 lingering_timeout 123Syntax: lingering_timeout time;Default: lingering_timeout 5s;Context: http, server, location 指定更多客户端数据到达的最长等待时间。如果在此期间未收到数据，则关闭连接。否则，将读取并忽略已等待的时间，并且nginx会再次开始等待更多数据。重复“wait-read-ignore”循环，但不会超过lingering_time指令指定的循环 。 123Syntax: reset_timedout_connection on | off;Default: reset_timedout_connection off;Context: http, server, location 开启此指令之后，超时的连接将会被立即关闭，释放相关的内存。默认的状态是FIN_WAIT1，这种状态将会一直保持连接。 send_lowat 123Syntax: send_lowat size;Default: send_lowat 0;Context: http, server, location 如果该指令设置为非零值，nginx将尝试通过使用kqueue方法的NOTE_LOWAT标志或套接字选项来最小化客户端套接字上的发送操作数。在Linux，Solaris和Windows上忽略此指令。 send_timeout 123Syntax: send_timeout time;Default: send_timeout 60s;Context: http, server, location 该指令设置两次成功的客户端接收响应的写操作之间的超时时间。仅在两次连续写操作之间设置超时时间，而不是整个响应的传输。如果客户端在此时间内未收到任何内容，则会关闭连接。 tcp_nodelay 123Syntax: tcp_nodelay on | off;Default: tcp_nodelay on;Context: http, server, location 启用或禁用TCP_NODELAY选项，用于keep-alive连接。禁用Nagle算法，即数据包立即发送出去。 在网络拥塞控制领域，有一个非常有名的算法叫做Nagle算法（Nagle algorithm），这是使用它的发明人John Nagle的名字来命名的，John Nagle在1984年首次用这个算法来尝试解决福特汽车公司的网络拥塞问题（RFC 896），该问题的具体描述是：如果我们的应用程序一次产生1个字节的数据，而这个1个字节数据又以网络数据包的形式发送到远端服务器，那么就很容易导致网络由于太多的数据包而过载。比如，当用户使用Telnet连接到远程服务器时，每一次击键操作就会产生1个字节数据，进而发送出去一个数据包，所以，在典型情况下，传送一个只拥有1个字节有效数据的数据包，却要发费40个字节长包头（即ip头20字节+tcp头20字节）的额外开销，这种有效载荷（payload）利用率极其低下的情况被统称之为愚蠢窗口症候群（Silly Window Syndrome）。可以看到，这种情况对于轻负载的网络来说，可能还可以接受，但是对于重负载的网络而言，就极有可能承载不了而轻易的发生拥塞瘫痪。 tcp_nopush 123Syntax: tcp_nopush on | off;Default: tcp_nopush off;Context: http, server, location 仅依赖于senfile的使用。它能够使得Nginx在一个数据包中尝试发送响应头以及在数据包中发送一个完整的文件。tcp_nopush为on时会调用tcp_cork方法，结果就是数据包不会马上传送出去，等到数据包最大时，一次性的传输出去，这样有助于解决网络堵塞。 虚拟服务器相关指令任何由关键字server开始的部分都被称作“虚拟服务器”部分。它描述的是一组根据不同的server_name指令逻辑分割的资源，这些虚拟服务器响应 HTTP 请求，因此它们都包含在http部分中 。 一个虚拟服务器由listen和server_name指令组合定义。 Listen指令1234567891011121314151617Syntax: listen address[:port] [default_server] [ssl] [http2 | spdy] [proxy_protocol] [setfib=number] [fastopen=number] [backlog=number] [rcvbuf=size] [sndbuf=size] [accept_filter=filter] [deferred] [bind] [ipv6only=on|off] [reuseport] [so_keepalive=on|off|[keepidle]:[keepintvl]:[keepcnt]]; listen port [default_server] [ssl] [http2 | spdy] [proxy_protocol] [setfib=number] [fastopen=number] [backlog=number] [rcvbuf=size] [sndbuf=size] [accept_filter=filter] [deferred] [bind] [ipv6only=on|off] [reuseport] [so_keepalive=on|off|[keepidle]:[keepintvl]:[keepcnt]]; listen unix:path [default_server] [ssl] [http2 | spdy] [proxy_protocol] [backlog=number] [rcvbuf=size] [sndbuf=size] [accept_filter=filter] [deferred] [bind] [so_keepalive=on|off|[keepidle]:[keepintvl]:[keepcnt]]; Default: listen *:80 | *:8000;Context: server 参数注解： default_server该参数定义这样一个组合：address:port 默认的请求被绑定在此。 setfib该参数为套接字监听设置响应的FIB。该参数仅支持FreeBSD，不支持UNIX域套接字。 backlog该参数在listen()的调用中设置backlog参数调用。该参数在FreeBSD系统中默认值为-1，在其他系统中为511。 rcvbuf在套接字监听中，该参数设置SO_RCVBUG参数。 sndbuf在套接字监听中，该参数设置SO_SNDBUF参数。 accept_filter该参数设置接受的过滤器：dataready或者httpready。该参数仅支持FreeBSD。 deferred该参数使用延迟的accept()调用设置TCP_DEFER_ACCEPT选项。该参数仅支持Linux。 bind该参数为address:port套接字对打开一个单独的bind()调用。如果任何其他特定套接字参数被使用，name一个单独的bind()将会被隐式的调用。 ipv6only该参数设置IPV6_V6ONLY参数的值。该参数只能在一个全新的开始设置，不支持UNIX域套接字。 ssl该参数表明该端口仅接受HTTPS的连接。该参数允许更紧凑的配置。 http2该参数配置端口接受HTTP/2连接。通常ssl应该指定该参数，但是也可以将nginx配置为接受没有SSL的HTTP/2连接。 spdy该参数允许接受 SPDY这个端口上的连接。通常ssl也应该指定该参数，但是也可以将nginx配置为接受不带SSL的SPDY连接。 proxy_protocol该参数允许指定在这个端口上接受的所有连接都应该使用代理协议。该参数从版本1.13.11起支持PROXY协议版本2。 so_keepalive该参数为TCP监听套接字配置keepalive。 server_name指令123Syntax: server_name name ...;Default: server_name "";Context: server]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GlusterFS学习笔记-错误记录]]></title>
    <url>%2F2018%2F09%2F04%2FGlusterFS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%94%99%E8%AF%AF%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[GlusterFS错误记录重用brick出现的问题错误信息： volume create: datastore: failed: /bricks/brick1/datastore is already part of a volume. 从glusterfs3.3开始 ，有一个新的变化就是会检测这个目录是否已经加入了volume。这就导致了很多gluster支持的问题。 假如你移除了一个volume的brick并且继续使用这个volume，然后重新添加之前的这个brick，这时你的文件状态就会改变。就会造成一系列的问题，它们中的大多数会引起数据丢失。 假如你重新使用一个brick的话，请确保你知道自己在做什么。 解决办法： 123setfattr -x trusted.glusterfs.volume-id $brick_pathsetfattr -x trusted.gfid $brick_pathrm -rf $brick_path/.glusterfs 例子： 123setfattr -x trusted.glusterfs.volume-id /bricks/brick1/datastore/setfattr -x trusted.gfid /bricks/brick1/datastore/rm -f /bricks/brick1/.glusterfs/]]></content>
      <categories>
        <category>GlusterFS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>GlusterFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible学习笔记-变量]]></title>
    <url>%2F2018%2F09%2F04%2FAnsible%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[Ansible变量此笔记主要收集非自定义变量，方便以后用时查询。官网已经很方便了。。。 :globe_with_meridians:playbooks variables Facts变量 自带变量 Facts变量不说多了，需要的话用setup查。 自带变量这部分搭建集群、检查状态、做判断等挺有用的。 hostvars可以在一台主机上去访问另一台主机的变量，包括的另一台主机的Facts变量。 例子： 123456789# hostvars[在主机清单中主机名称][想收集的Facts变量]...- name: hostvars debug: msg: '&#123;&#123; hostvars["192.168.198.22"]["ansible_distribution"] &#125;&#125;'# 如果你主机清单中没有IP地址，但是想取到其中一台主机的IP地址的话。- name: hostvars debug: msg: '&#123;&#123; hostvars["env2"]["ansible_default_ipv4"]["address"] &#125;&#125;' group_names列出当前主机清单中所有组。 可以在jinja2模板中使用，根据主机清单中组成员或者角色来生成不同的模板文件。 例子： 123&#123;% if &apos;webserver&apos; in group_names %&#125; # some part of a configuration file that only applies to webservers&#123;% endif %&#125; groups主机清单中所有组和主机的列表，这个变量比较常用。 例子： 在jinjia2模板中使用For循环配置所有的主机。 123&#123;% for host in groups[&apos;app_server&apos;] %&#125; &#123;&#123; hostvars[host][&apos;ansible_eth0&apos;][&apos;ipv4&apos;][&apos;address&apos;] &#125;&#125;&#123;% endfor %&#125; 查找某个组中所有主机的IP地址。 123&#123;% for host in groups[&apos;app_servers&apos;] %&#125; &#123;&#123; hostvars[host][&apos;ansible_eth0&apos;][&apos;ipv4&apos;][&apos;address&apos;] &#125;&#125;&#123;% endfor %&#125; 我在mongodb脚本模板中用到的例子： 12345678use adminconfig=&#123;_id:&apos;&#123;&#123; mongodb_replication_replset &#125;&#125;&apos;,members:[&#123;&#123;%- for host in groups.mongodb -%&#125;_id:&#123;&#123; loop.index0 &#125;&#125;,host:&apos;&#123;&#123; host &#125;&#125;:&#123;&#123; mongodb_net_port &#125;&#125;&apos;&#123;%- if arbiteronly is defined and arbiteronly -%&#125;,arbiterOnly:true&#123;%- endif -%&#125;&#123;%- if not loop.last -%&#125; &#125;,&#123; &#123;%- endif -%&#125;&#123;%- endfor -%&#125;&#125;]&#125;;rs.initiate(config) inventory_hostname主机清单中配置的主机名的名称。 例子： 123- name: inventory hostname debug: msg: '&#123;&#123; inventory_hostname &#125;&#125;' 返回： 12345678910TASK [inventory hostname] ************************************************************************************ok: [mongodb-node1.emm] =&gt; &#123; &quot;msg&quot;: &quot;mongodb-node1.emm&quot;&#125;ok: [mongodb-node2.emm] =&gt; &#123; &quot;msg&quot;: &quot;mongodb-node2.emm&quot;&#125;ok: [mongodb-node3.emm] =&gt; &#123; &quot;msg&quot;: &quot;mongodb-node3.emm&quot;&#125; 如果主机名称时一个很长的FQDN，可以使用inventory_hostname_short获取第一个句号之前的名称。 例子： 123- name: short inventory hostname debug: msg: '&#123;&#123; inventory_hostname_short &#125;&#125;' 返回： 12345678910TASK [short inventory hostname] ************************************************************************************ok: [mongodb-node1.emm] =&gt; &#123; "msg": "mongodb-node1"&#125;ok: [mongodb-node2.emm] =&gt; &#123; "msg": "mongodb-node2"&#125;ok: [mongodb-node3.emm] =&gt; &#123; "msg": "mongodb-node3"&#125; ansible_play_hosts主机清单中所有活动状态的主机名列表。 123- name: ansible play hosts debug: msg: '&#123;&#123; ansible_play_hosts &#125;&#125;' 返回： 12345678TASK [ansible play hosts] ************************************************************************************ok: [mongodb-node1.emm] =&gt; &#123; &quot;msg&quot;: [ &quot;mongodb-node1.emm&quot;, &quot;mongodb-node2.emm&quot;, &quot;mongodb-node3.emm&quot; ]&#125; ansible_play_batchAnsible 2.2版本之前为play_hosts。 当前Play中批量运行中的主机名列表。 批量大小由serial定义，如果serial未定义则相当于ansible_play_hosts变量。 例子： 不使用serial时： 123- name: ansible play batch debug: msg: '&#123;&#123; ansible_play_batch &#125;&#125;' 返回： 12345678TASK [ansible play batch] ************************************************************************************ok: [mongodb-node1.emm] =&gt; &#123; "msg": [ "mongodb-node1.emm", "mongodb-node2.emm", "mongodb-node3.emm" ]&#125; 使用serial做批量运行时，返回： 123456- hosts: mongodb serial: 1 tasks: - name: ansible play batch debug: msg: '&#123;&#123; ansible_play_batch &#125;&#125;' 返回的是当前批次的主机名列表： 123456TASK [ansible play batch] ************************************************************************************ok: [mongodb-node3.emm] =&gt; &#123; "msg": [ "mongodb-node3.emm" ]&#125; ansible_playbook_python调用Ansible命令行工具的python可执行文件的路径。 inventory_dirAnsible主机清单文件的目录。 inventory_fileAnsible主机清单文件的路径。 playbook_dir执行Playbook的根目录。 role_path角色的路径。 ansible_check_mode如果此变量设置为True，则运行Playbook时会添加--check选项。]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible学习笔记-指令]]></title>
    <url>%2F2018%2F09%2F04%2FAnsible%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Ansible常用指令Play 指令 说明 accelerate 开启加速模式 accelerate_ipv6 是否开启ipv6 accelerate_port 加速模式的端口 always_run any_errors_fatal 有任务错误时，立即停止 become 是否提权 become_flags 提权命令的参数 become_method 提权得方式 become_user 提权的用户 check_mode 当为True时，只检查，不做修改 connection 连接方式 environment 定义远端系统的环境变量 force_handlers 任务失败后，是否依然执行handlers中的任务 gather_facts 是否获取远端系统得facts gather_subset 获取facts得哪些键值 gather_timeout 获取facts的超时时间 handlers 定义task执行完成以后需要调用的任务 hosts 指定运行得主机 ignore_errors 是否忽略错误 max_fail_percentage 最大的错误主机数，超过则立即停止ansbile name 定义任务得名称 no_log 不记录日志 port 定义ssh的连接端口 post_tasks 执行任务后要执行的任务 pre_tasks 执行任务前要执行的任务 remote_user 远程登陆的用户 roles 定义角色 run_once 任务只运行一次 serial 任务每次执行的主机数 strategy play运行的模式 tags 标记标签 tasks 定义任务 vars 定义变量 vars_files 包含变量文件 vars_prompt 要求用户输入内容 vault_password 加密密码 Role 指令 说明 always_run become 是否提权 become_flags 提权命令的参数 become_method 提权的方式 become_user 提权的用户 check_mode 当为True时，只检查，不做修改 connection 连接方式 delegate_facts 委托facts delegate_to 任务委派 environment 定义远端系统的环境变量 ignore_errors 是否忽略错误 no_log 不记录日志 port 定义ssh的连接端口 remote_user 远端系统的执行用户 run_once 只运行一次 tags 标记标签 vars 定义变量 when 条件表达式结果为True则执行block Block 指令 说明 always always里的任务总是执行 always_run any_errors_fatal 有错误时立即中断ansbile become 是否提权 become_flags 提权命令的参数 become_method 提权的方式 become_user 提权的用户 block 分组执行 check_mode 当为True时，只检查，不做修改 connection 连接方式 delegate_facts 委托facts delegate_to 任务委派 environment 定义远端系统的环境变量 ignore_errors 是否忽略错误 no_log 不记录日志 port 定义ssh的连接端口 remote_user 远端系统的执行用户 rescue block中的任务在执行中，如果有任何错误，将执行rescue中的任务。 run_once 只运行一次 tags 标记标签 vars 定义变量 when 条件表达式结果为True则执行block Task 说明 action 执行动作 always_run any_errors_fatal 为True时，只要任务有错误，就立即停止ansible args 定义任务得参数 async 是否异步执行任务 become 是否提权 become_flags 提权命令的参数 become_method 提权的方式 become_user 提权的用户 changed_when 条件表达式为True时，使任务状态为changed check_mode 为True时，只检查运行状态，在远端不做任何修改 connection 连接方式 delay 等待多少秒，才执行任务 delegate_facts 委托facts delegate_to 任务委派 environment 定义远端的环境变量 failed_when 条件表达式为True时，使任务为失败状态 ignore_errors 是否忽略错误 local_action 本地执行 loop loop_args loop_control 改变循环的变量项 name 定义人物的名称 no_log 不记录日志 notify 用于任务执行完，执行handlers里的任务 poll 轮询时间 port 定义ssh的连接端口 register 注册变量 remote_user 远端系统的执行用户 retries 重试次数 run_once 只运行一次 tags 标记为标签 until 直到为真时，才继续执行任务 vars 定义变量 when 条件表达式，结果为True则执行task with_&lt;lookup_plugin&gt; 循环 参考链接:globe_with_meridians:Ansible 小手册系列 十九（常见指令表）]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GlusterFS学习笔记-部署与管理]]></title>
    <url>%2F2018%2F08%2F31%2FGlusterFS%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%83%A8%E7%BD%B2%E4%B8%8E%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[GlusterFS集群搭建简介Glusterfs是一个开源的分布式文件系统。具有强大的横向扩展能力，通过扩展能够支持数PB存储容量和处理数千客户端。 GlusterFS借助TCP/IP或InfiniBand RDMA（一种支持多并发链接的“转换线缆”技术）网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据。 部署Glusterfs属于C/S架构，即客户端、服务端，分别安装。 试验环境： GFS节点 IP 磁盘 gfs-node1 192.168.198.22 sdb:2G gfs-node2 192.168.198.23 sdb:2G gfs-client 192.168.198.21 服务端配置host文件在gfs-node1和gfs-node2做以下操作： 123&gt; vi /etc/hosts192.168.198.22 gfs-node1192.168.198.23 gfs-node2 初始化磁盘在gfs-node1和gfs-node2做以下操作： 每台主机添加一块2G磁盘。 初始化磁盘： 123456789101112# 格式化磁盘&gt; mkfs.xfs -i size=512 /dev/sdb# 创建块目录&gt; mkdir -p /bricks/brick1# 添加设备&gt; vi /etc/fstab/dev/sdb /bricks/brick1 xfs defaults 1 2# 挂载磁盘&gt; mount -a &amp;&amp; mount# 查看磁盘&gt; df -Th/dev/sdb xfs 2.0G 33M 2.0G 2% /bricks/brick1 CentOS 6： 需要安装xfsprogs支持xfs文件系统。 安装启动在gfs-node1和gfs-node2做以下操作： 安装 1234# 使用Storage SIG Yum Repos&gt; yum -y install centos-release-gluster# 安装GlusterFS服务&gt; yum -y install glusterfs-server 12345# 使用epel，同样来自于Storage SIG&gt; yum search centos-release-glustercentos-release-gluster41.noarch : Gluster 4.1 (Long Term Stable) packages from the CentOS Storage SIG&gt; yum -y install centos-release-gluster41.noarch&gt; yum -y install glusterfs-server Storage SIG还为GlsuterFS提供其他生态系统包（例如Samba）： 🌐 Other ecosystem packages for GlusterFS in Storage SIG 启动 12&gt; systemctl start glusterd&gt; systemctl enable glusterd 关闭防火墙在gfs-node1和gfs-node2做以下操作： 12&gt; systemctl stop firewalld&gt; systemctl disable firewalld 配置信任池在gfs-node1或gfs-node2做以下操作： 12345678910111213# 将分布式存储主机加入到信任主机池# gfs-node1主机上操作&gt; gluster peer probe gfs-node2peer probe: success.# 查看信任主机池# 此时是在gfs-node1上查看的，如果在gfs-node2主机上查看会发现gfs-node2也信任了gfs-node1&gt; gluster peer statusNumber of Peers: 1Hostname: gfs-node2Uuid: 6fd442f2-7dbb-4c09-9655-cfab606ad397State: Peer in Cluster (Connected) 创建复制卷在gfs-node1或gfs-node2做以下操作： 1234567891011121314151617181920212223242526272829303132333435363738# 创建复制卷# gfs-node1主机上操作&gt; gluster volume create datastore replica 2 transport tcp gfs-node1:/bricks/brick1 gfs-node2:/bricks/brick1 forcevolume create: datastore: success: please start the volume to access data# 启动卷&gt; gluster volume start datastore volume start: datastore: success# 查看卷信息&gt; gluster volume info datastoreVolume Name: datastoreType: ReplicateVolume ID: 8404aecb-7ead-431d-b163-3bb3e40442d6Status: StartedSnapshot Count: 0Number of Bricks: 1 x 2 = 2Transport-type: tcpBricks:Brick1: gfs-node1:/bricks/brick1Brick2: gfs-node2:/bricks/brick1Options Reconfigured:transport.address-family: inetnfs.disable: onperformance.client-io-threads: off# 查看卷状态&gt; gluster volume status datastoreStatus of volume: datastoreGluster process TCP Port RDMA Port Online Pid------------------------------------------------------------------------------Brick gfs-node1:/bricks/brick1 49152 0 Y 1985 Brick gfs-node2:/bricks/brick1 49152 0 Y 1742 Self-heal Daemon on localhost N/A N/A Y 2008 Self-heal Daemon on gfs-node2 N/A N/A Y 1765 Task Status of Volume datastore------------------------------------------------------------------------------ 客户端配置host文件123&gt; vi /etc/hosts192.168.198.22 gfs-node1192.168.198.23 gfs-node2 关闭防火墙12&gt; systemctl stop firewalld&gt; systemctl disable firewalld 安装挂载 安装 1234# 使用Storage SIG Yum Repos&gt; yum -y install centos-release-gluster# 安装GlusterFS服务&gt; yum -y install glusterfs glusterfs-fuse 挂载 123456789# 手动挂载&gt; mount -t glusterfs gfs-node1:/datastore /opt/test # 自动挂载&gt; vi /etc/fstabgfs-node1:/datastore /opt/test glusterfs defaults,_netdev,log-level=WARNING,log-file=/var/log/gluster.log 0 0&gt; mount -a &amp;&amp; mountgfs-node1:/datastore on /opt/test type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072) 管理节点管理节点状态1gluster peer status 添加节点1gluster peer probe &lt;server&gt; 删除节点1gluster peer detach &lt;server&gt; 卷管理创建卷1gluster volume create [stripe | replica | disperse] [transport tcp | rdma | tcp,rdma] 分布式卷分布式卷（Distributed Glusterfs Volume，又称DHT），默认创建即为分布式卷。 1gluster volume create NEW-VOLNAME [transport tcp | rdma | tcp,rdma] NEW-BRICK... 特点： 根据hash算法，将多个文件分布到卷中的多个brick server上，类似（不是）raid0，但文件无分片。 方便扩展空间，但无冗余保护。 由于使用本地文件系统进行存储（brick server 的本地文件系统），导致以下问题： 存取效率并没有什么没有提高，反而会因为网络通信的原因而有所降低。 支持超大型文件会有一定的难度。虽然ext4已经可以支持最大16T的单个文件，但是本地存储设备的容量实在有限。 所以如果有大量的大文件存储需求，可以考虑使用Stripe模式来实现，如考虑新建专门存储超大型文件的stripe卷。 功能： 分布式卷将数据以哈希算法方式分布到各个brick上，数据是以文件为单位存取，基本达到分布均衡，提供的容量和为各个brick容量的总和。 副本卷副本卷（Replicated Glusterfs Volume）又称AFR（Auto File Replication）。 12# 前提是必须创建信任主机池gluster volume create NEW-VOLNAME [replica COUNT] [transport tcp | rdma | tcp,rdma] NEW-BRICK... 特点： 每个文件同步复制镜像到多个brick，相当于文件级raid1。 副本数量通常设置为2或3，设置的副本数量需要是brick数量（至少为2）的倍数（如2台brick server，可设置副本数为2/4/6/…；如3台brick server，可设置副本数为3/6/9/…；依次类推），且每个brick的容量相等。 读性能提升，写性能下降。因为glusterfs的复制是同步事务操作，即写文件时，先把这个文件锁住，然后同时写两个或多个副本，写完后解锁，操作结束（ceph采用异步写副本，即写到一个主OSD便返回，这个OSD再通过内部网络异步写到其余OSD）。 通常与分布式卷或条带卷组合使用，解决前两者的冗余问题； 提升数据可靠性，但磁盘利用率低； 副本数设置为2时，可能会有脑裂（Split-brain）的风险（风险提示，但可配置），主要因在两个副本不一致时，无法仲裁以哪个副本为准，解决方案是加入仲裁或者设置3副本。 功能： 副本卷提供数据副本，副本数为replica的值，即每个文件存储replica份数，文件不分割。以文件为单位存储，副本数需要等于brick数，当brcik数是副本数的倍数时，则自动转化为replicated-distributed卷。 AFR恢复原理： 数据恢复只针对复制卷，AFR数据修复主要涉及三个方面：ENTRY，META，DATA。 记录描述副本状态的称之为ChangeLog，记录在每个副本文件扩展属性里，读入内存后以矩阵形式判断是否需要修复以及要以哪个副本为Source进行修复；初始值以及正常值为0（注：ENTRY和META,DATA分布对应着一个数值）。 以冗余度为2，即含有2个副本A和B的DATA修复为例，write的步骤分解为： 下发Write操作； 加锁Lock； 向A，B副本的ChangeLog分别加1，记录到各副本的扩展属性中； 对A，B副本进行写操作； 若副本写成功则ChangeLog减1，若该副本写失败则ChangLog值不变，记录到各个副本的扩展属性中； 解锁UnLock； 向上层返回，只要有一个副本写成功就返回成功。 上述操作在AFR中是完整的一个transaction动作，根据两个副本记录的ChangeLog的数值确定了副本的几种状态： WISE：智慧的，即该副本的ChangeLog中对应的值是0，而另一副本对应的数值大于0； INNOCENT：无辜的，即两副本的ChangeLog对应的值都是0； FOOL：愚蠢的，即该副本的ChangeLog对应的值大于是0，而另一副本对应的数值是0； IGNORANT，忽略的，即该副本的ChangeLog丢失。 恢复分以下场景： 1个节点changelog状态为WISE，其余节点为FOOL或其他非WISE状态，以WISE节点去恢复其他节点； 所有节点是IGNORANT状态，手动触发heal，通过命令以UID最小的文件作为source，去恢复大小为0的其他文件； 多个状态是WISE时，即出现脑裂状态，脑裂的文件通常读不出来，报”Input/Output error”，可查看日志/var/log/glusterfs/glustershd.log。 123# 通过命令查看副本文件的扩展属性：getfattr -m . -d -e hex [filename]# “trusted.afr.xxx”部分即扩展属性，值是24bit，分3部分，依次标识DATA ，META， ENTRY 3者的changelog&gt; getfattr -m . -d -e hex /brick1/repl_volume/replica1.txt 副本卷仲裁节点： 1gluster volume create &lt;VOLNAME&gt; replica 3 arbiter 1 host1:brick1 host2:brick2 host3:brick3 脑裂解决方案： :globe_with_meridians:Split brain and the ways to deal with it 条带卷（已弃用）条带卷（Striped Glusterfs Volume）。 1gluster volume create NEW-VOLNAME [stripe COUNT] [transport [tcp | dma | tcp,rdma]] NEW-BRICK... 特点： 每个文件被分片成等同于brick数量的chunks，然后以round robin的方式将每个chunk存储到1个brick，相当于raid0。 单一超大容量文件可被分片，不受brick server本地文件系统的限制。 文件分片后，并发粒度是chunks，分布式读写性能较高，但分片随机读写可能会导致硬盘iops较高。 无冗余，1个server节点故障会导致所有数据丢失。 功能： Stripe卷类似Raid0，将数据条带化，分布在不同的brick，该方式将文件分成stripe块，分别进行存储。在大文件读取时有优势。Stripe需要等于brcik数，当brick数等于stripe数的倍数时，则自动转化为stripe-distributed卷。 分布式副本卷分布式副本卷（Distributed Replicated Glusterfs Volume）是副本卷和分布卷的组合。 12# 必须指定卷类型（默认为分布式卷）与对应的副本数量，brick server数量是副本数量的倍数，且&gt;=2倍gluster volume create NEW-VOLNAME [replica COUNT] [transport [tcp | rdma | tcp,rdma]] NEW-BRICK... 特点： 若干brick组成1个复制卷，另外若干brick组成其他复制卷；单个文件在复制卷内数据保持副本，不同文件在不同复制卷之间进行哈希分布；即分布式卷跨复制卷集（replicated sets ）。 brick server数量是副本数量的倍数，且&gt;=2倍，即最少需要4台brick server，同时组建复制卷集的brick容量相等。 分布式条带卷（已弃用）分布式条带卷（Distributed Striped Glusterfs Volume），是分布式卷与条带卷的组合。 12# 必须指定卷类型（默认为分布式卷）与对应的条带数，brick server数量是条带数量的倍数，且&gt;=2倍gluster volume create NEW-VOLNAME [stripe COUNT] [transport [tcp | rdma | tcp,rdma]] NEW-BRICK... 特点： 若干brick组成1个条带卷，另外若干brick组成其他条带卷；单个文件在条带卷内数据以条带的形式存储，不同文件在不同条带卷之间进行哈希分布；即分布式卷跨条带卷。 brick server数量是条带数的倍数，且&gt;=2倍，即最少需要4台brick server。 条带副本卷（已弃用）条带复制卷（STRIPE REPLICA Volume），是条带与复制卷的组合。 1gluster volume create NEW-VOLNAME [stripe COUNT] [replica COUNT] [transport tcp | rdma | tcp,rdma] NEW-BRICK... 特点： 若干brick组成1个复制卷，另外若干brick组成其他复制卷；单个文件以条带的形式存储在2个或多个复制集（replicated sets ），复制集内文件分片以副本的形式保存；相当于文件级raid01； brick server数量是副本数的倍数，且&gt;=2倍，即最少需要4台brick server。 分布式条带镜像卷（已弃用）分布式条带复制卷（DISTRIBUTE STRIPE REPLICA VOLUME），是分布式卷，条带与复制卷的组合。 特点： 多个文件哈希分布到到多个条带集中，单个文件在条带集中以条带的形式存储在2个或多个复制集（replicated sets ），复制集内文件分片以副本的形式保存；brick server数量是副本数的倍数，且&gt;=2倍，即最少需要4台brick server。 纠删卷纠删卷（Dispersed Volumes）是v3.6版本后发布的一种volume。 1gluster volume create [disperse [&lt;count&gt;]] [redundancy &lt;count&gt;] [transport tcp | rdma | tcp,rdma] NEW-BRICK... 分布式纠删卷分布式纠删卷（Distributed Dispersed Volumes）等效于分布式复制卷，但使用的是纠删子卷，而非复制子卷。 删除卷12345# 删除卷操作，必须先停用卷；# 最后可清空brick server节点对应目录下的内容gluster volume stop distributed-volumegluster volume delete distributed-volumerm -f /brick1/dis_volume 均衡卷12# 每做一次扩容后都需要做一次磁盘平衡。 磁盘平衡是在万不得已的情况下再做的，一般再创建一个卷就可以了gluster volume VOLNAME rebalance [fix-layout start | start | startforce | status | stop] 修复卷12345678# 修复卷（只针对复制卷）# 只修复有问题的文件 gluster volume heal REPLICATE-VOLNAME/DISPERSE-VOLNAME# 修复所有文件gluster volume heal REPLICATE-VOLNAME/DISPERSE-VOLNAME full # 查看自愈详情gluster volume heal REPLICATE-VOLNAME/DISPERSE-VOLNAME info gluster volume heal REPLICATE-VOLNAME/DISPERSE-VOLNAME info healed|heal-failed|split-brain 设置卷1gluster volume set options 常用选项： Auth.allow：IP访问授权，缺省值（*.allow all）。合法值：Ip地址。 Cluster.min-free-disk：剩余磁盘空间阀值，缺省值（10%）。合法值：百分比。 Cluster.stripe-block-size：条带大小，缺省值（128KB）。合法值：字节。 Network.frame-timeout：请求等待时间，缺省值（1800s）。合法值：1-1800。 Network.ping-timeout：客户端等待时间，缺省值（42s）。合法值：0-42。 Nfs.disabled：关闭NFS服务，缺省值（Off）。合法值：Off|on。 Performance.io-thread-count：IO线程数，缺省值（16）。合法值：0-65。 Performance.cache-refresh-timeout：缓存校验时间，缺省值（1s）。合法值：0-61。 Performance.cache-size：读缓存大小，缺省值（32MB）。合法值：字节。 Performance.quick-read：优化读取小文件的性能。 Performance.read-ahead：用预读的方式提高读取的性能，有利于应用频繁持续性的访问文件，当应用完成当前数据块读取的时候，下一个数据块就已经准备好了。 Performance.write-behind：先写入缓存内，在写入硬盘，以提高写入的性能。 Performance.io-cache：缓存已经被读过的。 块管理添加块12# 若是副本卷或者条带化卷，则一次添加的brick数必须为replica或stripe的整数倍gluster volume add-brick VOLNAME NEW-BRICK 移除块12345678910# 若是副本卷或者条带化卷，则移除的brick数必须为replica或stripe的整数倍# 执行移除brick的时候会将数据迁移到其他可用的brick上，当数据迁移结束后才将brick移除# 执行start命令，开始迁移数据，正常移除brick# 执行status可以查看移除任务的状态# 执行commit命令则不不会进行数据迁移而直接删除brick# 扩容时，可以先增加系统节点，然后添加新增节点上的brick即可# 缩容时，先移除brick，再进行节点删除gluster volume remove-brick VOLNAME BRICK start/status/commit 替换块12345# 执行start启动命令后，开始将原始brick的数据迁移到即将需要替换的brick上。# 执行status可查看替换任务是否完成。# 执行abort可终止brick替换。# 执行commitmi命令结束任务进行brick替换。gluster volume replace-brick VOLNAME BRICK NEW-BRICK start/pause/abort/status/commit 参考文献🌐[gluster-Quickstart] :globe_with_meridians:[Setting up GlusterFS Volumes] :globe_with_meridians:[Managing Volumes] :globe_with_meridians:[Setting Up Clients] :globe_with_meridians:[GlusterFS企业级功能之EC纠删码] :globe_with_meridians:[GlusterFS分布式存储集群] :globe_with_meridians:[GlusterFS Dispersed Volume总结] :globe_with_meridians:[glusterfs分布式存储部署]]]></content>
      <categories>
        <category>GlusterFS</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>GlusterFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB学习笔记-配置]]></title>
    <url>%2F2018%2F08%2F29%2FMongoDB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[MongoDB配置文件解析:globe_with_meridians:MongoDB Configuration File Options systemLog （系统日志）系统日志相关选项： 123456789101112131415systemLog: verbosity: &lt;int&gt; quiet: &lt;boolean&gt; traceAllExceptions: &lt;boolean&gt; syslogFacility: &lt;string&gt; path: &lt;string&gt; logAppend: &lt;boolean&gt; logRotate: &lt;string&gt; destination: &lt;string&gt; timeStampFormat: &lt;string&gt; component: accessControl: verbosity: &lt;int&gt; command: verbosity: &lt;int&gt; systemLog.verbosity默认：0 日志冗长级别。 详细程度可以是0到5： 0：MongoDB的默认日志详细级别，包括Informational消息。 1到5：增加详细级别，都包括Debug信息。 日志级别：Log Messages systemLog.quiet运行mongos或者mongod是否处于静默模式，用来限制输出信息。 生产环境不建议使用。 systemLog.traceAllExceptions打印详细信息以进行调试，用来支持相关故障排除的其他日志记录。 systemLog.syslogFacility默认：user 将消息记录到syslog时使用的设备级别。指定的值必须被操作系统的syslog所支持。 要使用此选项，必须设置systemLog.destination为syslog。 systemLog.path日志文件路径。 systemLog.logAppend默认：False True：当实例重新启动时，将日志添加到现有日志文件的末尾。 False：当实例重新启动时，将备份现有日志并创建新文件来记录日志。 systemLog.logRotate默认：rename 使用日志分割时的行为： rename：重命名日志文件。 reopen：按照Linux/Unix日志轮换行为关闭并重新打开日志文件。如果指定reopen，则还必须设置systemLog.logAppend为true。 systemLog.destination指定日志类型。 file：日志为一个文件，如果指定file，则还必须指定systemLog.path。 syslog：记录到系统日志。 未指定：所有日志输出为标准输出。 systemLog.timeStampFormat默认：iso8601-local 日志消息中时间戳的时间格式。指定以下值之一： ctime：将时间戳显示为Wed Dec 31 18:17:54.811。 iso8601-utc：以ISO-8601格式显示协调世界时（UTC）的时间戳。例如，在大纪元开始的纽约：1970-01-01T00:00:00.000Z。 iso8601-local：以ISO-8601格式显示本地时间的时间戳。例如，在大纪元开始的纽约： 1969-12-31T19:00:00.000-0500。 systemLog.component相关组件的日志级别。 processManagement （进程管理）1234processManagement: fork: &lt;boolean&gt; pidFilePath: &lt;string&gt; timeZoneInfo: &lt;string&gt; processManagement.fork默认：False True：以守护进程运行mongos或mongod。 False：不以守护进程运行mongos或mongod。 processManagement.pidFilePathPid文件路径。配合processManagement.fork为True一起使用。 如果不指定，将不会创建PID文件。 processManagement.timeZoneInfoLinux和MacOS系统默认：/usr/share/zoneinfo 时区数据库的完整路径。如果未提供此选项，则MongoDB将使用其内置时区数据库。 net （网络）1234567891011121314151617181920212223242526272829net: port: &lt;int&gt; bindIp: &lt;string&gt; bindIpAll: &lt;boolean&gt; maxIncomingConnections: &lt;int&gt; wireObjectCheck: &lt;boolean&gt; ipv6: &lt;boolean&gt; unixDomainSocket: enabled: &lt;boolean&gt; pathPrefix: &lt;string&gt; filePermissions: &lt;int&gt; ssl: sslOnNormalPorts: &lt;boolean&gt; # deprecated since 2.6 mode: &lt;string&gt; PEMKeyFile: &lt;string&gt; PEMKeyPassword: &lt;string&gt; clusterFile: &lt;string&gt; clusterPassword: &lt;string&gt; CAFile: &lt;string&gt; CRLFile: &lt;string&gt; allowConnectionsWithoutCertificates: &lt;boolean&gt; allowInvalidCertificates: &lt;boolean&gt; allowInvalidHostnames: &lt;boolean&gt; disabledProtocols: &lt;string&gt; FIPSMode: &lt;boolean&gt; compression: compressors: &lt;string&gt; transportLayer: &lt;string&gt; serviceExecutor: &lt;string&gt; net.port默认：27017 端口号。 net.bindIp默认：localhost 绑定网络接口。要绑定到多个地址，请输入逗号分隔值列表。 1bindIp: 127.0.0.1,/tmp/mongod.sock net.bindIpAll默认：False True：绑定到所有的IP地址。 等同于bindIp: 0.0.0.0,:: 注意： net.bindIp和net.bindIpAll是相互排斥的。也就是说，可以指定其中一个，但不能同时指定两者。 net.maxIncomingConnections默认：65536 mongod/mongos进程允许的最大连接数，如果此值超过操作系统配置的连接数阀值，将不会生效(ulimit)；默认值为65536。通常客户端将会使用连接池机制，可以有效的控制每个客户端的链接个数。 net.wireObjectCheck默认：True True：当客户端写入数据时，mongos/mongod是否检测数据的有效性(BSON)，如果数据格式不良，此insert、update操作将会被拒绝。会稍微影响性能。 False：不验证有效性。 net.ipv6默认：False 启用或禁用ipv6支持，此值需要在整个cluster中保持一致。 net.unixDomainSocket12345net: unixDomainSocket: enabled: &lt;boolean&gt; pathPrefix: &lt;string&gt; filePermissions: &lt;int&gt; net.unixDomainSocket.enabled默认：True 启用或禁用UNIX socket监听。 出现以下任意一种情况不会启用UNIX domain socket监听： net.unixDomainSocket.enabled为false。 启动时添加--nounixsocket。命令行选项优先于配置文件设置。 net.bindIp 没有设置。 net.bindIp 没有设置为127.0.0.1。 net.unixDomainSocket.pathPrefix默认：/tmp Socket文件路径。 net.unixDomainSocket.filePermissions默认：0700 Socket文件权限。 net.ssl123456789101112131415net: ssl: sslOnNormalPorts: &lt;boolean&gt; # deprecated since 2.6 mode: &lt;string&gt; PEMKeyFile: &lt;string&gt; PEMKeyPassword: &lt;string&gt; clusterFile: &lt;string&gt; clusterPassword: &lt;string&gt; CAFile: &lt;string&gt; CRLFile: &lt;string&gt; allowConnectionsWithoutCertificates: &lt;boolean&gt; allowInvalidCertificates: &lt;boolean&gt; allowInvalidHostnames: &lt;boolean&gt; disabledProtocols: &lt;string&gt; FIPSMode: &lt;boolean&gt; 配置TLS/SSL参考： 🌐Configure mongod and mongos for TLS/SSL :globe_with_meridians:TLS/SSL Configuration for Clients net.ssl.sslOnNormalPorts从2.6版开始不推荐使用：改为使用net.ssl.mode。 net.ssl.mode启用或禁用用于所有网络连接的TLS/SSL或混合TLS/SSL。 disabled：服务器不使用TLS/SSL。 allowSSL：服务器之间的连接不使用TLS/SSL。对于传入连接，服务器接受TLS/SSL和非TLS/非SSL。 preferSSL：服务器之间的连接使用TLS/SSL。对于传入连接，服务器接受TLS/SSL和非TLS/非SSL。 requireSSL：服务器仅使用和接受TLS/SSL加密连接。 从版本3.4开始，如果--sslCAFile或未ssl.CAFile指定且未使用x.509身份验证，则在连接到启用TLS / SSL的服务器时将使用系统范围的CA证书存储。 如果使用x.509身份验证，--sslCAFile或ssl.CAFile 必须指定。 net.ssl.PEMKeyFilepem文件名。 net.ssl.PEMKeyPasswordpem文件密码。 net.ssl.clusterFilecluster文件名。 net.ssl.clusterPasswordcluster文件密码。 net.ssl.CAFileCA文件名。 net.ssl.CRLFileCRL文件名。 net.compression123net: compression: compressors: &lt;string&gt; net.compression.compressors默认：snappy 网络通信压缩。 snappy zlib 当双方都启用网络压缩时，将压缩消息。否则，各方之间的消息将被解压缩。 net.transportLayer默认值：asio 除非MongoDB支持团队要求您更改此设置，否则请勿更改此设置。 net.serviceExecutormongos/mongod用于执行客户端请求的线程模型。 synchronous adaptive 其他选项以后再记录。]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB学习笔记-副本集]]></title>
    <url>%2F2018%2F08%2F28%2FMongoDB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%89%AF%E6%9C%AC%E9%9B%86%2F</url>
    <content type="text"><![CDATA[MongoDB副本集基本概念MongoDB的replica set是一个mongod进程实例簇，数据在这个簇中相互复制，并自动进行故障切换。 MongoDB的数据库复制增加了冗余，确保了高可用性，简化了管理任务如备份，并且增加了读能力。大多数产品部署都使用了复制。MongoDB中primary处理写操作，其它进行复制的成员则是secondaries。 一个副本集可以最多支持12个成员，但是只有7个成员可以参与投票。 注：MongoDB同时提供了传统的master/slave复制，这种复制的操作方法与副本集相同，但是master/slave复制不支持自动故障切换。很容易理解，主备模式下，cli端是指定了地址和端口进行mongodb的访问的，而副本集模式则是通过访问mongos来隐藏动态切换的。 成员配置成员可以是以下某种角色： 成为primary 对客户端可见 参与投票 延迟同步 复制数据 Default √ √ √ ∕ √ Secondary-Only ∕ √ √ ∕ √ Hidden ∕ ∕ √ ∕ √ Delayed ∕ √ √ √ √ Arbiters ∕ ∕ √ ∕ ∕ Non-Voting √ √ ∕ ∕ √ Secondary-Only:这种类型的节点和secondary节点一样拥有数据副本，但是它们在任何情形下都成为不了primary节点。 Hidden:这种类型的节点对客户端程序来说是不可见的，同样也不能成为primary节点，但是Hidden成员能够参与选举投票。 Delayed：这种类型的成员通过人为的设置，可以指定一个时间来延迟从primary节点同步数据。Delayed成员的作用在于帮助集群从一些误操作中恢复，比如管理员误删除了某个集合。不至于迅速扩散到整个集群中。因此Delayed节点必须不能成为primary节点（priority为0）并且是Hidden的。 Non-Voting：这就是上面提到了没有选举权的secondary节点。这种类型的节点一般当集群节点数超过12才会需要。 Arbiters：仲裁者只参与投票不拥有实际的数据，因此它对物理资源要求不严格。 故障切换副本集能够自动进行故障切换恢复。如果primary掉线或者无反应且多数的副本集成员能够相互连接，则选出一个新的primary。 在多数情况下，当primary宕机、不可用或者是不适合做primary时，在没有管理者干预的几秒后会进行故障切换。 如果MongoDB部署没有如预期那样进行故障切换，则可能是下面的问题： 剩余的成员个数少于副本集的一半 没有成员有资格成为primary 数据回滚多数情况下，回滚操作可以优雅的对不能进行故障切换恢复的情况进行恢复。 Rollbacks操作发生在primary处理写操作，但其它成员没有成功的进行复制之前primary掉线时。当先前的primary开始复制时，则表现出rollback。如果操作复制到其它成员，该成员可用，并且可以和大多数的副本集连接，则没有rollback。 Rollbacks删除了那些没有进行复制的操作，以保证数据集的一致性。 投票选举当任意的故障切换发生，都会伴随着一个选举的出现，以此来决定哪个成员成为primary。 选举提供了一种机制，用于副本集中的成员无需管理员的干预，自动的选出一个新的primary。选举可以让副本集快速和坚决的从故障中恢复。 当primary变为不可达时，secondary成员发起选举，第一个收到大多数选票的成员成为新的primary。 成员优先级在副本集中，每个成员都有优先级，它可以帮助决定选举出primary。默认情况下，所有的成员的优先级都为1。 一致性在MongoDB中，所有针对于primary的读操作都与最后的写操作结果相一致。 如果客户端配置了读选项以允许secondary读，读操作能从没有近期复制更新或操作的secondary成员返回结果。在这种情况下，查询操作可能返回之前的状态。 这种行为有时称为最终一致性，因为secodary成员的状态最终都会是primary的状态。MongoDB不能保证从secondary成员的读操作的强一致性。 没有办法保证从secondary成员读的一致性，除非在配置时保证写操作成功的在所有节点上都执行成功后才返回成功。 部署架构架构不存在一个理想的副本集架构可以满足任意部署环境。 最小的副本集推荐架构为三成员集合，其中一个为primary，另外两个为secondary，secondary在一定情况下可以成为primary。 如果副本集中的成员多于三个，则需要遵照下面的架构条件： 集合中有奇数个参与投票的成员。如果有偶数个投票成员，则部署一个仲裁者将个数变为奇数。 集合中同一时刻不多于7个参与投票的成员 如果不想让某些成员在故障切换时成为primary，则将它们的优先级设为0。 集合的多数成员运行在主要的数据中心 地理上的分布式集一个基于地理的分布式副本集可以应对一个数据中心恢复失败的情况。这种集合至少包含了一个在备份数据中心的集合成员。 图：基于地理上的分布式副本集 如果primary掉线，则副本集选出一个新的primary；如果主数据中心和备数据中心连接失败，备数据中心的secondary不能成为primary。如果主数据中心失败，则需要人为的从备数据中心恢复数据。 值得注意的是，这种架构下，必须注意在主数据中心要保持奇数个参与投票的成员，上图中则需要在主数据中心添加一个仲裁者。 非生产者成员在有些时候，我们可能想有一个成员能够拷贝整个的数据集，但并不使其成为primary。这种成员可以作为备份、支持报告操作或者作为一个冷备。这种成员被分为以下几种： 低优先级：通过local.system.replset.members[n].priority设置成员的低优先级，从而使其不可能成为primary。 隐藏（Hidden）：这种成员不可能primary，对客户端不可见。 投票（Voting）：这会改变进行选举的副本集的票数。 副本集配置12345replication: oplogSizeMB: &lt;int&gt; replSetName: &lt;string&gt; secondaryIndexPrefetch: &lt;string&gt; enableMajorityReadConcern: &lt;boolean&gt; ## Deprecated in 3.6 replication.oplogSizeMB：oplog最大值。 默认情况下，该mongod进程基于可用的最大空间量创建oplog。对于64位系统，oplog通常占可用磁盘空间的5％。 一旦mongod第一次创建了oplog，更改replication.oplogSizeMB选项将不会影响oplog的大小。 要更改正在运行的副本集成员的oplog大小，请使用replSetResizeOplog管理命令。replSetResizeOplog可以动态调整oplog的大小，而无需重新启动mongod。 replication.replSetName：副本集名称。 如果应用程序连接到多个副本集，则每个副本集都应具有不同的名称。某些驱动程序按副本集名称对副本集进行连接。 replication.secondaryIndexPrefetch：副本集成员在接受oplog之前是否加载索引到内存。默认情况下，secondary会将所有索引都加载到内存中。 选项有： none：secondary节点不会将索引加载到内存中。 all：加载所有索引到内存中。 _id_only：值加载_id索引到内存中。 副本集实例环境三台服务器： 123192.168.198.21192.168.198.22192.168.198.23 配置文件三台服务器添加相同的replication配置： 1234567# /etc/mongod.confnet: port: 27017 bindIp: 192.168.198.21 # 根据主机IP更改，或者使用0.0.0.0replication: replSetName: mongo.test 重启mongod： 1systemctl restart mongod 初始化副本集在任意一台上连接mongodb，初始化： 123# mongo&gt; use admin&gt; rs.initiate() 12345&#123; &quot;info2&quot; : &quot;no configuration specified. Using a default configuration for the set&quot;, &quot;me&quot; : &quot;192.168.198.21:27017&quot;, &quot;ok&quot; : 1&#125; 确认配置： 1&gt; rs.conf() 1234567891011121314151617181920212223242526272829303132333435&#123; &quot;_id&quot; : &quot;mongo.test&quot;, &quot;version&quot; : 1, &quot;protocolVersion&quot; : NumberLong(1), &quot;members&quot; : [ &#123; &quot;_id&quot; : 0, &quot;host&quot; : &quot;192.168.198.21:27017&quot;, &quot;arbiterOnly&quot; : false, &quot;buildIndexes&quot; : true, &quot;hidden&quot; : false, &quot;priority&quot; : 1, &quot;tags&quot; : &#123; &#125;, &quot;slaveDelay&quot; : NumberLong(0), &quot;votes&quot; : 1 &#125; ], &quot;settings&quot; : &#123; &quot;chainingAllowed&quot; : true, &quot;heartbeatIntervalMillis&quot; : 2000, &quot;heartbeatTimeoutSecs&quot; : 10, &quot;electionTimeoutMillis&quot; : 10000, &quot;catchUpTimeoutMillis&quot; : 60000, &quot;getLastErrorModes&quot; : &#123; &#125;, &quot;getLastErrorDefaults&quot; : &#123; &quot;w&quot; : 1, &quot;wtimeout&quot; : 0 &#125;, &quot;replicaSetId&quot; : ObjectId(&quot;5b864c759be9fb9508a68f6e&quot;) &#125;&#125; 添加成员12&gt; rs.add(&#123;host: "192.168.198.22:27017"&#125;)&gt; rs.add(&#123;host: "192.168.198.23:27017", priority: 5&#125;) # 优先级大的优先被选取为主库 查看副本集状态1&gt; rs.status() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103&#123; &quot;set&quot; : &quot;mongo.test&quot;, &quot;date&quot; : ISODate(&quot;2018-08-29T07:47:42.016Z&quot;), &quot;myState&quot; : 2, &quot;term&quot; : NumberLong(2), &quot;syncingTo&quot; : &quot;192.168.198.23:27017&quot;, &quot;syncSourceHost&quot; : &quot;192.168.198.23:27017&quot;, &quot;syncSourceId&quot; : 2, &quot;heartbeatIntervalMillis&quot; : NumberLong(2000), &quot;optimes&quot; : &#123; &quot;lastCommittedOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1535528853, 1), &quot;t&quot; : NumberLong(2) &#125;, &quot;appliedOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1535528853, 1), &quot;t&quot; : NumberLong(2) &#125;, &quot;durableOpTime&quot; : &#123; &quot;ts&quot; : Timestamp(1535528853, 1), &quot;t&quot; : NumberLong(2) &#125; &#125;, &quot;members&quot; : [ &#123; &quot;_id&quot; : 0, &quot;name&quot; : &quot;192.168.198.21:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 844, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(1535528853, 1), &quot;t&quot; : NumberLong(2) &#125;, &quot;optimeDate&quot; : ISODate(&quot;2018-08-29T07:47:33Z&quot;), &quot;syncingTo&quot; : &quot;192.168.198.23:27017&quot;, &quot;syncSourceHost&quot; : &quot;192.168.198.23:27017&quot;, &quot;syncSourceId&quot; : 2, &quot;infoMessage&quot; : &quot;&quot;, &quot;configVersion&quot; : 3, &quot;self&quot; : true, &quot;lastHeartbeatMessage&quot; : &quot;&quot; &#125;, &#123; &quot;_id&quot; : 1, &quot;name&quot; : &quot;192.168.198.22:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 2, &quot;stateStr&quot; : &quot;SECONDARY&quot;, &quot;uptime&quot; : 577, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(1535528853, 1), &quot;t&quot; : NumberLong(2) &#125;, &quot;optimeDurable&quot; : &#123; &quot;ts&quot; : Timestamp(1535528853, 1), &quot;t&quot; : NumberLong(2) &#125;, &quot;optimeDate&quot; : ISODate(&quot;2018-08-29T07:47:33Z&quot;), &quot;optimeDurableDate&quot; : ISODate(&quot;2018-08-29T07:47:33Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2018-08-29T07:47:41.366Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2018-08-29T07:47:40.250Z&quot;), &quot;pingMs&quot; : NumberLong(0), &quot;lastHeartbeatMessage&quot; : &quot;&quot;, &quot;syncingTo&quot; : &quot;192.168.198.23:27017&quot;, &quot;syncSourceHost&quot; : &quot;192.168.198.23:27017&quot;, &quot;syncSourceId&quot; : 2, &quot;infoMessage&quot; : &quot;&quot;, &quot;configVersion&quot; : 3 &#125;, &#123; &quot;_id&quot; : 2, &quot;name&quot; : &quot;192.168.198.23:27017&quot;, &quot;health&quot; : 1, &quot;state&quot; : 1, &quot;stateStr&quot; : &quot;PRIMARY&quot;, &quot;uptime&quot; : 571, &quot;optime&quot; : &#123; &quot;ts&quot; : Timestamp(1535528853, 1), &quot;t&quot; : NumberLong(2) &#125;, &quot;optimeDurable&quot; : &#123; &quot;ts&quot; : Timestamp(1535528853, 1), &quot;t&quot; : NumberLong(2) &#125;, &quot;optimeDate&quot; : ISODate(&quot;2018-08-29T07:47:33Z&quot;), &quot;optimeDurableDate&quot; : ISODate(&quot;2018-08-29T07:47:33Z&quot;), &quot;lastHeartbeat&quot; : ISODate(&quot;2018-08-29T07:47:41.366Z&quot;), &quot;lastHeartbeatRecv&quot; : ISODate(&quot;2018-08-29T07:47:40.128Z&quot;), &quot;pingMs&quot; : NumberLong(0), &quot;lastHeartbeatMessage&quot; : &quot;&quot;, &quot;syncingTo&quot; : &quot;&quot;, &quot;syncSourceHost&quot; : &quot;&quot;, &quot;syncSourceId&quot; : -1, &quot;infoMessage&quot; : &quot;&quot;, &quot;electionTime&quot; : Timestamp(1535528302, 1), &quot;electionDate&quot; : ISODate(&quot;2018-08-29T07:38:22Z&quot;), &quot;configVersion&quot; : 3 &#125; ], &quot;ok&quot; : 1&#125; 副本集优化读写分离官网中建议不使用向从节点取数据。原因： 所有的从节点拥有与主节点一样的写入负载，读的加入会增加其负载 对于分片的集合，在平衡器的关系下，数据的返回结果可能会缺失或者重复某部分数据。 相对而言，官方建议使用分片（shard）来分散读写请求。 使用的场景； 操作不影响前端应用程序，比如备份或者报表； 在一个物理上分布的副本集群中，为了减少应用程序的延迟，可能会优先选择离应用程序更近的secondary节点而不是远在千里之外机房的主节点； 故障发生时候能够提供一个优雅的降级。副本集primary节点宕机后再选出新的primary节点这段时间内（10秒或更长时间）能够依然响应客户端应用的读请求； 应用能够容忍一定程度的数据不一致性。 副本集不是为了提高读性能存在的，在进行oplog的时候，读操作时被阻塞的。提高读取性能应该使用分片和索引，它的存在更多是作为数据冗余，备份。尤其当主库本来就面临着大量的写入压力，对于副本集的节点，也同样会面临写的压力。 Read References: 应用程序驱动通过read reference来设定如何对副本集进行读取操作，默认的,客户端驱动所有的读操作都是直接访问primary节点的，从而保证了数据的严格一致性。 但有时为了缓解主节点的压力，我们可能需要直接从secondary节点读取，只需要保证最终一致性就可以了。 MongoDB 2.0之后支持五种的read preference模式： Primary：默认，只从主节点上进行读取操作。 primaryPreferred：在绝大部分的情形都是从主节点上读取数据的,只有当主节点不可用的时候，比如在进行failover的10秒或更长的时间内会从secondary节点读取数据。 secondary：只从secondary节点上进行读取操作，存在的问题是secondary节点的数据会比primary节点数据“旧”。 secondaryPreferred：优先从secondary节点进行读取操作。 nearest：既有可能从primary，也有可能从secondary节点读取，这个决策是通过一个叫member selection过程处理的。（网络延迟） 基本上常用的是，Primary，secondary，nearest 副本集的设定中可以通过Tag把成员归类，通过下面方法指定读的类型： 程序连接的时候，指定读的类型ReadPreference 用mongo命令连接，只对当前连接有效 1&gt; db.getMongo().setReadPref(``'secondaryPreferred'``) OpLogoplog是一种特殊的capped collection,用来滚动的保存MongoDB中所有数据操作的日志。副本集中secondary节点异步的从primary节点同步oplog然后重新执行它记录的操作，以此达到了数据同步的作用。这就要求oplog必须是幂等的，也就是重复执行相同的oplog记录得到的数据结构必须是相同的。 事实上副本集中所有节点之间都相互进行heartbeat来维持联系，任何节点都能从其它节点复制oplog。 capped collection是MongoDB中一种提供高性能插入、读取和删除操作的固定大小集合。当集合被填满的时候，新的插入的文档会覆盖老的文档。因为oplog是capped collection所以指定它的大小非常重要。如果太小那么老的文档很快就被覆盖了，那么宕机的节点就很容易出现无法同步数据的结果，但也不是越大越好，MongoDB在初始化副本集的时候都会有一个默认的oplog大小： 在64位的Linux,Solaris,FreeBSD以及Windows系统上，MongoDB会分配磁盘剩余空间的5%作为oplog的大小，如果这部分小于1GB则分配1GB的空间。 在64的OS X系统上会分配183MB。 在32位的系统上则只分配48MB。 首先生产环境使用MongoDB毫无疑问必须的是64为操作系统。其次大多数情况下默认的大小是比较适合的。举个例子，如果oplog大小为空闲磁盘的5%，它在24H内能被填满，也就是说secondary节点可以停止复制oplog达24H后仍然能够catch up上primary节点。而且通常的MongoDB副本集的操作量要比这低得多。 oplog数据结构： oplog的数据结构如下所示： { ts : …, op: …, ns: …, o: … o2: … } ts：8字节的时间戳，由4字节unix timestamp + 4字节自增计数表示。这个值很重要，在选举(如master宕机时)新primary时，会选择ts最大的那个secondary作为新primary。 op：1字节的操作类型，例如i表示insert，d表示delete。 ns：操作所在的namespace。 o：操作所对应的document,即当前操作的内容（比如更新操作时要更新的的字段和值）。 o2: 在执行更新操作时的where条件，仅限于update时才有该属性。 其中op有以下几个值： “i“： insert。 “u“： update。 “d“： delete。 “c“： db cmd。 “db“：声明当前数据库 (其中ns 被设置成为=&gt;数据库名称+ ‘.’)。 “n“: no op,即空操作，其会定期执行以确保时效性。 查看oplog大小： 通过db.printReplicationInfo() 可以查看副本集节点的oplog状态： 1234567mongo.test:PRIMARY&gt; db.printReplicationInfo() configured oplog size: 990MBlog length start to end: 2409secs (0.67hrs)oplog first event time: Wed Aug 29 2018 15:38:04 GMT+0800 (CST)oplog last event time: Wed Aug 29 2018 16:18:13 GMT+0800 (CST)now: Wed Aug 29 2018 16:18:19 GMT+0800 (CST) 修改oplog大小： 若集群未启动过，则可以通过配置参数来指定oplog大小： 12replication: oplogSizeMB: &lt;int&gt; 若已经启动集群，可以动态修改oplog大小： 123use localdb.oplog.rs.stats().maxSizedb.adminCommand(&#123;replSetResizeOplog: 1, size: 16000&#125;) 建议： 在现有的副本集中修改oplog的大小是相当麻烦的而且影响副本集性能，因此我们最好是预先根据应用的情况评估好oplog的大小：如果应用程序是读多写少，那么默认的大小已经足够了。 如果你的应用下面几种场景很多可能考虑需要更大的oplog: 在同一个时刻更新多个文档：oplog为了维持幂等性必须将mutil-updates翻译成一个个独立的操作，这会用去大量的oplog空间，但数据库中的数据量却没有相对称的增加。 多文档同时更新从1.1.3就有的特性，在mongo shell执行类似如下的命令，第四个参数必须制定为true：db.test.update({foo: &quot;bar&quot;}, {$set: {test: &quot;success!&quot;}}, false, true); 在插入时同时删除相同大小数据：和上面的结果一样在数据量没有增加的情况下却消耗了大量的oplog空间。 大量的In-Place更新操作：In-Place更新是指更新文档中原有的部分，但并不增加文档的大小。 上面三点总结起来就是消耗了大量的oplog但是数据量却没有等量的增加。 数据同步数据滞后: 前面已经提到MongoDB副本集中secondary节点是通过oplog来同步primary节点数据的，那具体的细节是怎么样的？在说数据如何同步之间先介绍一下replication lag，因为存在数据同步那必然存在一定程度的落后。这个问题对于整个MongoDB副本集的部署是至关重要的。 “滞后”是不可避免的，需要做的就是尽可能减小这种滞后，主要涉及到以下几点： 网络延迟：这是所有分布式系统都存在的问题。我们能做的就是尽可能减小副本集节点之间的网络延迟。 磁盘吞吐量：secondary节点上数据刷入磁盘的速度比primary节点上慢的话会导致secondary节点很难跟上primary节点的节奏。 并发 ：并发大的情况下，primary节点上的一些耗时操作会阻塞secondary节点的复制操作，导致复制操作跟不上主节点的写入负荷。解决方法是通过设置操作的write concern默认的副本集中写入操作只关心primary节点，但是可以指定写入操作同时传播到其他secondary节点，代价就是严重影响集群的并发性。 注意：而且这里还存在一个问题如果，如果写入操作关心的某个节点宕机了，那么操作将会一直被阻塞直到节点恢复。 适当的write concern：我们为了提高集群写操作的吞吐量经常会将writer concern设置为unacknowledged write concern，这导致primary节点的写操作很快而secondary节点复制操作跟不上。解决方法和第三点是类似的就是在性能和一致性之间做权衡。 数据同步: 副本集中数据同步有两个阶段。 初始化(initial sync):这个过程发生在当副本集中创建一个新的数据库或其中某个节点刚从宕机中恢复,或者向副本集中添加新的成员的时候,默认的,副本集中的节点会从离它最近的节点复制oplog来同步数据,这个最近的节点可以是primary也可以是拥有最新oplog副本的secondary节点。这可以防止两个secondary节点之间相互进行同步操作。 复制(replication):在初始化后这个操作会一直持续的进行着,以保持各个secondary节点之间的数据同步。 在MongoDB2.0以后的版本中,一旦初始化中确定了一个同步的目标节点后，只有当和同步节点之间的连接断开或连接过程中产生异常才可能会导致同步目标的变动，并且具有就近原则。考虑两种场景： 有两个secondary节点在一个机房，primary在另外一个机房。假设几乎在同一时间启动这三个实例(之前都没有数据和oplog)，那么两个secondary节点应该都是从primary节点同步数据，因为他们之前见都不会拥有比对方更新的oplog。如果重启其中一个secondary，那么它的同步目标将会变成另一个secondary，因为就近原则。 如果有一个primary和一个secondary分别在不同的机房，那么在之前secondary所在的机房中向副本集中新加一个节点时，那么新节点必然是从原先的那个secondary节点同步数据的。 在2.2版本以后，数据同步增加了一些额外的行为： secondary节点只有当集群中没有其他选择的时候才会从delayed节点同步数据； secondary节点绝不会从hidden节点同步数据； 当一个节点新加入副本集中会有一个recovering过程，在这段时间内secondary不会进行数据同步操作； 当一个节点从另一个节点同步数据的时候，需要保证两个节点的local.system.replset.members[n].buildIndexes值是一样的，要不都是false，要不都是true。 注：buildIndexes指定副本集中成员是否可以创建索引(某些情况下比如没有读操作或者为了提高写性能可以省略索引的创建)。当然即使该值为false，MongoDB还是可以在_id上创建索引以为复制操作服务。 参考链接🌐MongoDB的副本集Replica Set :globe_with_meridians:MongoDB Configuration File Options :globe_with_meridians:MongoDB副本集学习（三）：性能和优化相关]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记-主从]]></title>
    <url>%2F2018%2F08%2F23%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%BB%E4%BB%8E%2F</url>
    <content type="text"><![CDATA[Redis主从复制Redis 支持简单且易用的主从复制（master-slave replication）功能， 该功能可以让从服务器(slave server)成为主服务器(master server)的精确复制品。 以下是关于 Redis 复制功能的几个重要方面： Redis 使用异步复制。 从 Redis 2.8 开始， 从服务器会以每秒一次的频率向主服务器报告复制流（replication stream）的处理进度。 一个主服务器可以有多个从服务器。 不仅主服务器可以有从服务器， 从服务器也可以有自己的从服务器， 多个从服务器之间可以构成一个图状结构。 复制功能不会阻塞主服务器： 即使有一个或多个从服务器正在进行初次同步， 主服务器也可以继续处理命令请求。 复制功能也不会阻塞从服务器： 只要在 redis.conf 文件中进行了相应的设置， 即使从服务器正在进行初次同步， 服务器也可以使用旧版本的数据集来处理命令查询。 不过， 在从服务器删除旧版本数据集并载入新版本数据集的那段时间内， 连接请求会被阻塞。 你还可以配置从服务器， 让它在与主服务器之间的连接断开时， 向客户端发送一个错误。 复制功能可以单纯地用于数据冗余（data redundancy）， 也可以通过让多个从服务器处理只读命令请求来提升扩展性（scalability）： 比如说， 繁重的 SORT 命令可以交给附属节点去运行。 可以通过复制功能来让主服务器免于执行持久化操作： 只要关闭主服务器的持久化功能， 然后由从服务器去执行持久化操作即可。 原理复制流程无论是初次连接还是重新连接， 当建立一个从服务器时， 从服务器都将向主服务器发送一个 SYNC 命令。 接到 SYNC 命令的主服务器将开始执行 BGSAVE ， 并在保存操作执行期间， 将所有新执行的写入命令都保存到一个缓冲区里面。 当 BGSAVE 执行完毕后， 主服务器将执行保存操作所得的 .rdb 文件发送给从服务器， 从服务器接收这个 .rdb 文件， 并将文件中的数据载入到内存中。 之后主服务器会以 Redis 命令协议的格式， 将写命令缓冲区中积累的所有内容都发送给从服务器。 你可以通过 telnet 命令来亲自验证这个同步过程： 首先连上一个正在处理命令请求的 Redis 服务器， 然后向它发送 SYNC 命令， 过一阵子， 你将看到 telnet 会话（session）接收到服务器发来的大段数据（.rdb 文件）， 之后还会看到， 所有在服务器执行过的写命令， 都会重新发送到 telnet 会话来。 即使有多个从服务器同时向主服务器发送 SYNC ， 主服务器也只需执行一次 BGSAVE 命令， 就可以处理所有这些从服务器的同步请求。 从服务器可以在主从服务器之间的连接断开时进行自动重连， 在 Redis 2.8 版本之前， 断线之后重连的从服务器总要执行一次完整重同步（full resynchronization）操作， 但是从 Redis 2.8 版本开始， 从服务器可以根据主服务器的情况来选择执行完整重同步还是部分重同步（partial resynchronization）。 部分重同步从 Redis 2.8 开始， 在网络连接短暂性失效之后， 主从服务器可以尝试继续执行原有的复制进程（process）， 而不一定要执行完整重同步操作。 这个特性需要主服务器为被发送的复制流创建一个内存缓冲区（in-memory backlog）， 并且主服务器和所有从服务器之间都记录一个复制偏移量（replication offset）和一个主服务器 ID （master run id）， 当出现网络连接断开时， 从服务器会重新连接， 并且向主服务器请求继续执行原来的复制进程： 如果从服务器记录的主服务器 ID 和当前要连接的主服务器的 ID 相同， 并且从服务器记录的偏移量所指定的数据仍然保存在主服务器的复制流缓冲区里面， 那么主服务器会向从服务器发送断线时缺失的那部分数据， 然后复制工作可以继续执行。 否则的话， 从服务器就要执行完整重同步操作。 Redis 2.8 的这个部分重同步特性会用到一个新增的 PSYNC 内部命令， 而 Redis 2.8 以前的旧版本只有 SYNC 命令， 不过， 只要从服务器是 Redis 2.8 或以上的版本， 它就会根据主服务器的版本来决定到底是使用 PSYNC 还是 SYNC ： 如果主服务器是 Redis 2.8 或以上版本，那么从服务器使用 PSYNC 命令来进行同步。 如果主服务器是 Redis 2.8 之前的版本，那么从服务器使用 SYNC 命令来进行同步。 配置在从服务器上添加REPLICATION相关配置： 123slaveof &lt;masterip&gt; &lt;masterport&gt;# 如果主服务器设置了连接密码则添加：# masterauth &lt;master-password&gt; 其他相关配置参照Redis学习笔记-配置 主服务器只读模式从 Redis 2.6 开始， 从服务器支持只读模式， 并且该模式为从服务器的默认模式。 只读模式由 redis.conf 文件中的 slave-read-only 选项控制， 也可以通过 CONFIG SET 命令来开启或关闭这个模式。 只读从服务器会拒绝执行任何写命令， 所以不会出现因为操作失误而将数据不小心写入到了从服务器的情况。 即使从服务器是只读的， DEBUG 和 CONFIG 等管理式命令仍然是可以使用的， 所以我们还是不应该将服务器暴露给互联网或者任何不可信网络。 不过， 使用 redis.conf 中的命令改名选项， 我们可以通过禁止执行某些命令来提升只读从服务器的安全性。 你可能会感到好奇， 既然从服务器上的写数据会被重同步数据覆盖， 也可能在从服务器重启时丢失， 那么为什么要让一个从服务器变得可写呢？ 原因是， 一些不重要的临时数据， 仍然是可以保存在从服务器上面的。 比如说， 客户端可以在从服务器上保存主服务器的可达性（reachability）信息， 从而实现故障转移（failover）策略。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记-哨兵]]></title>
    <url>%2F2018%2F08%2F23%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%93%A8%E5%85%B5%2F</url>
    <content type="text"><![CDATA[:globe_with_meridians:Sentinel 123456789101112131415161718192021222324252627282930# 后台运行daemonize yes# 保护模式，开启保护模式后需要添加密码和bind。protected-mode &#123;&#123; redis_sentinel_protected_mode &#125;&#125;# 绑定网卡bind &#123;&#123; redis_sentinel_bind_interface &#125;&#125;# sentinel监听端口port &#123;&#123; redis_sentinel_port &#125;&#125;# 这一行代表sentinel监控的master的名字叫做mymaster，地址为redis_master_host，端口号为redis_master_port。# 行尾最后的一个2代表什么意思呢？我们知道，网络是不可靠的，有时候一个sentinel会因为网络堵塞而误以为一个master redis已经死掉了，当sentinel集群式，解决这个问题的方法就变得很简单，只需要多个sentinel互相沟通来确认某个master是否真的死了，这个2代表，当集群中有2个sentinel认为master死了时，才能真正认为该master已经不可用了。（sentinel集群中各个sentinel也有互相通信，通过gossip协议）。sentinel monitor mymaster &#123;&#123; redis_master_host &#125;&#125; &#123;&#123; redis_master_port &#125;&#125; &#123;&#123; redis_sentinel_quorum &#125;&#125;# down-after-milliseconds# sentinel会向master发送心跳PING来确认master是否存活，如果master在“一定时间范围”内不回应PONG 或者是回复了一个错误消息，那么这个sentinel会主观地(单方面地)认为这个master已经不可用了(subjectively down, 也简称为SDOWN)。而这个down-after-milliseconds就是用来指定这个“一定时间范围”的，单位是毫秒。# 不过需要注意的是，这个时候sentinel并不会马上进行failover主备切换，这个sentinel还需要参考sentinel集群中其他sentinel的意见，如果超过某个数量的sentinel也主观地认为该master死了，那么这个master就会被客观地(注意哦，这次不是主观，是客观，与刚才的subjectively down相对，这次是objectively down，简称为ODOWN)认为已经死了。需要一起做出决定的sentinel数量在上一条配置中进行配置。sentinel down-after-milliseconds mymaster &#123;&#123; redis_down_after_milliseconds &#125;&#125;sentinel failover-timeout mymaster &#123;&#123; redis_sentinel_failover_timeout &#125;&#125;# parallel-syncs# 在发生failover主备切换时，这个选项指定了最多可以有多少个slave同时对新的master进行同步，这个数字越小，完成failover所需的时间就越长，但是如果这个数字越大，就意味着越多的slave因为replication而不可用。可以通过将这个值设为 1 来保证每次只有一个slave处于不能处理命令请求的状态。sentinel parallel-syncs mymaster &#123;&#123; redis_sentinel_parallel_syncs &#125;&#125;# master密码&#123;% if redis_masterauth %&#125;sentinel auth-pass mymaster &#123;&#123; redis_masterauth &#125;&#125;&#123;% endif %&#125;# 日志设置loglevel &#123;&#123; redis_sentinel_loglevel &#125;&#125;logfile &#123;&#123; redis_logdir &#125;&#125;/&#123;&#123; redis_sentinel_logfile &#125;&#125;]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记-持久化]]></title>
    <url>%2F2018%2F08%2F23%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%8C%81%E4%B9%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[Redis的两种持久化方式及原理:globe_with_meridians:Redis文档 SNAPSHOTTING快照模式。 默认redis是会以快照的形式将数据持久化到磁盘的（一个二进制文件，dump.rdb，名字可以指定），在配置文件中的格式是：save N M ，表示在N秒之内，redis至少发生M次修改则redis抓快照到磁盘。比如： 123456# 900秒内如果至少有1个key的值发生变化，则保存到磁盘。# 300秒内如果至少有10个key的值发生变化，则保存到磁盘。# 60秒内如果至少有10000个key的值发生变化，则保存到磁盘。save 900 1save 300 10save 60 10000 这种持久化方式被称为快照（snapshot）。 当然我们也可以手动执行save或bgsave（异步）做快照： client也可以使用save或者bgsave命令通知redis做一次快照持久化。 save操作是在主线程中保存快照的，由于redis是用一个主线程来处理所有的client的请求，这种方式会阻塞所有client请求。所以不推荐使用。 工作原理当redis需要做持久化时，redis会fork一个子进程，子进程将数据写到磁盘上一个临时RDB文件中，当子进程完成写临时文件后，将原来的RDB文件替换掉。 这样做的好处是可以copy-on-write（写时复制）。 快照保存过程 redis调用fork，现在有了子进程和父进程。 父进程继续处理client请求，子进程负责将内存内容写入到临时文件。由于OS的写时复制机制（copy on write）父子进程会共享相同的物理页面，当父进程处理写请求时os会为父进程要修改的页面创建副本，而不是写共享的页面，所以子进程的地址空间内的数据是fork时刻整个数据库的一个快照。 当子进程将快照写入临时文件完毕后，用临时文件替换原来的快照文件，然后子进程退出（fork一个进程内在也被复制了，即内存会是原来的两倍左右）。 RDB优点 RDB 是一个非常紧凑（compact）的文件，它保存了 Redis 在某个时间点上的数据集。 这种文件非常适合用于进行备份： 比如说，你可以在最近的 24 小时内，每小时备份一次 RDB 文件，并且在每个月的每一天，也备份一个 RDB 文件。 这样的话，即使遇上问题，也可以随时将数据集还原到不同的版本。 RDB 非常适用于灾难恢复（disaster recovery）：它只有一个文件，并且内容都非常紧凑，可以（在加密后）将它传送到别的数据中心，或者亚马逊 S3 中。 RDB 可以最大化 Redis 的性能：父进程在保存 RDB 文件时唯一要做的就是 fork 出一个子进程，然后这个子进程就会处理接下来的所有保存工作，父进程无须执行任何磁盘 I/O 操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 RDB缺点 如果你需要尽量避免在服务器故障时丢失数据，那么 RDB 不适合你。 虽然 Redis 允许你设置不同的保存点（save point）来控制保存 RDB 文件的频率， 但是， 因为RDB 文件需要保存整个数据集的状态， 所以它并不是一个轻松的操作。 因此你可能会至少 5 分钟才保存一次 RDB 文件。 在这种情况下， 一旦发生故障停机， 你就可能会丢失好几分钟的数据。 每次保存 RDB 的时候，Redis 都要 fork() 出一个子进程，并由子进程来进行实际的持久化工作。 在数据集比较庞大时， fork() 可能会非常耗时，造成服务器在某某毫秒内停止处理客户端； 如果数据集非常巨大，并且 CPU 时间非常紧张的话，那么这种停止时间甚至可能会长达整整一秒。 虽然 AOF 重写也需要进行 fork() ，但无论 AOF 重写的执行间隔有多长，数据的耐久性都不会有任何损失。 AOFAOF（Append-only-file）模式。 filesnapshotting方式在redis异常死掉时，最近的数据会丢失（丢失的数据多少视你save策略的配置），所以这是它最大的缺点，当业务量很大时，丢失的数据是很多的。 append-only方法可以做到全部数据不丢失，但redis的性能就要差些。 AOF可以做到全持久化，只需要在配置文件中开启，appendonly yes开启AOF之后，redis每执行一个修改数据的命令，都会把它添加到aof文件中，当redis重启时，将会读取AOF文件进行”重放”以恢复到redis关闭前的最后时刻。 AOF文件刷新方式有三种：alway，everysec，no。详解请参考redis配置文件笔记。 AOF重写随着修改数据的执行AOF文件会越来越大，其中很多内容记录某一个key变化情况。因此redis有了一种比较有意思的特性：在后台重建AOF文件，而不会影响client端操作。在任何时候执行BGREWRITEAOF命令，都会把当前内存中最短序列的命令写到磁盘，这些命令可以完全构建当前的数据情况，而不会存在多余的变化情况（比如状态变化，计数器变化等），缩小AOF文件的大小。 所以当使用AOF时，redis推荐使用BGREWRITEAOF功能。 AOF修复可能由于系统原因导致了AOF损坏，redis无法再加载这个AOF，可以按照下面的步骤来修复： 为现有的 AOF 文件创建一个备份。 使用 Redis 附带的 redis-check-aof 程序—redis-check-aof --fix对原来的 AOF 文件进行修复。 （可选）使用 diff -u 对比修复后的 AOF 文件和原始 AOF 文件的备份，查看两个文件之间的不同之处。 重启 Redis 服务器，等待服务器载入修复后的 AOF 文件，并进行数据恢复。 工作原理LOG Rewrite同样用到了copy-on-write： redis会fork一个子进程，子进程将最新的AOF写入一个临时文件， 父进程增量的把内存中的最新执行的修改写入（这时仍写入旧的AOF，rewrite如果失败也是安全的） 当子进程完成rewrite临时文件后，父进程会受到一个信号，并把之前的内存中的增量的修改写入临时文件末尾，这时redis将旧AOF文件重命名，临时文件重命名，开始向新的AOF中写入。 具体过程： redis调用fork，现在有父子两个进程。 子进程根据内存中的数据库快照，往临时文件中写入重建数据库状态的命令。 父进程继续处理client请求，除了把写命令写入到原来的aof文件中，同时把收到的写命令缓存起来。这样就能保证如果子进程重写失败的话并不会出问题。 当子进程把快照内容写入以命令方式写到临时文件中后，子进程发信号通知父进程。然后父进程把缓存的写命令也写入到临时文件。 现在父进程可以使用临时文件替换旧的aof文件，并重命名。后面收到的写命令也开始往新的aof文件中追加。 AOF优点 使用 AOF 持久化会让 Redis 变得非常耐久（much more durable）：你可以设置不同的 fsync 策略，比如无 fsync ，每秒钟一次 fsync ，或者每次执行写入命令时 fsync 。 AOF 的默认策略为每秒钟 fsync 一次，在这种配置下，Redis 仍然可以保持良好的性能，并且就算发生故障停机，也最多只会丢失一秒钟的数据（ fsync 会在后台线程执行，所以主线程可以继续努力地处理命令请求）。 AOF 文件是一个只进行追加操作的日志文件（append only log）， 因此对 AOF 文件的写入不需要进行 seek ， 即使日志因为某些原因而包含了未写入完整的命令（比如写入时磁盘已满，写入中途停机，等等）， redis-check-aof 工具也可以轻易地修复这种问题。 Redis 可以在 AOF 文件体积变得过大时，自动地在后台对 AOF 进行重写： 重写后的新 AOF 文件包含了恢复当前数据集所需的最小命令集合。 整个重写操作是绝对安全的，因为 Redis 在创建新 AOF 文件的过程中，会继续将命令追加到现有的 AOF 文件里面，即使重写过程中发生停机，现有的 AOF 文件也不会丢失。 而一旦新 AOF 文件创建完毕，Redis 就会从旧 AOF 文件切换到新 AOF 文件，并开始对新 AOF 文件进行追加操作。 AOF 文件有序地保存了对数据库执行的所有写入操作， 这些写入操作以 Redis 协议的格式保存， 因此 AOF 文件的内容非常容易被人读懂， 对文件进行分析（parse）也很轻松。 导出（export） AOF 文件也非常简单： 举个例子， 如果你不小心执行了 FLUSHALL 命令， 但只要 AOF 文件未被重写， 那么只要停止服务器， 移除 AOF 文件末尾的 FLUSHALL 命令， 并重启 Redis ， 就可以将数据集恢复到 FLUSHALL 执行之前的状态。 AOF缺点 对于相同的数据集来说，AOF 文件的体积通常要大于 RDB 文件的体积。 根据所使用的 fsync 策略，AOF 的速度可能会慢于 RDB 。 在一般情况下， 每秒 fsync 的性能依然非常高， 而关闭 fsync 可以让 AOF 的速度和 RDB 一样快， 即使在高负荷之下也是如此。 不过在处理巨大的写入载入时，RDB 可以提供更有保证的最大延迟时间（latency）。 AOF 在过去曾经发生过这样的 bug ： 因为个别命令的原因，导致 AOF 文件在重新载入时，无法将数据集恢复成保存时的原样。 （举个例子，阻塞命令 BRPOPLPUSH 就曾经引起过这样的 bug 。） 测试套件里为这种情况添加了测试： 它们会自动生成随机的、复杂的数据集， 并通过重新载入这些数据来确保一切正常。 虽然这种 bug 在 AOF 文件中并不常见， 但是对比来说， RDB 几乎是不可能出现这种 bug 的。 RDB 和 AOF ，我应该用哪一个？一般来说， 如果想达到足以媲美 PostgreSQL 的数据安全性， 你应该同时使用两种持久化功能。 如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。 有很多用户都只使用 AOF 持久化， 但我们并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快， 除此之外， 使用 RDB 还可以避免之前提到的 AOF 程序的 bug 。 RDB 持久化切换到 AOF 持久化在 Redis 2.2 或以上版本，可以在不重启的情况下，从 RDB 切换到 AOF ： 为最新的 dump.rdb 文件创建一个备份。 将备份放到一个安全的地方。 执行以下两条命令： 123redis-cli&gt; CONFIG SET appendonly yes redis-cli&gt; CONFIG SET save "" 确保命令执行之后，数据库的键的数量没有改变。 确保写命令会被正确地追加到 AOF 文件的末尾。 步骤 3 执行的第一条命令开启了 AOF 功能： Redis 会阻塞直到初始 AOF 文件创建完成为止， 之后 Redis 会继续处理命令请求， 并开始将写入命令追加到 AOF 文件末尾。 步骤 3 执行的第二条命令用于关闭 RDB 功能。 这一步是可选的， 如果你愿意的话， 也可以同时使用 RDB 和 AOF 这两种持久化功能。 别忘了在 redis.conf 中打开 AOF 功能！ 否则的话， 服务器重启之后， 之前通过 CONFIG SET 设置的配置就会被遗忘， 程序会按原来的配置来启动服务器。 RDB 和 AOF 之间的相互作用在版本号大于等于 2.4 的 Redis 中， BGSAVE 执行的过程中， 不可以执行 BGREWRITEAOF 。 反过来说， 在 BGREWRITEAOF 执行的过程中， 也不可以执行 BGSAVE 。 这可以防止两个 Redis 后台进程同时对磁盘进行大量的 I/O 操作。 如果 BGSAVE 正在执行， 并且用户显示地调用 BGREWRITEAOF 命令， 那么服务器将向用户回复一个 OK 状态， 并告知用户，BGREWRITEAOF 已经被预定执行： 一旦 BGSAVE 执行完毕， BGREWRITEAOF 就会正式开始。 当 Redis 启动时， 如果 RDB 持久化和 AOF 持久化都被打开了， 那么程序会优先使用 AOF 文件来恢复数据集， 因为 AOF 文件所保存的数据通常是最完整的。 备份 Redis 数据Redis 对于数据备份是非常友好的， 因为你可以在服务器运行的时候对 RDB 文件进行复制： RDB 文件一旦被创建， 就不会进行任何修改。 当服务器要创建一个新的 RDB 文件时， 它先将文件的内容保存在一个临时文件里面， 当临时文件写入完毕时， 程序才使用 rename(2) 原子地用临时文件替换原来的 RDB 文件。 这也就是说， 无论何时， 复制 RDB 文件都是绝对安全的。 建议： 创建一个定期任务（cron job）， 每小时将一个 RDB 文件备份到一个文件夹， 并且每天将一个 RDB 文件备份到另一个文件夹。 确保快照的备份都带有相应的日期和时间信息， 每次执行定期任务脚本时， 使用 find 命令来删除过期的快照： 比如说， 你可以保留最近 48 小时内的每小时快照， 还可以保留最近一两个月的每日快照。 至少每天一次， 将 RDB 备份到你的数据中心之外， 或者至少是备份到你运行 Redis 服务器的物理机器之外。 容灾备份Redis 的容灾备份基本上就是对数据进行备份， 并将这些备份传送到多个不同的外部数据中心。 容灾备份可以在 Redis 运行并产生快照的主数据中心发生严重的问题时， 仍然让数据处于安全状态。 因为很多 Redis 用户都是创业者， 他们没有大把大把的钱可以浪费， 所以下面介绍的都是一些实用又便宜的容灾备份方法： Amazon S3 ，以及其他类似 S3 的服务，是一个构建灾难备份系统的好地方。 最简单的方法就是将你的每小时或者每日 RDB 备份加密并传送到 S3 。 对数据的加密可以通过 gpg -c 命令来完成（对称加密模式）。 记得把你的密码放到几个不同的、安全的地方去（比如你可以把密码复制给你组织里最重要的人物）。 同时使用多个储存服务来保存数据文件，可以提升数据的安全性。 传送快照可以使用 SCP 来完成（SSH 的组件）。 以下是简单并且安全的传送方法： 买一个离你的数据中心非常远的 VPS ， 装上 SSH ， 创建一个无口令的 SSH 客户端 key ， 并将这个 key 添加到 VPS 的 authorized_keys 文件中， 这样就可以向这个 VPS 传送快照备份文件了。 为了达到最好的数据安全性，至少要从两个不同的提供商那里各购买一个 VPS 来进行数据容灾备份。 需要注意的是， 这类容灾系统如果没有小心地进行处理的话， 是很容易失效的。 最低限度下， 你应该在文件传送完毕之后， 检查所传送备份文件的体积和原始快照文件的体积是否相同。 如果你使用的是 VPS ， 那么还可以通过比对文件的 SHA1 校验和来确认文件是否传送完整。 另外， 你还需要一个独立的警报系统， 让它在负责传送备份文件的传送器（transfer）失灵时通知你。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记-配置]]></title>
    <url>%2F2018%2F08%2F22%2FRedis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Redis配置文件解析NETWORK （网络）NETWORK相关配置： 1234567891011121314151617181920212223242526272829303132333435363738394041421. 监听地址# 默认监听所有网卡，也可以指定监听一个或多个。# 网络中的服务是通过ip+进程来进行区分的，当一个服务器拥有两个ip时，自然就在网络中拥有两个人身份，如内网，外网，当你指向让redis在一个网络上监听时，就可以用此配置。bind 127.0.0.1 192.168.198.222. 保护模式# 默认为开启。# 保护模式开启时出现以下两种情况只能以127.0.0.1（ipv4）、::1（ipv6）或socket连接：# 1. 没有设置bind绑定网卡。# 2. 没有设置密码。protected-mode yes3. 监听端口# 指定redis监听端口，默认端口为6379。# 如果端口为0，则redis不会再TCP socket上进行监听（不在TCP socket上进行监听，不代表没法连接，只是无法使用网络连接而已）。port 63794. TCP连接队列长度# 此参数确定了TCP连接中已完成队列（完成三次握手之后）的长度，当然此值必须不大于linux系统定义的/proc/sys/net/core/somaxconn值，如果超过somaxconn值，则linux系统会将其截断，默认为511。# 此值需要参考linux系统的两个内核参数：# 1. /proc/sys/net/core/somaxconn 服务端所能接受最大客户端数量，即完成连接上限。# 2. /proc/sys/net/ipv4/tcp_max_syn_backlog 服务端所能接受SYN同步包的最大客户端数量，即半连接上限。# 所以需要提高somaxconn与tcp_max_syn_backlog的值来达到预期的效果。tcp-backlog 5115. Socket连接# 指定unix sock的路径来进行连接监听，默认是不指定，因此redis不会再unix socket上进行监听。# 本地运行时，建议打开 socket 连接方式。unixsocket /tmp/redis.sockunixsocketperm 7556. 连接超时时间# 客户端和服务端的连接超时时间，默认为0，表示不超时。timeout 07. 连接保活策略# 单位为秒。# 从3.2.1版本之后，官方给出的合理值为300，默认值为300s。# 1. 如果设置为300秒，则服务端会每300秒向连接空闲的客户端发起一次ACK请求，以检查客户端是否已经挂掉。# 2. 对于无响应的客户端则会关闭其连接。# 如果设置为0，则不会进行连接检测。tcp-keepalive 300 GENRAL （通用）GENRAL相关配置： 1234567891011121314151617181920212223242526272829303132331. 守护进程# redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启动守护进程。daemonize no# 选择是upstart还是systemd管理redis进程。此参数和操作系统相关：# 1. supervised no 没有监督互动# 2. supervised upstart upstart信号通过将Redis置于SIGSTOP模式发送# 3. supervised systemd systemd信号通过READY=1写入NOTIFY_SOCKET环境变量# 4. supervised auto 通过UPSTART_JOB或NOTIFY_SOCKET环境变量来自动选择supervised no# 当redis以守护进程方式运行时，指定pid文件路径。redis默认会把pid写入/var/run/redis.pid。pidfile /var/run/redis_6379.pid2. 日志相关# redis总共支持四个日志级别：# 1. debug：记录很多信息，用于开发和测试。# 2. varbose：少但有用的信息，不像debug会记录那么多信息。# 3. notice：适量的信息，常用于生产环境。# 4. warning：只有非常重要或者严重的信息会记录到日志。# 默认级别是notice。loglevel notice# 日志文件位置。# 当指定为空字符串时，为标准输出，如果redis已守护进程模式运行，那么日志将会输出到/dev/null。logfile /var/log/redis/redis.log# 如果想把日志记录到系统日志syslog中，就把它改成 yes。还可以通过下面的参数来达到符合的要求。syslog-enabled no# 指定syslog的标示符（系统日志中的身份），如果syslog-enabled为no，则这个选项无效。syslog-ident redis# 指定syslog设备（facility），必须是USER或者LOCAL0到LOCAL7。具体可以参考syslog服务本身的用法。syslog-facility local03. 数据库数量# 可用的数据库数，默认值为16。默认数据库为0，数据库范围在0-（database-1）之间。databases 16 SNAPSHOTTING （快照）SNAPSHOTTING相关配置： 1234567891011121314151617181920212223242526272829303132333435363738391. 快照持久化策略# 这个参数是redis持久化的支持，基于snapshot机制 ，定期执行持久化存储，生成rdb文件。# 格式为：save &lt;seconds&gt; &lt;changes&gt; # &lt;seconds&gt;和&lt;changes&gt;都满足时就会触发数据保存动作。# 以下为默认策略：# 900秒内如果至少有1个key的值发生变化，则保存到磁盘。# 300秒内如果至少有10个key的值发生变化，则保存到磁盘。# 60秒内如果至少有10000个key的值发生变化，则保存到磁盘。# 如果不想让redis自动保存数据，就把以下配置注释掉。也可以直接配置空字符来实现停用：save ""。save 900 1save 300 10save 60 100002. 持久化错误策略# 如果用户开启了RDB快照功能，那么在redis持久化数据到磁盘时如果出现失败，默认情况下，redis会停止接受所有的写请求。# 这样做的好处在于可以让用户很明确的知道内存中的数据和磁盘上的数据已经存在不一致了。# 如果redis不顾这种不一致，一意孤行的继续接收写请求，就可能会引起一些灾难性的后果。# 如果下一次RDB持久化成功，redis会自动恢复接受写请求。# 如果不在乎这种数据不一致或者有其他的手段发现和控制这种不一致的话，可以关闭这个功能，以便在快照写入失败时，也能确保redis继续接受新的写请求。stop-writes-on-bgsave-error yes3. 快照压缩# 存储到磁盘中的快照，可以设置是否进行压缩存储。# 默认为yes，redis会采用LZF算法进行压缩。# 如果你不想消耗CPU来进行压缩的话，可以设置为关闭此功能，但是存储在磁盘上的快照会比较大。rdbcompression yes4. 快照校验# 在存储快照后，我们还可以让redis使用CRC64算法来进行数据校验，但是这样做会增加大约10%的性能消耗。# 如果希望获取到最大的性能提升，可以关闭此功能。rdbchecksum yes5. 快照名称# 设置快照文件的名称dbfilename dump.rdb6. 快照路径# 设置这个快照文件存放的路径dir /var/lib/redis REPLICATION （复制）REPLICATION相关设置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485861. 主从复制# 主从复制功能，设置该数据库为其他数据库的从数据库。只需在slave角色中添加此选项。# 建议用户为slave设置一个不同频率的快照持久化的周期（区别于master），或者为从redis配置一个不同的服务端口。slaveof &lt;masterip&gt; &lt;masterport&gt;2. 密码校验# 如果master设置了验证密码的话（使用requirepass来设置），则在slave的配置中要使用masterauth来设置校验密码，否则的话，主redis会拒绝从redis的访问请求。masterauth &lt;master-password&gt;3. Slave响应请求策略# 当slave丢失与master的连接时，或者slave仍在在于master进行数据同步时（还没有与master数据保持一致），slave可以用两种方式来响应客户端请求：# 1. 如果slave-serve-stale-data设置为yes，则slave仍会继续响应客户端的读写请求，此时可能会有问题。# 2. 如果slave-serve-stale-data设置为no，则slave会对客户端的请求返回“SYNC with master in progress”。当然也有例外，当客户端发来INFO请求和SLAVEOF请求，slave还是会进行处理。slave-serve-stale-data yes4. Slave读写策略# 2.6版本之后，默认slave为只读。# 可以控制一个slave是否可以接受写请求。将数据直接写入slave，一般只适用于那些生命周期非常短的数据，因为在主从同步时，这些临时数据就会被清理掉。# 只读的slave并不适合直接暴露给不可信的客户端。为了尽量降低风险，可以使用rename-command指令来将一些可能有破坏力的命令重命名，避免外部直接调用。比如：# rename-command CONFIG 73e99d350a4aa6f1a5af04ec29173f73slave-read-only yes5. 无硬盘复制# 备份同步策略：硬盘还是套接字# 新slave服务连接或者旧slave重新连接时不能只复制与master有差异的数据，必须做一个全部同步。需要一个新RDB文件dump出来，然后从master传输到slave。有两种方式：# 1. 基于硬盘（disk-backed）：master创建一个新进程dump RDB，创建完成后由父进程（即主进程）增量传给slaves。# 2. 基于socket（diskless）：master创建一个进程直接dump RDB到slave的socket，不经过主进程，不经过磁盘。# 基于硬盘的话，RDB文件创建完毕后，可以同时服务更多的slave。# 基于socket的话，新slave建立连接后需要排队。同步完一个在进行下一个。repl-diskless-sync no# 当使用diskless时，master收到第一个slave同步请求时，会等待多个slave请求，可以一起同步数据。请求间隔为repl-disk-sync-delay的配置时间。超过了repl-disk-sync-delay间隔时间，后来的slave需要排队。# 可以配置当收到第一个请求时，等待多个slave一起来请求之间的间隔时间。# 如果为0，则立即启动同步repl-diskless-sync-delay 56. Slave Ping监测# slave会按照一个时间间隔向master发送PING请求，默认为10秒。repl-ping-slave-period 107. 同步超时时间# 在主从同步时，可能在这些情况下会有超时发生：# 1. 以slave的角度来看，当有大规模IO传输时。# 2. 以slave的角度来看，当数据传输或PING时，master超时# 3. 以master的角度来看，在回复从redis的PING时，slave超时# 可以设置上述超时的时限，不过要确保这个时限比repl-ping-slave-period的值要大，否则每次master都会认为slave超时。repl-timeout 608. 同步延迟策略# 如果为yes，则禁用NO_DELAY，redis会使用较少量的TCP包和带宽向slave发送数据。但这会导致在slave增加一点数据的延时。（Linux内核默认配置情况下最多40毫秒的延时。）# 如果为no，则启用NO_DELAY，slave的数据延时不会那么多，但同步需要的带宽相对较多。repl-disable-tcp-nodelay no9. 同步缓存# 队列长度（backlog)是master中的一个缓冲区，在与slave断开连接期间，master会用这个缓冲区来缓存应该发给slave的数据。这样的话，当slave重新连接上之后，就不必重新全量同步数据，只需要同步这部分增量数据即可。repl-backlog-zise 1mb# 如果master等了一段时间之后，还是无法连接到slave，那么缓冲队列中的数据将被清理掉。# 可以设置master要等待的时间，默认是1小时。# 如果设置为0，则表示永远不清理。repl-backlog-ttl 360010. Slave优先级# 编号越小，优先级越高# 在不止1个slave存在的部署环境下，当master宕机时，Redis Sentinel会将priority值最小的slave提升为master。# 若配置为0，则对应的slave永远不会自动提升为master。slave-priority 10011. Master接收写请求条件# 如果master有至少N个slave，并且ping心跳的超时不超过M秒，那么它就会接收写请求。# 如果N和M的条件都无法达到，那么master会回复一个错误信息。写请求也不会被处理。# 默认min-slaves-to-write为0，表示对slaves数量无限制，网络延迟为10s。# 两个配置中有一个被置为0，则这个特性将被关闭。# 需要至少3个slave链接，且延迟小于等于10秒时，master才能写数据。min-slaves-to-write 3min-slaves-max-lag 1012. Slave信息# master能够以不同的方式列出所连接slave的地址和端口。# 例如，“INFO replication”部分提供此信息，除了其他工具之外，Redis Sentinel还使用该信息来发现slave实例。# 此信息可用的另一个地方在masterser的“ROLE”命令的输出中。# 通常由slave报告的列出的IP和地址,通过以下方式获得：# 1. IP：通过检查slave与master连接使用的套接字的对等体地址自动检测地址。# 2. 端口：端口在复制握手期间由slavet通信，并且通常是slave正在使用列出连接的端口。# 然而，当使用端口转发或网络地址转换（NAT）时，slave实际上可以通过(不同的IP和端口对)来到达。 slave可以使用以下两个选项，以便向master报告一组特定的IP和端口，以便INFO和ROLE将报告这些值。# 如果你需要仅覆盖端口或IP地址，则没必要使用这两个选项。slave-announce-ip 5.5.5.5slave-announce-port 1234 SECURITY （安全）SECURITY相关配置： 123456789101. 连接密码# 设置客户端连接后进行任何其他指令前需要使用的密码。# 警告：因为redis速度相当快，所以在一台比较好的服务器下，一个外部用户可以在一秒钟进行150k次的密码尝试，这意味着需要指定非常非常强大的密码来防止暴力破解。requirepass foobared2. 指令重命名# 比如将一些比较危险的命令改个名字，避免被误执行。比如可以把CONFIG命令改成一个很复杂的名字，这样可以避免外部的调用，同时还可以满足内部调用的需要，比如：# rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52# 如果想禁用一个命令，直接把它重命名为一个空字符""即可。rename-command CONFIG "" LIMITS （限制）LIMITS相关配置： 12345678910111213141516171819202122232425262728291. 客户端最大连接数# 设置同一时间最大客户端连接数，默认为10000。# Redis可以同时打开的客户端连接数为redis进程可以打开的最大文件描述符数-32（redis server自身会使用一些）。# 如果设置maxclients 0，表示不作限制。# 当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息。maxclients 100002. 内存限制及清除策略# 指定redis最大内存限制，redis在启动时会把数据加载到内存中，达到最大内存后，redis会按照清除策略尝试清除已到期的key。# 如果redis无法根据移除规则来移除内存中的数据，或者我们设置了“不允许移除”，那么redis则会针对那些需要申请内存的指令返回错误信息，比如SET、LPUSH等。但是对于无内存申请的指令，仍然会正常响应，比如GET等。# 注意：redis新的vm机制，会把key存放内存，value会存放在swap分区。# 注意：如果redis有slave的话，maxmemory最好设置为master内存的一半及以下，因为master需要启动新进程去dump rdb文件。maxmemory &lt;bytes&gt;# 当内存达到最大值的时候redis会选择清除一些数据，清除规则有：# 1. volatile-lru：利用LRU算法移除设置过过期时间的key。# 2. allkeys-lru：利用LRU算法移除任何key。# 3. volatile-random：随机地删除过期set中的key。# 4. allkeys-random：随机删除任意一个key。# 5. volatile-ttl：移除那些TTL值最小的key，即那些最近才过期的key。# 6. noeviction：不删除任何key，写操作会报错。# 注意：对于上面的策略，如果没有合适的key可以移除，当写的时候redis会返回一个错误。# 默认值为：noevictionmaxmemory-policy noeviction# LRU 和 minimal TTL 算法都不是精准的算法，但是相对精确的算法(为了节省内存)。# 所以可以对它进行优化，以获得速度或准确性。默认情况下，Redis会检查5个键，并选择最近使用较少的键，您可以使用下面的配置指令更改样本大小。# 默认值为5个样本通常情况下能够达到很好的效果。10个样本精确度更高，但是花费更多的CPU。3个样本是非常快的，但不是很准确。maxmemory-samples 5 APPEND ONLY MODE （AOF）APPEND ONLY MODE相关配置： 1234567891011121314151617181920212223242526272829303132333435363738391. AOF持久化功能# 默认情况下，redis会异步的将数据持久化到磁盘。这种模式在大部分应用程序中已被验证是很有效的，但是在一些问题发生时，比如断电，则这种机制可能会导致数分钟的写请求丢失。# 所以redis提供了另外一种更加高效的数据库备份及灾难恢复方式，AOF(Append Only File)。# 开启append only模式之后，redis会把所接收到的每一次写操作请求都追加到appendonly.aof文件中，当# # # 但是这样会造成appendonly.aof文件过大，所以redis还支持了BGREWRITEAOF指令，对appendonly.aof进行重新整理。# 默认是不开启的。# 注意：如果需要，可以同时开启AOF模式和RDB（快照）模式。这种情况下，redis重建数据集时会优先使用appendonly.aof而忽略dump.rdb。appendonly no# 设置AOF文件名称，默认为appendonly.aof。appendfilename "appendonly.aof"2. AOF持久化策略# redis支持三种同步AOF文件的策略：# 1. always：每一次写操作都会调用一次fsync，这时数据是最安全的。当然，由于每次都会执行fsync，所以其性能也会受到影响。这种模式下，redis会相对较慢，但数据最安全。# 2. everysec：redis会默认每隔一秒进行一次fsync调用，将缓冲区的数据写入磁盘。但是当这一次的fsync调用时长超过1秒钟时，redis会采取延迟fsync的策略，再等一秒钟。也就是在两秒后在进行fsync，这一次的fsync不管会执行多长时间都会进行。这时候由于在fsync时文件描述符会阻塞，所以当前的写操作就会阻塞。所以，在绝大多数情况下，redis会每隔一秒进行一次fsync。在最坏的情况下，两秒钟会进行一次fsync操作。这一操作在大多数数据库系统中被称为group commit，就是组合多次写操作的数据，一次性将日志写到磁盘。这种模式是性能和安全的折中。# 3. no：redis不会主动调用fsync去将AOF日志内容同步到磁盘，所以这一切就完全依赖于操作系统的调试了。对于大多数linux系统，是每30秒进行一次fsync，将缓冲区中的数据写到磁盘上。这种模式下，redis的性能会最快。# 默认为，everysec。appendfsync everysec3. AOF重写策略# 当aof增长到一定规模时，redis会隐式调用BGREWRITEAOF来重写log文件，以缩减文件体积。# 设置为no时在后台aof文件rewrite期间调用fsync，默认为no，表示要调用fsync。# 设置为yes表示rewrite期间对新写操作不fsync,暂时存在内存中,等rewrite完成后再写入。（Linux的默认fsync策略是30秒。可能丢失30秒数据。）# redis在后台写RDB文件或重写AOF文件期间会存在大量磁盘IO，此时在某些linux系统中，调用fsync可能会阻塞。# 如果对延迟要求很高的应用，这个字段可以设置为yes，否则还是设置为no，这样对持久化特性来说这是更安全的选择。no-appendfsync-on-rewrite no# 1. auto-aof-rewrite-percentage：指定redis重写aof文件的条件，默认为100，表示与上次rewrite的aof文件大小相比，当前aof文件增长量超过上次aof文件大小的100%时，就会触发backgroud rewrite。# 若auto-aof-rewrite-percentage配置为0，则会禁用自动rewrite功能。# 2. auto-aof-rewrite-min-size：指定触发rewrite的aof文件大小。若aof文件小于该值，即时当前文件的增量比例达到auto-aof-rewrite-percentage的配置值，也不会触发自动rewrite。# 这两个配置同时满足时，才会触发rewrite。auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb4. AOF文件修复# AOF文件可能在尾部是不完整的（尤其是mount ext4文件系统时没有加上data=ordered选项）。那redis重启时load进内存的时候就可能出现问题。# 配置aof-load-truncated为yes时，redis会自动发布一个log给客户端然后load。# 如果配置为no时，用户必须手动redis-check-aof修复aof文件才可以。aof-load-truncated yes LUA SCRIPTING （LUA脚本）LUA SCRIPTING相关配置： 12345671. 脚本执行时间限制# Redis以毫秒为单位限定lua脚本的最大执行时间。# 当lua脚本在超出最大允许执行时间之后，redis会记录这个脚本到日志中，并且会向这个请求返回error的错误。# 仅当SCRIPT KILL和SHUTDOWN NOSAVE命令可用的时候，一个运行时间超过最大限定时间的脚本才会被继续执行。# SCRIPT KILL用来停止一个没有调用写入命令的脚本。当用户不想等待脚本的自然中止又在进行写操作的时候，采用SHUTDOWN NOSAVE是解决这个问题的唯一方法。可以立即停掉这个脚本。# 以下配置为一个lua脚本最长的执行时间为5000毫秒（5秒），如果为0或者负数，则表示无限执行时间。lua-time-limit 5000 REDIS CLUSTER （集群）REDIS CLUSTER相关配置： 12345678910111213141516171819202122232425262728293031321. 集群模式# 集群开关，默认是不开启集群模式。cluster-enabled yes2. 集群配置文件# 集群配置文件的名称，每个节点都有一个集群相关的配置文件，持久化保存集群的信息。# 这个文件并不需要手动配置，这个配置文件有Redis生成并更新，每个Redis集群节点需要一个单独的配置文件# 确保与实例运行的系统中配置文件名称不冲突cluster-config-file nodes-6379.conf3. 节点超时时间# 节点互连超时的阀值，集群节点超时毫秒数。cluster-node-timeout 150004. 故障转移策略 # 在进行故障转移的时候，全部slave都会请求申请为master，但是有些slave可能与master断开连接一段时间了，导致数据过于陈旧，这样的slave不应该被提升为master。该参数就是用来判断slave节点与master断线的时间是否过长。# 判断方法是：# 1. slave断开连接的时间# 2. (node-timeout * slave-validity-factor) + repl-ping-slave-period# 如果节点超时时间为30s, 并且slave-validity-factor为10,即如果超过310秒slave将不会尝试进行故障转移。# 默认为10cluster-slave-validity-factor 10# master的slave数量大于该值，slave才能迁移到其他孤立master上。# 如这个参数若被设为2，那么只有当一个主节点拥有2个可工作的从节点时，它的一个从节点会尝试迁移。cluster-migration-barrier 15. 集群Slot策略# 默认情况下，集群全部的slot有节点负责，集群状态才为ok，才能提供服务。# 设置为no，可以在slot没有全部分配的时候提供服务。# 不建议打开该配置，这样会造成分区的时候，小分区的master一直在接受写请求，而造成很长时间数据不一致。cluster-require-full-coverage yes SLOW LOG （慢日志）SLOW LOG 相关配置： 1234567891011121. 慢日志策略# Redis Slow Log记录超过特定执行时间的命令。执行时间不包括I/O计算比如连接客户端、返回结果等，只是命令执行时间（线程因为执行这个命令而锁定且无法处理其他请求的阶段）。即执行比较慢的命令。# 可以通过两个参数设置slow log：# 1. 一个是慢查询的阈值，单位是毫秒。# 2. 另一个是slow log的长度（即记录的最大数量），相当于一个队列。# 默认配置记录redis执行超过10000毫秒的参数。# 负数则关闭slow log，0则会导致每个命令都被记录。slowlog-log-slower-than 10000# 不设置会消耗过多内存。可以通过SLOWLOG RESET回收慢日志消耗的内存。# 默认值为128，当慢日志超过128时，最先进入的队列的记录会被踢出。slowlog-max-len 128 LATENCY MONITOR（延迟监控）LATENCY MONITOR相关配置： 123456781. 延迟监控# Redis延迟监控子系统案例与操作系统收集的redis实例相关的数据不同。# 通过LATENCY命令，可以为用户打印出相关信息的图形和报告。# 这个系统只会记录运行时间和超出指定时间值的命令，如果设置为0，这个监控会被关闭。# 默认情况下是关闭延迟监控。因为如果没有延迟的问题大部分情况下不需要，并且收集数据的行为会对性能造成影响。# 延迟监控可以使用命令来打开：# CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;latency-monitor-threshold 0 EVENT NOTIFICATION （事件通知）EVENT NOTIFICATION相关配置： 1234567891011121314151617181920212223241. 事件通知# Redis可以在key空间中采用发布/订阅模式来通知时间的发生。# 对于一个实例，如果键空间事件通知是启用状态，当一个客户端执行在一个存储在Database 0名为“foo”的key DEL操作时，有如下两条信息将会通过发布订阅系统产生：# PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo# 可以在下表中选择Redis要通知的事件类型，事件类型由单个字符来标识：# K keyspace事件，以__keyspace@__的前缀方式发布# E keyevent事件，以__keysevent@__的前缀方式发布# g 通用事件（不指定类型），像DEL，EXPIRE，RENAME，...# $ string命令# l list命令# s set命令# h hash命令# z 有序集合命令# x 过期时间（每次key过期时生成）# e 清除时间（当key在内存被清除时生成）# A g$shzxe的别称，意味着通知所有的事件# 例如：启用list和通用事件# notify-keyspace-events elg# 默认所有的通知被禁用，空字符串的意思是通知被禁用。# 注意如果不指定至少K或E之一，不会发送任何事件。notify-keyspace-events "" ADVANCED CONFIG （高级配置）ADVANCED CONFIG相关配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081821. 编码策略# 创建空白哈希表时，程序默认使用REDIS_ENCODING_ZIPLIST编码，当以下任何一个条件被满足时，程序将编码切换为REDIS_ENCODING_HT：# 1. 哈希表中某个键或某个值的长度大于server.hash_max_ziplist_value（默认值为64）。# 2. 压缩列表中的节点数量大于server.hash_max_ziplist_entries（默认值为512）。# ziplist是一个解决空间问题的紧凑数据库存储结构，但是当数据超过阈值时，将采用原生的数据存储结构。hash-max-ziplist-entries 512hash-max-ziplist-value 64# 当取正值的时候，表示按照数据项个数来限定每个quicklist节点上的ziplist长度。比如，当这个参数配置成5的时候，表示每个quicklist节点的ziplist最多包含5个数据项。# 当取负值的时候，表示按照占用字节数来限定每个quicklist节点上的ziplist长度。这时，它只能取-1到-5这五个值，每个值含义如下：# 5: 每个quicklist节点上的ziplist大小不能超过64 Kb。（注：1kb =&gt; 1024 bytes）# 4: 每个quicklist节点上的ziplist大小不能超过32 Kb。# 3: 每个quicklist节点上的ziplist大小不能超过16 Kb。# 2: 每个quicklist节点上的ziplist大小不能超过8 Kb。（-2是Redis给出的默认值）# 1: 每个quicklist节点上的ziplist大小不能超过4 Kb。list-max-ziplist-size -2# 数据量小于等于set-max-intset-entries用intset，大于set-max-intset-entries用setset-max-intset-entries 512# 数据量小于等于zset-max-ziplist-entries用ziplist，大于zset-max-ziplist-entries用zsetzset-max-ziplist-entries 128zset-max-ziplist-value 642. 节点压缩# 这个参数表示一个quicklist两端不被压缩的节点个数。# 注：这里的节点个数是指quicklist双向链表的节点个数，而不是指ziplist里面的数据项个数。# 实际上，一个quicklist节点上的ziplist，如果被压缩，就是整体被压缩的。# 参数list-compress-depth的取值含义如下：# 0: 是个特殊值，表示都不压缩。这是Redis的默认值。# 1: 表示quicklist两端各有1个节点不压缩，中间的节点压缩。# 2: 表示quicklist两端各有2个节点不压缩，中间的节点压缩。# 3: 表示quicklist两端各有3个节点不压缩，中间的节点压缩。# 依此类推...# 由于0是个特殊值，很容易看出quicklist的头节点和尾节点总是不被压缩的，以便于在表的两端进行快速存取。list-compress-depth 05. 数据结构# value大小小于等于hll-sparse-max-bytes使用稀疏数据结构（sparse）。# 大于hll-sparse-max-bytes使用稠密的数据结构（dense），一个比16000大的value是几乎没用的。# 建议的value大概为3000。如果对CPU要求不高，对空间要求较高的，建议设置到10000左右。hll-sparse-max-bytes 30006. 内存释放# Redis将在每100毫秒时使用1毫秒的CPU时间来对redis的hash表进行重新hash，可以降低内存的使用。# 当使用场景中有严格的实时性需要，不能够接受redis时不时的对请求有2毫秒的延迟的话，把这项配置为no。# 如果没有这么严格的实时性要求，可以设置为yes，以便能够尽可能快的释放内存。activerehashing yes7. 缓存限制# 客户端输出缓冲区显示可以用来解决由于某些原因导致的强制断线而造成的不能读到足够的数据。# 一个比较常见的原因是由于发布订阅中，客户端不能够足够快速地消费发布者生成的信息。# 这个限制可以设置为如下的三种类型：# 1. normal：正常普通的客户端，包含监控客户端# 2. slave：主从服务器的从客户端# 3. pubsub：订阅了最少一个频道的客户端# 每一个client-output-buffer-limit格式如下：# client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;# 在两种情况下服务器认为客户端不是意外临时掉线：# 1. 缓冲区的数据达到硬限制# 2. 缓冲区的数据达到软限制，同时时间超过了指定值# 因为一个客户离线，有可能是临时性的网络故障或者传输问题，也有可能是永久性离线或者强制性离线，此时服务器将不会保留它的缓存数据。# 以下的设置就是判断这一情况的。# 硬限制和软限制都可以通过将其设置为0来关闭掉。# 对于normal client，第一个0表示取消hard limit，第二个0和第三个0表示取消soft limit，normal client默认取消限制，因为如果没有查询，他们是不会接收数据的。client-output-buffer-limit normal 0 0 0# 对于slave client和MONITER client，如果client-output-buffer一旦超过256mb，又或者超过64mb持续60秒，那么服务器就会立即断开客户端连接。client-output-buffer-limit slave 256mb 64mb 60# 对于pubsub client，如果client-output-buffer一旦超过32mb，又或者超过8mb持续60秒，那么服务器就会立即断开客户端连接client-output-buffer-limit pubsub 32mb 8mb 608. 执行任务频率# redis会按照一定的频率来处理诸如关闭超时连接，清理没有被使用的过期key等等此类后台任务，并不是所有的任务都是以相同的频率来执行的，redis通过一个hz的值执行检查任务。# 提高该值将在redis空闲时使用更多的CPU，但同时当有多个key同时到期会使redis的反应更灵敏，以及超时可以更精准地处理。# 范围是1到500之间，但是值超过100通常不是一个好主意。# 默认情况下，hz的值被设定为10。大多数用户应该使用10这个预设值，只有在非常低的延迟情况下有必要提高最大到100。# 计算方法：1s / hzhz 109. fsync增量同步# 在aof重写的时候，如果打开了aof-rewrite-incremental-fsync开关，系统会每32MB数据执行一次fsync。# 这对于把文件写入磁盘是有帮助的，可以减少aof大文件写入对磁盘的操作次数，避免过大的延迟峰值。aof-rewrite-incremental-fsync yes]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible学习笔记-Roles]]></title>
    <url>%2F2018%2F08%2F16%2FAnsible%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[创建自己的模块参照：《奔跑吧 ANSIBLE 》。 :globe_with_meridians:Ansible官方文档 模块存放位置Ansible会在Playbook的library目录下寻找自定义模块。 12345├── hosts├── library│ └── custom_module.py├── roles├── site.yml 所以将自定义模块存入playbooks/library/目录下。 Ansible 如何调用模块 生成带有参数的独立Python脚本（仅限Python模块） 如果模块是使用Python编写的并且使用了Ansible提供的辅助代码，那么Ansible将会生成注入了辅助代码和模块参数的自包含Python脚本。 将模块复制到远程主机 Ansible将把生成的Python脚本（对于基于Python的模块）或者本地文件playbooks/library/模块（非基于Python的模块）复制到远程主机中的临时目录下 在远程主机上创建参数文件（仅限非Python模块） 如果模块不是使用Python编写的，Ansible将会在远程主机上创建一个参数文件。 在远程主机上调用模块并传入参数文件作为参数 分析模块的标准输出 Ansible预期的输出变量自定义模块可以返回任何你定义的变量，但是Ansible会对几个必须返回的变量做特殊处理。 changed所有的Ansible模块都应该返回changed变量。 changed变量是表示模块的执行是否会导致主机状态改变的布尔型变量。 在Ansible运行的时候，不管状态是否发生改变，它都会把这个变量显示在输出中。如果task中含有notify语句来通知handler，name只有changed为true的时候通知才会被触发。 failed如果模块未能执行完成，name它应该返回failed=true。 Ansible将会把这个task看做执行失败并且不会对发生失败的主机继续执行接下来的task，除非这个task含有isgore_errors或者failed_when语句。 如果模块执行成功，即可以返回failed=false，也可以直接忽略这个变量。 msgmsg变量用来添加描述失败原因的描述信息。 如果task失败了并且模块返回了msg变量，那么Ansible将会与其他变量稍有不同的输出这个变量。 使用Python来实现模块Ansible提供了AnsibleModule的Python类来简化你的如下工作： 解析输入 使用JSON格式返回输出 调用外部程序 实际上，在编写Python模块时，Ansible将会把参数直接注入生成的Python文件中，而不需要你去解析单独的参数文件。]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-FirewallD]]></title>
    <url>%2F2018%2F08%2F09%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-FirewallD%2F</url>
    <content type="text"><![CDATA[FirewallD笔记理解配置，理解区域、知道如何配置规则。 🌐readhat官网文档 :globe_with_meridians:firewalld官网文档 概念firewalld自身并不具备防火墙的功能，而是和iptables一样需要通过内核的netfilter来实现，也就是说 firewalld和 iptables一样，他们的作用都是用于维护规则，而真正使用规则干活的是内核的netfilter，只不过firewalld和iptables的结 构以及使用方法不一样罢了。 原理firewalld有两层设计：核心层和顶部的D-Bus层。 核心层：核心层负责处理配置和后端，如iptables，ip6tables，ebtables，ipset和模块加载器。 D-Bus层：D-Bus接口是更改和创建防火墙配置的主要方式。 firewalld提供的工具都使用该接口，例如firewall-cmd，firewall-config和firewall-applet。 firewalld正在运行时可以使用firewall-offline-cmd，但不推荐使用它，因为有大概5秒钟的延迟。 配置配置目录firewalld支持两个配置目录： /etc/firewalld/ ：用户配置文件目录 /usr/lib/firewalld：系统配置文件目录，即默认配置文件目录 使用时，当需要一个文件时，firewalld会首先到第一个目录进行查找，如果可以找到，那么就直接使用，否则会继续到第二个目录中查找。 这么做的好处： /usr/lib/firewalld存放的是firewalld提供的通用配置文件，如果想修改配置，可以直接拷贝一份到/etc/firewalld/，再进行修改，节省编写配置的时间。 如果想恢复默认配置，只需删除/etc/firewalld/目录下的配置文件即可。 配置状态 配置状态分为两种： 运行时配置：运行时配置是实际有效配置，并应用于内核防火墙。在firewalld服务启用时，永久配置将成为运行时配置。但运行时配置的更改不会自动保存到永久配置中。 永久配置：永久配置存储在配置文件中，并将在每次服务启动、重启、重载时成为新的运行时配置。 将运行时配置更改为永久配置的命令： 1firewall-cmd --runtime-to-permanent 配置文件这里说的配置文件仅仅是说firewalld.conf。 firewalld.conf是/etc/firewalld目录下提供firewalld的基本配置，如果/etc/firewalld目录下不存在该文件，将使用firewalld内部默认值。 12345678910111213141516171819202122232425262728# 默认区域# 默认： publicDefaultZone=public# 标记的最小值，linux内核会对每个进入的数据包都进行标记，目的是为了对它们进行区分。# 默认：100MinimalMark=100# 退出firewalld后是否清除防火墙规则# 默认：yesCleanupOnExit=yes# 是否锁定firewalld D-BUS接口，当Lockdown为yes时可以通过lockdown-whitelist.xml文件来限制都有哪些程序可以通过D-BUS接口对firewalld进行操作# 默认：no（没有限制）Lockdown=no# 判断所接受到的包是否是伪造的，检查方式主要是通过路由表中的路由条目实现的，更多详细的信息可以搜索uRPF相关的资料# 默认：yesIPv6_rpfilter=yes# 设置为yes时不使用combined-restore调用而是单独调用，增加了应用和启用服务所需的时间，对调试有帮助# 默认：noIndividualCalls=no# 在拒绝之前添加日志记录规则# 默认：off# 可选：all unicast broadcast multicastLogDenied=off 区域通过将网络划分成不同的区域，制定出不同区域之间的访问控制策略来控制不同程序区域间传送的数据流。 firewalld默认有9个Zone： drop（丢弃区域）：任何接收的网络数据包都被丢弃，没有任何回复，只能传出网络连接。 block（限制区域）：任何接受的网络连接都被IPv4的icmp-host-prohibited信息和IPv6的icmp6-adm-prohibited信息所拒绝。 piblic（公共区域）：在公共区域内使用，不能相信网络内其他计算机不会对你造成危害，只能接受经过选取的连接。 external（外部区域）：不能信任来自网络的其他计算机，不能相信它们不会对你造成伤害，只能接受经过选取的连接。 dmz（隔离区域）： 隔离区域也称为非军事区域，内外网络之间增加的一层网络，起到缓冲作用。对于隔离区域，只能接受经过选取的连接。 work（工作区域）：用于工作区，你可以基本信任网络内的其他电脑不会对你造成危害，仅仅接收经过选择的连接。 home（家庭区域）：用于内部网络，你可以基本上信任网络内其他电脑不会对你造成危害，仅仅接收经过选择的连接。 internal（内部区域）：用于内部网络，你可以基本上信任网络内其他电脑不会对你造成危害，仅仅接收经过选择的连接。 trusted（信任区域）：可接受所有的网络连接。 tagzone规则中首先最重要的是target的设置，可以取四个值：default、ACCEPT、%%REJECT%%、DROP。 1234# 查看指定Zone的target&gt; firewall-cmd --zone=work --get-target --permanent# 更改指定Zone的target&gt; firewall-cmd --zone=work --set-target=ACCEPT --permanent –permanent 表示是否将修改后的规则保存下来，如果不加这个参数，那么所做的修改当时会立即生效，但是在firewalld重启之后就会丢失，而加上这个参数后所做的修改就会永久保存下来，不过这时的修改不会立即生效而是需要reload后才可以生效。其实这个也非常容易理解，当不加--permanent修改规则时firewalld会实际修改运行时的规则，而如果加了这个参数firewalld其实是去修改的xml配置文件，和我们直接编辑xml文件一样，所以就需要reload才可以生效。 123456&lt;zone target="ACCEPT"&gt; &lt;short&gt;Work&lt;/short&gt; &lt;description&gt;For use in work areas. You mostly trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted.&lt;/description&gt; &lt;service name="ssh"/&gt; &lt;service name="dhcpv6-client"/&gt;&lt;/zone&gt; 服务服务就是对端口、协议和目的地址的封装。 这样做的好处： 通过服务名字来管理规则更加人性化 通过服务来组织端口分组的模式更加高效，如果一个服务使用了若干个网络端口，则服务的配置文件就相当于提供了到这些端口的规则管理的批量操作快捷方式 service配置文件的命名规则是&lt;服务名&gt;.xml。 默认保存在/usr/lib/firewalld/services/目录下，常见的服务其中都可以找到，如果我们想修改某个服务的配置，那么可以复制一份到/etc/firewalld/services/目录下然后进行修改就可以了。 123456&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;service&gt; &lt;short&gt;MySQL&lt;/short&gt; &lt;description&gt;MySQL Database Server&lt;/description&gt; &lt;port protocol="tcp" port="3306"/&gt;&lt;/service&gt; 个人觉得，firewalld中的service与iptables的自定义链有些类似，都是用来分类、方便管理针对的某些服务的。 ipset创建匹配整个地址的集合。 12345678&lt;?xml version="1.0" encoding="utf-8"?&gt; &lt;ipset type="hash:net"&gt; &lt;short&gt;white-list&lt;/short&gt; &lt;entry&gt;192.168.198.21&lt;/entry&gt; &lt;entry&gt;192.168.198.22&lt;/entry&gt; &lt;entry&gt;192.168.198.23&lt;/entry&gt; &lt;entry&gt;192.168.199.0/24&lt;/entry&gt; &lt;/ipset&gt; 12345&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;ipset type="hash:mac"&gt; &lt;short&gt;mac-list&lt;/short&gt; &lt;entry&gt;00:11:22:33:44:55&lt;/entry&gt;&lt;/ipset&gt; 更多支持的类型查看 1ipset --help 规则过滤规则 source：根据源地址过滤 interface：根据网卡过滤 service：根据服务名过滤 port：根据端口过滤 icmp-block：icmp 报文过滤，按照 icmp 类型配置 masquerade：ip 地址伪装 forward-port：端口转发 rule：自定义规则 优先级过滤规则的优先级遵循如下顺序： source interface firewalld.conf中配置的默认zone 命令查看规则 查看运行状态 1&gt; firewall-cmd --state 查看已被激活的Zone信息 123&gt; firewall-cmd --get-active-zonespublic interfaces: ens33 查看指定接口的Zone信息 1&gt; firewall-cmd --get-zone-of-interface=ens33 查看指定Zone的接口 1&gt; firewall-cmd --zone=public --list-interfaces 查看指定Zone的所有信息 1234567891011121314&gt; firewall-cmd --zone=public --list-allpublic (active) target: default icmp-block-inversion: no interfaces: ens33 sources: services: ssh dhcpv6-client ports: protocols: masquerade: no forward-ports: source-ports: icmp-blocks: rich rules: 查询指定Zone被放行的服务（即运行时状态） 12&gt; firewall-cmd --zone=public --list-servicesssh dhcpv6-client 查询指定Zone永久放行的服务（即永久状态） 12&gt; firewall-cmd --zone=public --list-services --permanentssh dhcpv6-client 配置规则sourceinterfaceservice对服务进行操作。 服务就是对端口、协议和目的地址的封装。 详细定义一个服务： 123&gt; cd /etc/firewalld/services/&gt; cp /usr/lib/firewalld/services/mysql.xml .&gt; vim mysql.xml 加--permanent相关操作是对文件进行操作，不加--permanent对当前运行状态配置进行操作。 1234&lt;!-- 配置文件中指定服务 --&gt;&lt;zone&gt; &lt;service name="string"/&gt;&lt;/zone&gt; 12345678910# 查看public区域永久放行的服务&gt; firewall-cmd --zone=public --list-services --permanent# 添加一个放行服务&gt; firewall-cmd --zone=public --add-service=mysql# 添加一个放行服务，设置超时时间为600s，注意--timeout无法与--permanent一起使用，即无法写入指定zone文件&gt; firewall-cmd --zone=public --add-service=vnc-server --timeout=600# 移除放行服务&gt; firewall-cmd --zone=public --remove-service=mysql --permanent# 根据某个是否放行返回一个布尔值&gt; firewall-cmd --zone=public --query-service=mysql port对端口进行操作。 1234&lt;!-- 配置文件中指定端口号 --&gt;&lt;zone&gt; &lt;port port="portid[-portid]" protocol="tcp|udp"/&gt;&lt;/zone&gt; 12345678# 查看指定public区域放行的端口号&gt; firewall-cmd --zone=public --list-ports# 添加一个放行端口&gt; firewall-cmd --zone=public --add-port=47017/tcp# 添加一个放行端口，设置超时时间为600s&gt; firewall-cmd --zone=public --add-port=22/tcp --timeout=600s# 移除放行端口&gt; firewall-cmd --zone=public --remove-port=22/tcp icmp-blockmasqueradeforward-portrule]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-SSH]]></title>
    <url>%2F2018%2F08%2F09%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Iptables%2F</url>
    <content type="text"><![CDATA[Iptables笔记理解流程，理解表链、知道如何制定规则。 :globe_with_meridians:readhat官网文档 概念iptables并不是真正的防火墙，只是一个客户端代理，用户通过iptables代理将用户的安全设定执行到对应的安全框架中，这个安全框架才是真正的防火墙，叫netfilter。 netfilter/iptables组成Linux平台下的包过滤防火墙，完成封包过滤、封包重定向、网络地址转换（NAT）等功能。 netfilter才是防火墙真正的安全框架（framework），位于内核空间。 iptables是一个命令行工具，位于用户空间。 用iptables命令行工具操作netfilter安全框架。 netfilter是一个Linux操作系统核心层内部的一个数据包处理模块，具有以下功能： 网络地址转换（Network Address Translate） 数据包内容修改 数据包过滤 所以说，iptables启动后并没有一个守护进程，不能算是真正意义上的服务，应该算是内核提供的功能。 原理简单的数据流向，稍后再流程中再详细讲完整的流程图。 表链Iptables包含四表五链，其实是对规则的分类。 表是对数据包的操作进行分类，可以理解为每个表代表不同的功能。 链是对不同的hook点进行分类，可以理解为每个链代表不同的关卡。 表链实际上是netfilter的两个维度。 表四表五链中的四表： filter：负责过滤功能，防火墙 nat：负责网络地址转换功能 mangle：拆解报文，做出修改，并重新封装功能 raw：关闭nat表上启用的连接追踪机制 链四表五链中的五链： PREROUTING：数据包进入路由表之前 INPUT：通过路由表后目的地为本机 FORWARD：通过路由表后，目的地不为本机 OUTPUT：由本机产生，向外转发 关系链表关系（方便理解） 在理解iptables时，通常会用链表关系来进行理解。 每个链中的规则都存在于哪些表中（即每个关卡可以拥有哪些功能）： PREROUTING链的规则可以存在于：raw表、mangle表、nat表 INPUT链的规则可以存在于：mangle表、filter表、nat表（仅限CentOS 7，CentOS 6中没有） FORWORD链的规则可以存在于：mangle表、filter表 OUTPUT链的规则可以存在于：raw表、mangle表、nat表、filter表 POSTROUTING链的规则可以存在于：mangle表，nat表 表链关系（方便操作） 实际使用过程中，往往是通过表为操作入口，对规则进行定义的。 每个表中的规则可以被哪些链应用（即哪些功能可以被哪个关卡应用）： raw表的规则可以被哪些链使用：PREROUTING、OUTPUT mangle表中的规则可以被哪些链使用：PREROUTING、INPUT、FORWORD、OUTPUT、POSTROUTING nat表中的规则可以被哪些链使用：PREROUTING、INPUT（仅限CentOS 7，CentOS 6没有）、OUTPUT、POSTROUTING filter表中的规则可以被哪些链使用：INPUT、FORWORD、OUTPUT、 顺序在某条链存在多个表时（即某个关卡存在多个功能），执行的优先级如下： raw → mangle → nat → filter 规则概念匹配即停： 根据指定的匹配条件尝试匹配每个经过此处的报文，一旦匹配成功，则由规则后面的处理动作进行处理。 匹配条件：原地址、目的地址、源端口、目标端口等 此处：链（关卡） 报文：数据块 处理动作：允许、拒绝、丢弃、转换、映射等 匹配条件基本匹配条件： 源地址：Source IP 目的地址：Destination IP 扩展匹配条件：（需要依赖对应的扩展模块） 源端口：Source Port 目标端口：Destination Port 处理动作 ACCEPT：允许数据包通过 DROP：直接丢弃数据包，不给任何回应信息，客户端的请求石沉大海，过了超时时间才会有反应 REJECT：拒绝数据包通过，必要时会给数据发送端一个响应信息，客户端刚请求就会收到拒绝的信息 SNAT：源地址转换，解决内网用户用同一个公网地址上网的问题 MASQUERADE：是SNAT的一种特殊形式，适用于动态的，临时会变的ip上 DNAT：目标地址转换 REDIRECT：本地做端口映射 LOG：在/var/log/messages文件中记录日志信息，然后将数据包传递给下一个规则。就是除了记录以外不对数据包做任何其他操作，仍然让下一条规则去匹配 总结匹配流程]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-SSH]]></title>
    <url>%2F2018%2F08%2F09%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-SSH%2F</url>
    <content type="text"><![CDATA[SSH学习笔记只记录相关命令。 SSH使用秘钥认证相关命令 交互式生成秘钥对 1ssh-keygen 指定秘钥对生成位置与名称 1ssh-keygen -f /opt/ssh/id_rsa ssh连接到对应主机时，指定对应的秘钥 1ssh -i /opt/ssh/id_rsa 直接生成秘钥对并设置密码 1ssh-keygen -P '123456' -f ~/.ssh/id_rsa 为私钥添加密码、取消密码、修改密码 1ssh-keygen -f ~/.ssh/id_rsa -p 生成指定类型的密钥 1ssh-keygen -t dsa -P '' -f ~/.ssh/id_rsa 生成指定位数的密钥 1ssh-keygen -t rsa -b 1024 -P '' -f ~/.ssh/id_rsa 根据私钥生成公钥 12ssh-keygen -f ~/.ssh/id_rsa -yssh-keygen -f ~/.ssh/id_rsa -y &gt; id_rsa.pub 将公钥加入到指定账户的认证文件中 123ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.198.22ssh-copy id -i ~/.ssh/id_rsa.pub root@192.168.198.22 -p 2222cat ~/.ssh/id_rsa.pub | ssh -p 22 root@192.168.198.22 "umask 077;mkdir -p ~/.ssh;cat - &gt;&gt; ~/.ssh/authorized_keys" 相关问题 ssh服务端不支持基于公钥认证，或者修改了默认的公钥认证文件。 服务端可以禁用使用公钥认证的机制，PubkeyAuthentication配置项用于控制是否可以使用公钥进行认证，PubkeyAuthentication默认为yes，表示支持公钥认证，默认使用authorized_keys作为存储用户公钥的文件，在/etc/ssh/sshd_config配置文件中可以使用AuthorizedKeysFile配置项指定其他文件为认证文件。 ssh服务端相关目录或者文件权限过大。 服务端用户宿主目录.ssh目录的权限正常为700，authorized_keys文件权限默认为600 ssh服务端的authorized_keys文件中包含Windows字符。 手动复制粘贴公钥内容到authorized_keys并且操作时经过Windows系统处理，会出现这种情况。 ssh客户端连接服务端时，没有指定对应账户的用户名。 连接时最好确认客户端的用户，否则默认使用服务端的当前用户。 ssh客户端私钥权限过大。 正常情况下，将私钥设置为600即可。 SSH代理相关命令 启动ssh-agent 1234567# 方式一：创建子shell，退出子shell自动结束代理ssh-agent $SHELL# 通常情况，默认shell为bash，可以直接指定bashssh-agent bash# 方式二：启动进程，退出shell时最好关闭对应代理eval 'ssh-agent' 关闭ssh-agent 1ssh-agent -k 将私钥添加到ssh代理 12ssh-add ~/.ssh/id_rsassh-add &lt;私钥文件路径&gt; 查看代理中的私钥 1ssh-add -l 查看代理中的私钥对应的公钥 1ssh-add -L 移除代理中指定的私钥 1ssh-add -d &lt;私钥文件路径&gt; 移除代理中所有私钥 1ssh-add -D 锁定ssh代理 12# 锁定时需要指定锁定密码，锁定后ssh代理暂时不能管理私钥ssh-add -x 解锁ssh代理 12# 解锁时需要输入锁定时设置的密码，解锁后ssh代理可正常工作ssh-add -X 代理转发 客户端：修改配置 12# 修改ssh_config（不是sshd_config）ForwardAgent yes 客户端：启动ssh代理 12ssh-agent bashssh-add ~/.ssh/id_rsa 代理端：修改配置 12# 修改sshd_config（不是ssh_config）AllowAgentForwarding yes # 保持默认即可 SSH隧道本地转发本地主机：ssh客户端，应用客户端 远程主机：ssh服务端，应用服务端 12345# 建立ssh隧道，在本地回环地址监听9999ssh -L 9999:192.168.198.22:3306 root@192.168.198.22ssh -f -N -L 9999:192.168.198.22:3306 root@192.168.198.22# 通过ssh隧道连接MySQL服务端mysql -h127.0.0.1 -P9999 -uroot 远程转发本地主机：ssh客户端，应用服务端 远程主机：ssh服务端，应用客户端 理解为ssh反向隧道、ssh逆向隧道 12345# 建立ssh隧道，在本地回环地址监听9999ssh -f -N -R 9999:192.168.198.22:3306 root@192.168.198.21# 通过ssh隧道连接MySQL服务端mysql -h127.0.0.1 -P9999 -uroot 相关命令 本地转发 1ssh -g -f -N -L forwardingPort:targetIP:targetPort user@sshServerIP 远程转发 1ssh -f -N -R forwardingPort:targetIP:targetPort user@sshServerIP 常用选项： -L：使用本地端口转发创建链接 -R：使用远程端口转发创建链接 -N：创建隧道后不连接到SSH服务端，通常与-f选项连用 -f：在后台运行ssh隧道，通常与-N选项连用 -g：开启网关模式，在远程端口转发中，无法开启网关功能]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible学习笔记-Roles]]></title>
    <url>%2F2018%2F08%2F07%2FAnsible%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E8%A7%92%E8%89%B2%2F</url>
    <content type="text"><![CDATA[Ansible Roles学习笔记在Ansible中，Role是将Playbook分割为多个文件的主要机制。 role并不是某一个具体的东西，而是一个规范与抽象，Ansible将独立的功能抽象成role，并将该功能依赖的文件、变量、模板、Playbook以统一的格式进行规范化组织，以此简化了复杂的Playbook的编写，同样增加了Playbook的可维护性。 Galaxy网站:globe_with_meridians:Galaxy 类似于Docker Hub，是一个开源网站。用于查找，下载和共享社区开发的角色。从Galaxy下载角色是快速启动自动化项目的好方法。 还可以使用该站点来共享您创建的角色。通过使用您的GitHub帐户对站点进行身份验证，您可以导入角色，使其可供Ansible社区使用。导入的角色在Galaxy搜索索引中可用，并在站点上可见，允许用户发现和下载它们。 还可以在公司或个人的Galaxy服务器上下载角色，默认使用服务器地址https://galaxy.ansible.com与Galaxy网站API进行通信，需要更改ansible.cfg配置文件来修改Galaxy服务器地址。 :globe_with_meridians: Ansible Github也提供了一些最佳实践的例子，可以在学习或者工作的时候进行参考。 Galaxy命令用法Roles目录结构 1234567891011121314151617181920212223242526272829303132.├── group_vars│ ├── all│ └── dbservers├── hosts├── LICENSE.md├── README.md├── roles│ ├── common│ │ ├── handlers│ │ │ └── main.yml│ │ ├── tasks│ │ │ └── main.yml│ │ └── templates│ │ └── ntp.conf.j2│ ├── db│ │ ├── handlers│ │ │ └── main.yml│ │ ├── tasks│ │ │ └── main.yml│ │ └── templates│ │ └── my.cnf.j2│ └── web│ ├── handlers│ │ └── main.yml│ ├── tasks│ │ ├── copy_code.yml│ │ ├── install_httpd.yml│ │ └── main.yml│ └── templates│ └── index.php.j2└── site.yml]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F08%2F04%2FAnsible%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%89%A7%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[Playbook笔记Ansible提供两种完成任务的方式： ansible — Ad-hoc，解决一些简单或者临时遇到的任务，相当于Linux的系统命令行下的shell命令 ansible-playbook — Playbook，解决复杂或者固化下来的任务，相当于Linux系统的Shell脚本 这次记录Palybook的使用方法。 Playbook命令用法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990Usage: ansible-playbook [options] playbook.yml [playbook2 ...]Options:# vault密码 --ask-vault-pass# 模拟任务运行，并不会真正执行。 -C, --check# 当更改（小的）文件和模板时，显示这些文件的差异，大文件使用-- -D, --diff# 设置外部变量，如果是变量文件在文件前面加'@' -e EXTRA_VARS, --extra-vars=EXTRA_VARS# 清除每个远程主机上的facts缓存 --flush-cache# 即使任务失败也强制运行handlers任务 --force-handlers# 指定并发数，默认为5 -f FORKS, --forks=FORKS# 帮助信息 -h, --help# 指定inventory文件 -i INVENTORY, --inventory=INVENTORY, --inventory-file=INVENTORY# 进一步限制所选主机/组模式 -l SUBSET, --limit=SUBSET# 列出主机列表，不执行任务 --list-hosts# 列出所有可用标记 --list-tags# 列出所有将要执行的任务 --list-tasks# 执行模块的存放路径 -M MODULE_PATH, --module-path=MODULE_PATH# 跳过指定标签的任务 --skip-tags=SKIP_TAGS# 从匹配的任务名称开始进行任务 --start-at-task=START_AT_TASK# 在运行前确认每个要执行的任务，一步一步的执行 --step # yml语法监测 --syntax-check# 仅运行指定的tags任务 -t TAGS, --tags=TAGS# 指定vault id --vault-id=VAULT_IDS# 指定vault password文件 --vault-password-file# 详细模式，-vvv表示更多，-vvvv表示启用连接调试 -v, --verbose # 版本信息 --version show program's version number and exit Connection Options:# ssh认证密码 -k, --ask-pass # 指定秘钥文件 --private-key=PRIVATE_KEY_FILE, --key-file=PRIVATE_KEY_FILE# 指定远程主机运行命令的用户，默认None（以当前系统用户） -u REMOTE_USER, --user=REMOTE_USER# 指定连接方式，默认smart -c CONNECTION, --connection=CONNECTION# 连接远程主机超时时间，单位秒，默认为10秒 -T TIMEOUT, --timeout=TIMEOUT# 指定sftp/scp/ssh连接时的参数（例如ProxyCommand） --ssh-common-args=SSH_COMMON_ARGS# 指定sftp的额外参数（例如-f,-l） --sftp-extra-args=SFTP_EXTRA_ARGS# 指定scp的额外参数（例如-l） --scp-extra-args=SCP_EXTRA_ARGS# 指定ssh的额外参数（例如-R） --ssh-extra-args=SSH_EXTRA_ARGS Privilege Escalation Options:# 相当于linuxsudu命令，不建议使用，建议使用become -s, --sudo# 所需的sudo用户，不建议使用，建议使用become -U SUDO_USER, --sudo-user=SUDO_USER# 使用su来运行任务，不建议使用，建议使用become -S, --su # 使用su来作为这个用户运行任务，不建议使用，建议使用become -R SU_USER, --su-user=SU_USER# 在运行命令时告诉可以成为另一个用户，没有密码提示 -b, --become# 指定升级特权时的方法，默认sudo，可以选择的有[ sudo | su | pbrun | pfexec | doas | dzdo | ksu | runas | pmrun | enable | machinectl ] --become-method=BECOME_METHOD# 指定运行任务的用户，默认root用户 --become-user=BECOME_USER# sudo密码，不建议使用，建议使用become --ask-sudo-pass# su密码，不建议使用，建议使用become --ask-su-pass# 请求权限提升的密码，即使用become时所需的密码 -K, --ask-become-pass 权限remote_user指定Play用户。 12345678---- hosts: test remove_user: root tasks: - name: test connection ping: # 为单个任务细分执行的用户 remote_user: gaopeng 也可以在ansible.cfg中配置连接远程服务器的默认用户。 become、become_method提权。 1234567891011---- hosts: test remote_user: gaopeng become: yes tasks: - service: name: nginx state: started # 为单个任务使用管理员权限 become: yes become_method: sudo 通知handlers是Ansible提供的条件机制，与tasks比较类似，都是去执行某些操作。但是，handler只有在被notify触发以后才会执行，如果没有被触发，则不会执行。 12345678910111213141516---- hosts: env2 remote_user: root tasks: - name: Modify the Nginx configuration replace: path=/etc/nginx/nginx.conf regexp="listen(.*) 80 (.*)" replace="listen\1 8088 \2" backup=yes notify: restart nginx - name: Modify the MySQL configuration replace: path=/etc/my.cnf regexp="\b3306\b" replace="6033" backup=yes notify: restart mysql handlers: - name: restart nginx service: name=nginx state=restarted - name: restart mysql service: name=mysqld state=restarted 在使用Handler的过程中，有以下几点需要额外注意： Handler只有在其所在的任务被执行时，才会被运行；如果一个任务中定义了notify去调用Handler，但是由于条件判断等原因，该任务未被执行，那么Handler同样不会被执行。 Handler只会在Play的末尾运行一次；如果想在一个Playbook的中间运行Handler，刚需要使用meta模块来实现，例如： 1- meta: flush_handlers 如果一个Play在运行到调用Handler的语句之前失败了，那么这个Handler将不会被执行。我们可以使用mega模块的—force-handlers选项来强制执行Handler，即使是Handler所在的Play中途运行失败。 标签tags可以对任务进行打标签的操作，当任务存在标签以后，就可以在执行playbook时，借助标签，指定执行哪些任务，或者指定不执行哪些任务。 12345678910111213141516171819---- name: Install ntp yum: name=ntp state=present tags: ntp- name: Configure ntp file template: src=ntp.conf.j2 dest=/etc/ntp.conf tags: ntp notify: restart ntp- name: Start the ntp service service: name=ntpd state=started enabled=yes # 多个标签 tags: ntp,service- name: test to see if selinux is running command: getenforce register: sestatus changed_when: false 执行包括ntp标签的任务： 12&gt; ansible-playbook --tags ntp ntp.yml&gt; ansible-playbook --tags ntp,service ntp.yml 不执行包括ntp标签的任务： 1&gt; ansible-playbook --skip-tags ntp.yml 特殊标签 always（yaml文件中使用） — 当任务的tags值指定为always时，这个任务总是会被执行，除非使用--skip-tags明确指定不执行对应的任务 never （yaml文件中使用）— 与always作用相反 tagged（ansible-playbook命令使用）— 仅执行（或跳过）有标签的任务 untagged（ansible-playbook命令使用）— 仅执行（或跳过）没有标签的任务 all（ansible-playbook命令使用）— 默认使用，所有任务都会被执行 变量用户自定义变量 定义变量 1234---- hosts: env2 vars: login_port: 3306 放入单独的文件中 1234---- hosts: env2 vars: - vars/server_vars.yml Facts变量直接使用。 1&gt; ansible env2 -m setup 关闭Facts： 123---- hosts: env2 gather_facts: no 注册变量把任务执行结果当做一个变量的值。 1234- name: Test database connection shell: mysql -P&#123;&#123; mysql_port &#125;&#125; -u&#123;&#123; mysql_root_user &#125;&#125; -p&#123;&#123; mysql_root_pass &#125;&#125; -e "use mysql;" register: mysql_connect_result ignore_errors: True 命令行变量执行时传入的额外变量。 12345&gt; ansible-playbook test.yml --extra-vars "login_port=3306"# Json格式&gt; ansible-playbook test.yml --extra-vars "&#123;'login_port': '3306'&#125;"# Json文件&gt; ansible-playbook test.yml --extra-vars "@vars.json" 变量优先级由低到高： role defaults inventory file or script group vars inventory group_vars playbook group_vars inventory group_vars playbook group_vars inventory file or script host vars inventory host_vars playbook host_vars host facts / cached set_facts play vars play vars_prompt play vars_files role vars (defined in role/vars/main.yml) block vars (only for tasks in block) task vars (only for the task) include_vars set_facts / registered vars role (and include_role) params include params extra vars (always win precedence) role defaults 在roles/x/defaults/main.yml文件中定义的变量。 123---# file roles/mysql/defaults/main.ymlmysql_port: 3306 inventory vars 在inventory文件中定义的变量。 123# file: /etc/ansible/hosts[mysql]192.168.198.22 mysql_port=3306 ntp_server=192.168.198.21 role=master inventory group_vars 有两个地方可以定义group_vars： inventory文件中直接定义。 inventory文件同级的子文件夹group_vars下定义，放在group同名的文件中。 1234567# inventory文件中直接定义[all:vars]ntp_server=192.168.198.21# inventory文件同级的子文件夹group_vars下定义---ntp_server=192.168.198.21 循环标准循环可以理解为Python中的列表循环，也等同于旧版本的with_list。 items中元素为字符串用法 1234567891011121314151617181920# loop用法---- hosts: env2 remote_user: root gather_facts: no tasks: - debug: msg: "&#123;&#123; item &#125;&#125;" loop: [1, 2, 3]# 旧版本---- hosts: env2 remote_user: root gather_facts: no tasks: - debug: msg: "&#123;&#123; item &#125;&#125;" with_list: [1, 2, 3] 123# 效果等同于Python中的list循环for i in [1, 2, 3]: print(i) 输出结果为： 123456789ok: [env2] =&gt; (item=1) =&gt; &#123; &quot;msg&quot;: 1&#125;ok: [env2] =&gt; (item=2) =&gt; &#123; &quot;msg&quot;: 2&#125;ok: [env2] =&gt; (item=3) =&gt; &#123; &quot;msg&quot;: 3&#125; items中元素为字典用法： 1234567891011---- hosts: env2 gather_facts: False tasks: - name: yum install package yum: name=&#123;&#123; item.name &#125;&#125; state=&#123;&#123; item.state &#125;&#125; loop: - &#123;name: "tree", state: "latest"&#125; - &#123;name: "openssh-clients", state: "present"&#125; - &#123;name: "vim", state: "latest" &#125; - &#123;name: "mysql", state: "present" &#125; 12345# 同样相当于Python中的list循环items = [&#123;'name': 'tree', 'state': 'latest'&#125;, &#123;'name': 'vim', 'state': 'present'&#125;]for item in items: name = item['name'] state = item['state'] 1234567891011# 测试---- hosts: env2 remote_user: root gather_facts: no tasks: - debug: msg: "&#123;&#123; item.key &#125;&#125;, &#123;&#123; item.value &#125;&#125;" loop: - &#123;key: 1, value: 2&#125; - &#123;key: 3, value: 4&#125; 输出结果为： 123456ok: [env2] =&gt; (item=&#123;u&apos;key&apos;: 1, u&apos;value&apos;: 2&#125;) =&gt; &#123; &quot;msg&quot;: &quot;1, 2&quot;&#125;ok: [env2] =&gt; (item=&#123;u&apos;key&apos;: 3, u&apos;value&apos;: 4&#125;) =&gt; &#123; &quot;msg&quot;: &quot;3, 4&quot;&#125; 嵌套循环可以理解为Python中product(it1, … , iteN, repeat=1)，等同于旧版本的with_nested/with_cartesian。 12345678910111213141516171819202122232425262728# product用法---- hosts: env2 gather_facts: no vars: list_one: - 1 - 2 - 3 list_two: - a - b - c tasks: - debug: msg: "&#123;&#123; item &#125;&#125;" loop: "&#123;&#123; list_one |product(list_two)|list &#125;&#125;" # 旧版本---- hosts: env2 gather_facts: no tasks: - debug: msg: "&#123;&#123; item &#125;&#125;" with_cartesian: - [1, 2, 3] - ['a', 'b', 'c'] 12345678910111213141516# 效果等同于Python中的product，计算笛卡尔积，从输入的各个可迭代对象中获取元素，合并成由N个元素组成的元组from itertools import productlist_one = [1, 2, 3]list_two = ['a', 'b', 'c']for i in product(list_one, list_two): print(i)# 更直白的方式，使用列表推导来计算笛卡尔积list_one = [1, 2, 3]list_two = ['a', 'b', 'c']items = [(x, y) for x in list_one for y in list_two]for i in product(list_one, list_two): print(i) 并行循环可以理解为Python中的zip(it1, …, itN)、zip_longest(it1, …, itN, fillvalue=None)，等同于旧版本的with_together。 with_together仅等同于zip_longest用法，一下会举例说明。 zip用法： 123456789101112# zip用法，旧版本没有此对应用法---- hosts: env2 gather_facts: no vars: list_one: [1, 2, 3, 4, 5] list_two: [a, b, c] list_three: [x, y, z, d] tasks: - debug: msg: "&#123;&#123; item &#125;&#125;" loop: "&#123;&#123; list_one|zip(list_two, list_three)|list &#125;&#125;" 输出结果为：（注意list_one中的4、5与list_three中的d都没有输出） 123456789101112131415161718192021ok: [env2] =&gt; (item=[1, u&apos;a&apos;, u&apos;x&apos;]) =&gt; &#123; &quot;msg&quot;: [ 1, &quot;a&quot;, &quot;x&quot; ]&#125;ok: [env2] =&gt; (item=[2, u&apos;b&apos;, u&apos;y&apos;]) =&gt; &#123; &quot;msg&quot;: [ 2, &quot;b&quot;, &quot;y&quot; ]&#125;ok: [env2] =&gt; (item=[3, u&apos;c&apos;, u&apos;z&apos;]) =&gt; &#123; &quot;msg&quot;: [ 3, &quot;c&quot;, &quot;z&quot; ]&#125; 1234567# 效果等同于Python中的zip，并行从输入的各个可迭代对象中获取元素，产生由N个元素组成的元组，只要有一个可迭代对象到头了，就默默停止list_one = [1, 2, 3]list_two = ['a', 'b', 'c', 'd']items = zip(list_one, list_two)for i in zip(list_one, list_two): print(i) zip_longest用法： 123456789101112131415161718192021222324# zip_longest用法---- hosts: env2 gather_facts: no vars: list_one: [1, 2, 3, 4, 5] list_two: [a, b, c, d] list_three: [x, y, z] tasks: - debug: msg: "&#123;&#123; item &#125;&#125;" loop: "&#123;&#123; list_one|zip_longest(list_two, list_three)|list &#125;&#125;" # 旧版本---- hosts: env2 gather_facts: no tasks: - debug: msg: "&#123;&#123; item &#125;&#125;" with_together: - [1, 2, 3, 4, 5] - [a, b, c, d] - [x, y, z] 输出结果为：（注意list_one中的4、5与list_three中的d都被输出） 1234567891011121314151617181920212223242526272829303132333435ok: [env2] =&gt; (item=[1, u&apos;a&apos;, u&apos;x&apos;]) =&gt; &#123; &quot;msg&quot;: [ 1, &quot;a&quot;, &quot;x&quot; ]&#125;ok: [env2] =&gt; (item=[2, u&apos;b&apos;, u&apos;y&apos;]) =&gt; &#123; &quot;msg&quot;: [ 2, &quot;b&quot;, &quot;y&quot; ]&#125;ok: [env2] =&gt; (item=[3, u&apos;c&apos;, u&apos;z&apos;]) =&gt; &#123; &quot;msg&quot;: [ 3, &quot;c&quot;, &quot;z&quot; ]&#125;ok: [env2] =&gt; (item=[4, u&apos;d&apos;, None]) =&gt; &#123; &quot;msg&quot;: [ 4, &quot;d&quot;, null ]&#125;ok: [env2] =&gt; (item=[5, None, None]) =&gt; &#123; &quot;msg&quot;: [ 5, null, null ]&#125; 123456789# 效果等同于Python中的zip_longest，并行从输入的各个可迭代对象中获取元素，产生由N个元素组成的元组，等到最长的可迭代对象到头后才停止，空缺的值使用fillvalue填充（默认为None）from itertools import zip_longestlist_one = [1, 2, 3]list_two = ['a', 'b', 'c', 'd']items = zip(list_one, list_two)for i in zip_longest(list_one, list_two): print(i) 当然也可以使用fillvalue修改默认值 1234567891011---- hosts: env2 gather_facts: no vars: list_one: [1, 2, 3, 4, 5] list_two: [a, b, c, d] list_three: [x, y, z] tasks: - debug: msg: "&#123;&#123; item &#125;&#125;" loop: "&#123;&#123; list_one|zip_longest(list_two, list_three, fillvalue='不知道')|list &#125;&#125;" 此时输出结果中默认值不在是null了： 1234567891011121314ok: [env2] =&gt; (item=[4, u&apos;d&apos;, u&apos;\u4e0d\u77e5\u9053&apos;]) =&gt; &#123; &quot;msg&quot;: [ 4, &quot;d&quot;, &quot;不知道&quot; ]&#125;ok: [env2] =&gt; (item=[5, u&apos;\u4e0d\u77e5\u9053&apos;, u&apos;\u4e0d\u77e5\u9053&apos;]) =&gt; &#123; &quot;msg&quot;: [ 5, &quot;不知道&quot;, &quot;不知道&quot; ]&#125; 展开循环可以理解为Python中的chain(it1, …, itN)与numpy.flatten，仅仅类似而不是用chain或numpy.flatten去实现的。旧版本为with_flattened、with_items。 with_items用法： 123456789101112131415161718192021222324# flatten(levels=1)用法---- hosts: env2 gather_facts: no vars: list_1: - [1, 2, [3, 4]] - [[a, b], [c, d]] tasks: - debug: msg: "&#123;&#123; item &#125;&#125;" loop: "&#123;&#123; list_1|flatten(levels=1) &#125;&#125;"# 旧版本---- hosts: env2 remote_user: root gather_facts: no tasks: - debug: msg: "&#123;&#123; item &#125;&#125;" with_items: - [1, 2, [3, 4]] - [[a, b], [c, d]] with_flattened用法： 123456789101112131415161718192021222324# flatten用法---- hosts: env2 gather_facts: no vars: list_1: - [1, 2, [3, 4]] - [[a, b], [c, d]] tasks: - debug: msg: "&#123;&#123; item &#125;&#125;" loop: "&#123;&#123; list_1|flatten &#125;&#125;"# 旧版本：---- hosts: env2 remote_user: root gather_facts: no tasks: - debug: msg: "&#123;&#123; item &#125;&#125;" with_flattened: - [1, 2, [3, 4]] - [[a, b], [c, d]] 源码实现： 1234567891011121314151617181920212223242526272829from collections import MutableSequencedef flatten(mylist, levels=None): ret = [] for element in mylist: if element in (None, 'None', 'null'): # ignore undefined items break elif isinstance(element, MutableSequence): if levels is None: ret.extend(flatten(element)) elif levels &gt;= 1: levels = int(levels) - 1 ret.extend(flatten(element, levels=levels)) else: ret.append(element) else: ret.append(element) return retif __name__ == '__main__': list_1 = [[1, 2, [3, 4]],[['a', 'b'], ['c', 'd']]] for i in flatten(list_1, levels=1): print(i) for i in flatten(list_1): print(i) 索引循环可以理解为Python中的enumerate(sequence, [start=0])，旧版本为with_indexed_items。 123456789101112131415161718192021# loop_control中index_var用法---- hosts: env2 gather_facts: no vars: list_1: [a, b, c] tasks: - debug: msg: "&#123;&#123; index &#125;&#125;-&#123;&#123; item &#125;&#125;" loop: "&#123;&#123; list_1|flatten(levels=1) &#125;&#125;" loop_control: index_var: index# 旧版本---- hosts: env2 gather_facts: no tasks: - debug: msg: "&#123;&#123; item.0 &#125;&#125;-&#123;&#123; item.1 &#125;&#125;" with_indexed_items: [a, b, c] 输出结果为： 123456789ok: [env2] =&gt; (item=a) =&gt; &#123; &quot;msg&quot;: &quot;0-a&quot;&#125;ok: [env2] =&gt; (item=b) =&gt; &#123; &quot;msg&quot;: &quot;1-b&quot;&#125;ok: [env2] =&gt; (item=c) =&gt; &#123; &quot;msg&quot;: &quot;2-c&quot;&#125; 1234# 效果等同于Python的emumeratelist_1 = [a, b, c]for i in enumerate(list_1): print(i) 数列循环可以理解为Python中的range(start, stop[, step])，旧版本为with_sequence。 1234567891011121314151617# range用法，为什么写4+1，因为range(4)不会循环到4---- hosts: env2 gather_facts: no tasks: - debug: msg: "&#123;&#123; 'test_%02x' | format(item) &#125;&#125;" loop: "&#123;&#123; range(0, 4 + 1, 2)|list &#125;&#125;"# 旧版本，为什么写4还能循环到4，源码处理过---- hosts: env2 gather_facts: no tasks: - debug: msg: "&#123;&#123; item &#125;&#125;" with_sequence: start=0 end=4 stride=2 format=test_%02x 输出结果为： 123456789ok: [env2] =&gt; (item=test_00) =&gt; &#123; &quot;msg&quot;: &quot;test_00&quot;&#125;ok: [env2] =&gt; (item=test_02) =&gt; &#123; &quot;msg&quot;: &quot;test_02&quot;&#125;ok: [env2] =&gt; (item=test_04) =&gt; &#123; &quot;msg&quot;: &quot;test_04&quot;&#125; 123# 效果等同于Python中的range与format的组合for i in range(4): print('test_%02x' % i) 散列循环可以理解为接受一个Python经过yaml.load的格式变量，通过字典来调取key或value。旧版本为with_dicts。 1234567891011121314151617181920212223242526272829303132333435363738394041# dictsort用法---- hosts: env2 gather_facts: no vars: users: root: root gaopeng: root ansible: ansible tasks: - debug: msg: "&#123;&#123; item.0 &#125;&#125;-&#123;&#123; item.1 &#125;&#125;" loop: "&#123;&#123; users|dictsort &#125;&#125;"# dict2items用法---- hosts: env2 gather_facts: no vars: users: root: root gaopeng: root ansible: ansible tasks: - debug: msg: "&#123;&#123; item.key &#125;&#125;-&#123;&#123; item.value &#125;&#125;" loop: "&#123;&#123; users|dict2items &#125;&#125;"# 老版本---- hosts: env2 gather_facts: no vars: users: root: root gaopeng: root ansible: ansible tasks: - debug: msg: "&#123;&#123; item.key &#125;&#125;-&#123;&#123; item.value &#125;&#125;" with_dicts: "&#123;&#123; users &#125;&#125;" 输出结果为： 123456789ok: [env2] =&gt; (item=[u&apos;ansible&apos;, u&apos;ansible&apos;]) =&gt; &#123; &quot;msg&quot;: &quot;ansible-ansible&quot;&#125;ok: [env2] =&gt; (item=[u&apos;gaopeng&apos;, u&apos;root&apos;]) =&gt; &#123; &quot;msg&quot;: &quot;gaopeng-root&quot;&#125;ok: [env2] =&gt; (item=[u&apos;root&apos;, u&apos;root&apos;]) =&gt; &#123; &quot;msg&quot;: &quot;root-root&quot;&#125; dict2items源码实现： 123456789101112131415from collections import MutableMappingfrom ansible.errors import AnsibleFilterErrordef dict_to_list_of_dict_key_value_elements(mydict): ''' takes a dictionary and transforms it into a list of dictionaries, with each having a 'key' and 'value' keys that correspond to the keys and values of the original ''' if not isinstance(mydict, MutableMapping): raise AnsibleFilterError("dict2items requires a dictionary, got %s instead." % type(mydict)) ret = [] for key in mydict: ret.append(&#123;'key': key, 'value': mydict[key]&#125;) return ret 子元素循环123456789101112131415161718---- hosts: env2 gather_facts: no vars: hosts: - hostname: env1 user: - root - gaopeng - hostname: env2 user: - gaopeng tasks: - debug: msg: "&#123;&#123; item &#125;&#125;" with_subelements: - "&#123;&#123; hosts &#125;&#125;"y7zxc34v1 - user 条件when，类似于编程中的if。]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible学习笔记-Module]]></title>
    <url>%2F2018%2F07%2F31%2FAnsible%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[Ansible常用模块笔记模块大致可分为（目前需要的，以后用到再添加）： 文件模块 命令模块 系统模块 安装包管理 网络工具模块 数据库模块 存储模块 模块分类参考：Ansible官方文档 模块的演示均采用Ad-Hoc模式，使用Playbook时按照固定格式传参即可。 关于Playbook实例相对比较复杂，会在Playbook笔记中整理。 Ansible-doc1234567891011121314151617181920212223Usage: ansible-doc [-l|-F|-s] [options] [-t &lt;plugin type&gt; ] [plugin]Options:# 仅供内部测试，显示所有模块的文档 -a, --all# 帮助信息 -h, --help # 仅供内部测试，以json格式返回所有模块信息 -j, --json# 可用模块列表 -l, --list# 仅显示插件名和模块路径 -F, --list_files# 指定模块默认路径 -M MODULE_PATH, --module-path=MODULE_PATH# 仅显示playbook用到的说明 -s, --snippet# 选择插件类型，默认为模块 -t TYPE, --type=TYPE# 详细信息 -v, --verbose# 版本号 --version 文件相关模块fetch从目标主机上拉取文件。 1ansible-doc fetch -s 参数 默认（可选） 说明 dest（必选） 指定存放文件的目录 src（必选） 指定从控制主机中拉取哪个文件，必须是一个文件，后续版本可能会增加递归拉取 fail_on_missing yes（no） 如果设置为yes，则当源文件不存在就视为失败，如果设置为no，则源文件不存在会视为成功 flat 默认拉取的文件会使用[/主机名/路径/文件名]来保存在指定的目录下，如果设置为flat=yes，则会直接使用[文件名]保存在指定的目录下，如果从多个远程主机拉取，会不断覆盖 validate_checksum yes（no） 在获取文件之后进行md5校验 例： 拉取文件 1234567891011# 执行命令&gt; ansible env2 -m fetch -a 'src=/opt/test dest=/opt/'env2 | SUCCESS =&gt; &#123; "changed": true, "checksum": "90c206af0bfefa95541d3e724efe1dbc1ed3877f", # 保存的路径：在指定的/opt目录下使用[/主机名/路径/文件名]方式保存 "dest": "/opt/env2/opt/test", "md5sum": "8b652b8c79f357694a04bd793f533c96", "remote_checksum": "90c206af0bfefa95541d3e724efe1dbc1ed3877f", "remote_md5sum": null&#125; flat作用 123456789101112# 添加flat=yes参数&gt; ansible env2 -m fetch -a 'src=/opt/test dest=/opt/ flat=yes'# 存放路径发生改变env2 | SUCCESS =&gt; &#123; "changed": true, "checksum": "90c206af0bfefa95541d3e724efe1dbc1ed3877f", # 直接在/opt目录下保存文件名，如果从多个远程主机拉取，会不断覆盖 "dest": "/opt/test", "md5sum": "8b652b8c79f357694a04bd793f533c96", "remote_checksum": "90c206af0bfefa95541d3e724efe1dbc1ed3877f", "remote_md5sum": null&#125; fail_on_missing作用 12345678910111213141516# 获取的文件不存在时执行命令&gt; ansible env2 -m fetch -a 'src=/opt/test_1 dest=/opt/ flat=yes'# 执行失败env2 | FAILED! =&gt; &#123; "changed": false, "msg": "file not found: /opt/test_1"&#125;# 添加fail_on_missing=no参数&gt; ansible env2 -m fetch -a 'src=/opt/test_1 dest=/opt/ flat=yes fail_on_missing=no'# 执行成功env2 | SUCCESS =&gt; &#123; "changed": false, "file": "/opt/test_1", "msg": "the remote file does not exist, not transferring, ignored"&#125; copy拷贝文件至远程主机。 1ansible-doc copy -s 参数 默认（可选） 说明 attributes 为文件添加额外属性，参考chattr命令 backup no（yes） 当远程主机的目标路径中已存在同名文件，并且与控制主机中的文件不同时，是否对远程主机的文件进行备份。设置为yes时，会先备份远程主机中的文件，然后再将控制主机中的文件拷贝到远程主机 checksum no（yes） 是否对传输的文件进行校验 content 当不适用src参数指定拷贝文件时，可以使用content直接指定文件内容，src和content两个参数必有其一 decrypt yes（no） 是否使用vault自动解密 dest（必选） 指定文件将被拷贝到远程主机的目录 directory_mode 系统默认值（centos:0755） 当进行递归复制时，设置目录的模式。 仅针对创建目录时的模式，不会影响已存在的目录 follow no（yes） 是否遵循远程主机的链接。如果设置为yes，如果远程主机的目标路径存在同名的软连接时，则会通过软连接覆盖真实的文件，保留远程主机的软连接。默认为no，直接覆盖软连接，成为真实的文件 force yes（no） 当远程主机的目标路径中已存在同名文件，是否强制覆盖。设置为no时，不会进行进行覆盖，远程主机的同名文件保持不变 group 指定文件拷贝到远程主机后的属组 local_follow yes（no） 是否遵循本地文件的链接 mode 指定文件拷贝到远程主机后的想权限可以为0644或者0775等可以为u+rw，u=rw等可以为preserve，表示和源文件的有相同权限 owner 指定文件拷贝到远程主机后的属主 remote_src no（yes） 如果为no，则在远程主机上搜索src如果为yes，则在远程主机上直接跳转到src只有在mode=preserve时生效 selevel s0 关于selinux设置（不知） serole 关于selinux设置（不知） setype 关于selinux设置（不知） seuser 关于selinux设置（不知） src 用于指定需要拷贝的文件或者目录 unsafe_writes 不安全写入（不知） validate 复制前是否检验需要复制目的地的路径 例： 复制文件或目录 123456789101112131415161718192021222324252627282930313233# 复制文件&gt; ansible env2 -m copy -a 'dest=/opt/ src=/opt/test' env2 | SUCCESS =&gt; &#123; "changed": true, "checksum": "a8fdc205a9f19cc1c7507a60c4f01b13d11d7fd0", "dest": "/opt/test", "gid": 0, "group": "root", "md5sum": "ba1f2511fc30423bdbb183fe33f3dd0f", "mode": "0644", "owner": "root", "size": 4, "src": "/root/.ansible/tmp/ansible-tmp-1533064117.51-270079277825130/source", "state": "file", "uid": 0&#125;# 复制目录，如果在目录后加了/则复制目录中的全部文件，如果不加/则复制整个目录&gt; ansible env2 -m copy -a 'dest=/opt/ src=/opt/ansible'env2 | SUCCESS =&gt; &#123; "changed": true, "checksum": "a8fdc205a9f19cc1c7507a60c4f01b13d11d7fd0", "dest": "/opt/ansible/test", "gid": 0, "group": "root", "md5sum": "ba1f2511fc30423bdbb183fe33f3dd0f", "mode": "0644", "owner": "root", "size": 4, "src": "/root/.ansible/tmp/ansible-tmp-1533064244.07-151699678500881/source", "state": "file", "uid": 0&#125; content用法 123456789101112131415# 将内容写入文件&gt; ansible env2 -m copy -a 'dest=/opt/test content="test\ntest"'env2 | SUCCESS =&gt; &#123; "changed": true, "checksum": "307325bdaf62c0e286f79b3d664ae5bef971b340", "dest": "/opt/test", "gid": 0, "group": "root", "md5sum": "be778b473235e210cc577056226536a4", "mode": "0644", "owner": "root", "size": 9, "src": "/root/.ansible/tmp/ansible-tmp-1533064396.65-112216928591291/source", "state": "file", "uid": 0 directory_mode用法 1234# 设置目录的权限，仅适用于在远程主机创建目录的情况&gt; ansible env2 -m copy -a 'dest=/opt/ src=/opt/ansible directory_mode=1644'# 此时远程主机的目录的权限发生变化drw-r--r-T 2 root root 18 Jul 31 15:25 ansible mode用法 123456789# 将文件拷贝至远程主机后改变文件的权限&gt; ansible env2 -m copy -a 'dest=/opt/ src=/opt/test mode=0755' # 此时远程主机的文件权限发生变化-rwxr-xr-x 1 root root 1 Jul 31 15:38 test# 或者通过u+x这种方式来指定&gt; ansible env2 -m copy -a 'dest=/opt/ src=/opt/test mode=u+x'# 此时远程主机的文件权限发生变化-rwxr--r-- 1 root root 1 Jul 31 15:39 test attributes用法 12345678# 为文件权限添加id属性，详情查阅chattr文档&gt; ansible env2 -m copy -a 'dest=/opt/ src=/opt/test follow=yes attributes=id' # 此时远程主机文件属性改变（lsattr命令）----i-d--------- ./test# 为文件权限关闭id属性&gt; ansible env2 -m copy -a 'dest=/opt/ src=/opt/test follow=yes attributes=' # 此时远程主机文件属性变为空---------------- ./test follow用法 123456789101112131415# 远程主机上有一个test软连接文件lrwxrwxrwx 1 root root 14 Jul 31 15:19 test -&gt; ./ansible/test# 执行命令进行拷贝&gt; ansible env2 -m copy -a 'dest=/opt/ src=/opt/test'# 成功之后发现远程主机test文件软连接没了，被覆盖成真实的文件-rw-r--r-- 1 root root 4 Jul 31 15:20 test# 远程主机上有一个test软连接文件lrwxrwxrwx 1 root root 12 Jul 31 15:24 test -&gt; ansible/test# 添加follow=yes参数&gt; ansible env2 -m copy -a 'dest=/opt/ src=/opt/test follow=yes'# 软连接存在lrwxrwxrwx 1 root root 12 Jul 31 15:24 test -&gt; ansible/test# 真实文件被覆盖-rw-r--r-- 1 root root 1 Jul 31 15:25 ansible/test backup用法 1234# 覆盖之前备份&gt; ansible env2 -m copy -a 'dest=/opt/ src=/opt/test backup=yes' # 此时远程主机多了备份文件-rw-r--r-- 1 root root 1 Jul 31 15:25 test.8892.2018-07-31@15:32:56~ owner、group用法 1234# 将文件的属主、属组变为其他用户&gt; ansible env2 -m copy -a 'dest=/opt/ src=/opt/test owner=gaopeng group=gaopeng'# 此时远程主机的文件的属主、属组发生变化-rw-r--r-- 1 gaopeng gaopeng 1 Jul 31 15:32 test file对远程主机上的文件或目录进行操作。包括创建文件或目录、删除文件或目录、修改文件和目录的权限。 1ansible-doc file -s 参数 默认（可选） 说明 attributes 效果等同于copy模块中attributes follow yes（no） 效果等同于copy模块中follow force no（yes） 当state=link，可配合此参数强制创建链接文件当force=yes时，表示强制创建链接文件，不过强制创建链接文件分为两种情况：1. 当你要创建的链接文件指向的源文件并不存在时，使用此参数，可以先强制创建出链接文件。2. 当你要创建的链接文件的目录中已经存在与链接文件同名的文件时，会将同名的文件覆盖为链接文件，相当于删除同名文件，创建链接文件。 group 效果等同于copy模块中group mode 效果等同于copy模块中mode owner 效果等同于copy模块中owner path（必选） 用于指定要操作的文件或目录 recurse no（yes） 递归设置目录中文件属性，仅适用于目录 selevel 关于selinux设置（不知） serole 关于selinux设置（不知） setype 关于selinux设置（不知） seuser 关于selinux设置（不知） src 当state设置为link或hard时，表示创建一个软连接。所以必须知名软链接或者硬链接指向的文件，通过src可以指定链接源 state file（absent、directory、hard、link、touch） 此参数有多个取值：directory、file、link、hard、touch、absentdirectory：如果目录不存在，则创建目录file：即使文件不存在也不创建文件link：创建软链接hard：创建硬链接touch：如果文件不存在，创建一个新的文件。如果文件已存在，更新最后访问和修改时间absent：删除目录、文件或链接 unsafe_writes no（yes） 不安全写入（不知） 例： 创建文件和目录 123456789101112131415161718192021222324252627# 创建一个文件，如果文件已经存在，则会更新文件的时间戳，与touch命令相同&gt; ansible env2 -m file -a 'path=/opt/test state=touch'env2 | SUCCESS =&gt; &#123; "changed": true, "dest": "/opt/test", "gid": 0, "group": "root", "mode": "0744", "owner": "root", "size": 1, "state": "file", "uid": 0&#125;# 创建一个目录，如果目录已经存在，则不进行任何操作&gt; ansible env2 -m file -a 'path=/opt/ansible state=directory'env2 | SUCCESS =&gt; &#123; "changed": false, "gid": 0, "group": "root", "mode": "01644", "owner": "root", "path": "/opt/ansible", "size": 18, "state": "directory", "uid": 0&#125; 创建一个软连接（硬链接类似） 12345678910111213&gt; ansible env2 -m file -a 'path=/opt/test src=/opt/ansible/test state=link force=yes'env2 | SUCCESS =&gt; &#123; "changed": true, "dest": "/opt/test", "gid": 0, "group": "root", "mode": "0777", "owner": "root", "size": 17, "src": "/opt/ansible/test", "state": "link", "uid": 0&#125; mode、attributes、owner、group用法 12345678# 创建一个权限为01777、属主属组为其他用户的文件、并且属性值为i&gt; ansible env2 -m file -a 'path=/opt/test state=touch mode=01777 attributes=i owner=gaopeng group=gaopeng'# 此时查看远程主机上的文件权限及属性-rwxrwxrwt 1 gaopeng gaopeng 1 Jul 31 17:20 test----i----------- test# 此时若想在删除文件必须执行2个操作chattr -i testchmod -t test 删除文件、目录、硬链接或软连接 1234567# 删除文件（不管是文件、目录还是链接都会被删除，linux下一切皆文件啊）&gt; ansible env2 -m file -a 'path=/opt/test state=absent'env2 | SUCCESS =&gt; &#123; "changed": true, "path": "/opt/test", "state": "absent"&#125; recurse用法 12345# 递归修改目下的文件属性&gt; ansible env2 -m file -a 'path=/opt/ansible owner=gaopeng group=gaopeng recurse=yes'# 此时目录及目录下文件的属性drw-r--r-T 2 gaopeng gaopeng 18 Jul 31 15:25 ansible-rw-r--r-- 1 gaopeng gaopeng 1 Jul 31 15:25 test blockinfile在指定的文件插入文本，这段文本是被标记过的，以便以后的操作中可以通过标记来找到这段文本。 1ansible-doc blockinfile -s 参数 默认（可选） 说明 attributes 同copy backup 同copy block 指定想要操作的文本 create 当要操作的文件不存在时，是否创建对应的文件 group 同copy insertafter EOF（*regex*） 在插入一段文本时，默认会在文件的末尾插入。如果你想要将文本在某一行的后面，可以使用此参数指定对应的行使用正则表达式：（Python re模块，终于可以使用不消耗匹配等功能了），表示将文本插入在符合正则表达式的行的后面。如果有多行文本都能够匹配对应的正则表达式，则以最后一个满足正则的行为准EOF：插入文件末尾 insertbefore （BOF，*regex*） 类似于insertafter，只是插入指定行前BOF：插入文本开头不指定insertbefore参数会默认使用insertafter=EOF marker {mark} ANSIBLE MANAGED BLOCK 自定义标记默认情况我们插入一段文本，Ansible会自动为这段文本添加两个标记：（# BEGIN ANSIBLE MANAGED BLOCK # END ANSIBLE MANAGED BLOCK ）可以使用marker参数自定义标记，使用{mark}会替换成开始标记和结束标记 marker_begin BEGIN 用于设置{mark}开始字符串 marker_end END 用于设置{mark}结束字符串 mode 同copy owner 同copy path 指定要操作的文件 selevel s0 关于selinux设置（不知） serole 关于selinux设置（不知） setype 关于selinux设置（不知） seuser 关于selinux设置（不知） state present（absent） present：默认情况下，指定一段文本插入到文件中时，如果对应的文件已经存在对应的标记的文本，会更新对应段落absent：删除对应标记与标记中的段落 validate 操作前是否检验需要复制目的地的路径 例： marker、marker_begin、marker_end用法 12345678910111213# 添加一段文本至文件末尾，标记为service to start，并修改&#123;mark&#125;开始结束的标记字符串为Begin,End&gt; ansible env2 -m blockinfile -a 'path=/opt/rc.local block="systemctl start httpd" marker="#&#123;mark&#125; service to start" marker_begin="Begin" marker_end="End"'env2 | SUCCESS =&gt; &#123; "changed": true, "msg": "Block inserted"&#125;# 查看/opt/rc.local文件末尾&gt; tail -3 /opt/rc.local#Begin service to startsystemctl start httpd#End service to start 替换标记中文本 12345678# 默认情况下state=present，即为替换，指定对应的标记即可&gt; ansible env2 -m blockinfile -a 'path=/opt/rc.local block="systemctl start mysqld" marker="#&#123;mark&#125; service to start" marker_begin="Begin" marker_end="End"' # 在远程主机上查看文件&gt; tail -3 /opt/rc.local #Begin service to startsystemctl start mysqld#End service to start 删除标记文本 12# 将state设置为absent，并指定对应的标记&gt; ansible env2 -m blockinfile -a 'path=/opt/rc.local block="systemctl start mysqld" marker="#&#123;mark&#125; service to start" marker_begin="Begin" marker_end="End" state=absent' insertafter、insertbefore用法 1234567# 插入到行首&gt; ansible env2 -m blockinfile -a 'path=/opt/rc.local block="systemctl start mysqld" marker="#&#123;mark&#125; service to start" marker_begin="Begin" marker_end="End" insertbefore=BOF'# 查看远程主机文件&gt; head -3 /opt/rc.local#Begin service to startsystemctl start mysqld#End service to start lineinfile对文件中的某一行进行操作。可以确保某一行是否存在，可以插入或删除某一行，也可以对文本进行替换。 1ansible-doc lineinfile -s 参数 默认（可选） 说明 attributes 同copy backrefs no（yes） 默认情况下，当根据正则替换文本时，即使regexp参数中的正则存在分组，在line参数中也不能对正则中的分组进行引用，除非将backrefs参数设置为yesbackrefs=yes表示开启向后引用默认情况下，当使用正则替换对应行时，如果正则没有匹配到任何的行，那么line对应的内容会被插入到文本末尾backerfs=yes时，如果正则没有匹配到任何行时，则不会对文件进行任何操作 backup no（yes） 同copy create no（yes） 同blockinfile firstmatch no（yes） 默认情况下，当使用insertafter或者insertbefore插入指定的行时会以最后一个匹配结果为准firstmatch=yes时以第一个匹配为准 group 同copy insertafter EOF（*regex*） 同blockinfile insertbefore （BOF，regex\） 同blockinfile line 指定文本内容 mode 同copy others 可以指定file模块的参数 owner 同copy path 指定操作的文件 regexp 使用正则表达式匹配对应的行当替换文本时，如果有多行都能被匹配，则只有匹配的最后面的行才会被替换当删除文本时，如果有多行文本都能被匹配，这些行都会被删除 state present（absent） 同blockinfile 例： 基本用法 1234567# 插入行到文本末尾&gt; ansible env2 -m lineinfile -a 'path=/opt/test line="test"'env2 | SUCCESS =&gt; &#123; "backup": "", "changed": true, "msg": "line added"&#125; regexp用法 1234567# 使用正则替换指定的行&gt; ansible env2 -m lineinfile -a 'path=/opt/test regexp="^test" line="test_1"'env2 | SUCCESS =&gt; &#123; "backup": "", "changed": true, "msg": "line replaced"&#125; insertafter、insertbefore用法 1234567# 使用正则插入到第一次匹配的行前&gt; ansible env2 -m lineinfile -a 'path=/opt/test line="test_2" insertbefore="^test"'env2 | SUCCESS =&gt; &#123; "backup": "", "changed": true, "msg": "line added"&#125; backrefs用法 123456789101112131415# 如果正则表达式没有匹配到任何一行，默认会在文本末尾添加行&gt; ansible env2 -m lineinfile -a 'path=/opt/test regexp="none" line="test_3"'env2 | SUCCESS =&gt; &#123; "backup": "", "changed": true, "msg": "line added"&#125;# 使用backrefs=yes时，则不会进行任何操作&gt; ansible env2 -m lineinfile -a 'path=/opt/test regexp="none" line="test_4" backrefs=yes'env2 | SUCCESS =&gt; &#123; "backup": "", "changed": false, "msg": ""&#125; backup参照copy模块用例，create、state用法参考blockinfile举例。 replace根据指定的正则表达式替换文件中的字符串，文件中所有被正则匹配的字符串都会被替换。 1ansible-doc replace -s 关于文件属性、备份、selinux选项等不在说明，参考copy、file模块即可。 参数 默认（可选） 说明 after 指定从某行之后进行匹配，可以结合before参数使用 before 指定从某行之前进行匹配，可以结果after参数使用 encoding utf-8 指定读写文件的字符编码 path（必选） 指定要操作的文件 regexp（必选） 指定正则表达式（python re模块） replace 指定需要替换成的字符串 例： 正则替换（终于可以使用不消耗匹配，使用grep、awk时不能使用(?=)(?&lt;=)挺蛋疼） 12345678910111213141516171819# 将文本中所有test都变成TEST&gt; ansible env2 -m replace -a 'path=/opt/test regexp="test" replace="TEST"' "changed": true, "msg": "3 replacements made"&#125;# 不消耗匹配，替换https://之后的内容&gt; ansible env2 -m replace -a 'path=/opt/test regexp="(?&lt;=https://).*" replace="python.org"'env2 | SUCCESS =&gt; &#123; "changed": true, "msg": "1 replacements made"&#125;# 引用分组&gt; ansible env2 -m replace -a 'path=/opt/test regexp="(https://).*" replace="\1baidu.com"'env2 | SUCCESS =&gt; &#123; "changed": true, "msg": "1 replacements made"&#125; after、before用法 123# 指定某行之后与某行之前进行匹配替换# 这里有一点搞不明白，单独使用after和before是从之后、之前进行匹配，可以结合一起使用就出现问题了，&gt; ansible env2 -m replace -a 'path=/opt/test regexp="a" replace="b" after="start" before="end"' find在远程主机中查找符合条件的文件，和find命令相同。 1ansible-doc find -s 参数 默认（可选） 说明 age 使用此参数可以根据时间范围查找文件，默认以文件的mtime为准与指定的时间对比可以使用的单位有s（秒）、m（分钟）、h（小时）、d（天）、w（星期）使用-表示小于指定的时间 age_stamp mtime（atime、ctime） 文件的时间属性有三种类型：atime、ctime、mtime，当根据时间范围查找文件时，可以指定以哪个文件的时间类型为准 contains 使用此参数可以根据文章内容查找文件，此参数的值为一个正则表达式，find会根据对应的正则表达式匹配文件内容 depth 设置目录查找的递归层数，递归查找到指定的层数时即停止查找默认为无限深度 excludes 指定一种或多种 （shell或regex） 模式，模式类型由 use_regex参数控制。多种模式使用列表指定 file_type file（any、directory、link） 指定查找文件的类型 follow no（yes） 如果设置为yes，则遵循python2.6+版本的符号链接 get_checksum no（yes） 当有符合查找条件的文件被找到时，会同时返回对应文件的sha1校验码。文件较大时，计算过程会耽误时间 hidden no（ues） 是否查找隐藏文件 paths（必须） 指定查找文件的目录。可以指定多个路径，用逗号隔开 patterns * 指定查找文件的文件名称，支持使用shell（比如通配符）或正则表达式匹配文件名称。默认使用shell，如果想使用正则表达式需要设置use_regex=yes recurse no（yes） 默认情况下，只会在指定的目录中查找文件，ansible并不会递归查找如果设置为yes时，则表示递归查找指定的目录中的文件可配合depth指定递归查找的深度 size 使用此参数可以根据文件大小查找文件可使用的单位有：t、g、m、k、b使用-表示小于指定的大小 use_regex no（yes） 如果为no，使用glob通配符（shell）。如果为yes，使用python regexes 例： patterns用法 12345678910# 查找包含test的文件，省略了文件信息&gt; ansible env2 -m find -a 'path=/opt patterns="*test*"'env2 | SUCCESS =&gt; &#123; "changed": false, "examined": 6, ... ... "matched": 2, "msg": ""&#125; contains用法 12# 查找文件内容包含test字符串的文件&gt; ansible env2 -m find -a 'path=/opt contains=".*test.*"' age、size、recurse用法 123456789# 递归查找时间超过3天的日志文件&gt; ansible env2 -m find -a 'paths=/var/log/ age=3d recurse=yes'# 递归查找时间不超过5m的日志文件&gt; ansible env2 -m find -a 'paths=/var/log/ age=-5m recurse=yes'# 递归查找超过1G的文件&gt; ansible env2 -m find -a 'paths=/var/ size=1g recurse=yes'# 递归查找小于10m的文件&gt; ansible env2 -m find -a 'paths=/opt size=-10m recurse=yes' hidden、use_regex用法 12# 查找root目录下隐藏文件ansible env2 -m find -a 'paths=/root patterns="^\..*" hidden=yes recurse=yes use_regex=yes' archive打包存档目录文件，在归档之前，不会将源文件从控制主机上拷贝到远程主机上。 unarchive将控制主机的压缩包拷贝到远程服务器，然后进行解压。 synchronize对rsync命令的封装。主要用于同步本地目录至远程主机。 命令相关模块command在远程主机上执行命令。（不支持管道重定向等符号） 1ansible-doc command -s 参数 默认（可选） 说明 argv 允许用户使用字符串或列表来提供命令，同时只能使用其中一种 chdir 指定运行命令的目录，会在执行命令之前先进入指定的目录中 creates 匹配文件，当文件存在时就不执行对应的命令，不会创建文件 free_form（必选） 指定远程主机执行的命令注意，这个参数不是关键字参数，不要使用free_form=这种方式，直接使用命令 removes 与creates相反，匹配文件，当文件不存在时就不执行对应的命令，不会删除文件 stdin 指定命令的stdin的值 warn yes（no） 默认为yes，如果命令发出警告，则继续进行。no则相反 例： free_form、chdir用法 12# 在远程主机上/opt执行ls命令&gt; ansible env2 -m command -a 'chdir=/opt ls' creates、removes用法 123456789# 如果文件存在，就不执行对应的命令&gt; ansible env2 -m command -a 'creates=/opt/test ls'env2 | SUCCESS | rc=0 &gt;&gt;skipped, since /opt/test exists# 如果文件不存在，就不执行对应的命令&gt; ansible env2 -m command -a 'removes=/opt/test_2 ls /opt'env2 | SUCCESS | rc=0 &gt;&gt;skipped, since /opt/test_2 does not exist shell在远程主机上执行命令，与command模块不同的是，shell模块在远程主机上执行命令时，会经过远程主机上的/bin/sh程序处理。（支持管道重定向等符号） 1ansible-doc shell -s 参数 默认（可选） 说明 chdir 同command creates 同command executable 默认情况下，shell模块会调用远程主机/bin/sh执行命令。如果想要使用其他类型的shell执行命令，可以使用此参数指定某种类型的shell执行对应的命令。 free_form 同command removes 同command stdin 同command warn yes（no） 同command 例： 使用管道、重定向 12345# 将输出的字符串重定向至文件&gt; ansible env2 -m shell -a 'chdir=/opt/ echo test &gt; test'# 管道&gt; ansible env2 -m shell -a 'ps -ef | wc -l' script在远程主机上执行控制主机上的脚本，脚本一直存在与控制主机本地，不需要拷贝到远程主机后再执行。 1ansible-doc shell -s 参数 默认（可选） 说明 chdir 同command creates 同command executable 同shell free_form 同command removes 同command 例： 编写一个脚本，在远程主机上执行 12345678910111213141516&gt; ansible env2 -m script -a '/opt/monitor.sh'env2 | SUCCESS =&gt; &#123; "changed": true, "rc": 0, "stderr": "Shared connection to env2 closed.\r\n", "stderr_lines": [ "Shared connection to env2 closed." ], "stdout": "cpu使用率: 56.2 %\r\n内存使用率: 129 M\r\n磁盘空间使用量: 1.6G\r\n磁盘空间利用率: 10%\r\n", "stdout_lines": [ "cpu使用率: 56.2 %", "内存使用率: 129 M", "磁盘空间使用量: 1.6G", "磁盘空间利用率: 10%" ]&#125; 系统相关模块cron管理远程主机中的计划任务，功能相当于crontab命令。 linux cron简要说明： 1234567891011121314151617181920# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed 分 时 日 月 周# @特殊时间 user-name command to be executed# 特殊时间。设置特殊时间后，不能再设置别的时间@reboot 重启之后运行@hourly 每小时运行@daily 每天运行@weekly 每周运行@monthly 每月运行@yearly 每年运行@annually 同@yearly@midnight 同@daily 1ansible-doc cron -s 参数 默认（可选） 说明 minute * 分 hour * 时 day * 日 month * 月 weekday * 星期 backup no（yes） 修改或删除之前是否先对计划任务进行备份 cron_file 如果使用该参数，则用该文件替换远程主机上cron.d目录下的用户计划任务 disabled 当计划任务有名称时，可以根据名称使对应的任务关闭（即注释）。使用此参数时，除了需要指定任务的名称，还需要同时指定任务的job以及任务的时间设定任务的时间设定必须和对应任务完全相同，否则在注释任务的同时，任务的时间设定会被修改 job 指定计划任务中需要实际执行的命令或脚本 name 指定计划任务的名称（即计划任务中的注释说明） reboot no（yes） 不建议使用，建议使用special_time special_time （reboot、yearly、annually monthly、weekly、daily、hourly） 指定特殊时间，参考cron简要说明 state present（absent） present：根据名称进行修改计划任务absent：根据名称进行删除计划任务 user root 指定执行计划任务的用户 例： minute、hour、day、month、weekday用法 123456# 添加计划任务，每三天在当天的1点30分执行一个任务&gt; ansible env2 -m cron -a 'name=test minute=30 hour=1 day=*/3 job="echo test"'# 查看远程主机的计划任务&gt; cat /var/spool/cron/root#Ansible: test30 1 */3 * * echo test special_time用法 123456# 每次重启执行一个任务&gt; ansible env2 -m cron -a 'name="test special time" special_time=reboot job="echo test"'# 查看远程主机的计划任务&gt; cat /var/spool/cron/root#Ansible: test special time@reboot echo test state用法 12345678# 默认为present，当任务test special time已经存在时，修改计划任务&gt; ansible env2 -m cron -a 'name="test special time" special_time=daily job="echo test"' &gt; cat /var/spool/cron/root#Ansible: test special time@daily echo test# 设置为absent时，删除该计划任务&gt; ansible env2 -m cron -a 'name="test special time" state=absent' user用法 123456# 使用其他用户来执行计划任务&gt; ansible env2 -m cron -a 'user=gaopeng name="test special time" special_time=daily job="echo test"'# 查看远程主机的计划任务(文件名不是root)&gt; cat /var/spool/cron/gaopeng#Ansible: test special time@daily echo test disabled、backup用法 123456789101112131415# 关闭计划任务，关闭之前先进行备份&gt; ansible env2 -m cron -a 'user=gaopeng name="test special time" special_time=daily job="echo test" disabled=yes backup=yes'# 备份文件为/tmp/crontabrCqyRqenv2 | SUCCESS =&gt; &#123; "backup_file": "/tmp/crontabrCqyRq", "changed": true, "envs": [], "jobs": [ "test special time" ]&#125;# 查看远程主机的计划任务，已被注释&gt; cat /var/spool/cron/gaopeng #Ansible: test special time#@daily echo test service管理远程主机上的服务。 可被service管理的情况： 如果想通过service模块管理远程主机中的某个服务，那么这个服务必须能被BSD init，OpenRC，SysV, Solaris SMF, systemd, upstart中任意一种所管理。假设使用CentOS 6，CentOS 6默认通过SysV管理服务，那么这个服务必须能通过service 服务名 start启动。 假设使用CentOS 7，CentOS 7默认通过systemd管理服务，那么这个服务必须能通过systemctl start 服务名启动。 1ansible-doc service -s 参数 默认（可选） 说明 arguments 指定额外参数 enabled no（ues） 指定是否将服务设置为开机启动项 name 指定服务名 pattern 指定一个模式，如果通过status指令来查看服务的状态时，没有响应，就会通过ps指令在进程中根据该模式进行查找，如果匹配到，则认为该服务依然在运行 runlevel default 针对于OpenRC管理服务，可以使用此参数来设置服务的运行级别 sleep 服务重启时，可以使用此参数设置stop与start之间的时间间隔 state （reloaded、restarted、running、started 、stopped） 指定服务状态 use auto serivce模块通常会自动监测当前系统使用的服务管理模块此参数可以强制指定服务管理的模块 例： name、state用法 1234567891011121314# 启动nginx&gt; ansible env2 -m service -a 'name=nginx state=started'env2 | SUCCESS =&gt; &#123; "changed": true, "name": "nginx", "state": "started", "status": &#123; ... ... &#125;&#125;# 关闭nginx&gt; ansible env2 -m service -a 'name=nginx state=stopped' enabled用法 12345# 将nginx设置为开机启动项&gt; ansible env2 -m service -a 'name=nginx enabled=yes'# 查看远程主机nginx开机启动状态&gt; systemctl list-unit-files | grep 'nginx'nginx.service enabled user管理远程主机上的用户。比如创建用户、修改用户、删除用户、为用户创建秘钥对等。 1ansible-doc user -s 参数 默认（可选） 说明 append no（yes） 如果为yes：用户添加到指定的组中如果为no：用户添加到指定的组中，然后将用户从其他组删除 comment 用户的注释信息 create_home yes（no） 创建用户时，是否创建家目录。 expires 指定用户过期时间，相当于/etc/shadow文件中的第8列指定值为unix时间戳，转换时间戳使用date -d命令或者使用python datetime或time模块 force no（yes） 只影响state=absent，表示强制删除用户，即使用户正在登录，作用与userdel -force相同 generate_ssh_key no（yes） 是否为用户生成ssh密钥对，默认为~/.ssh/id_rsa的私钥和id_rsa.pub的公钥。如果同名的密钥对已经存在对应的目录中，则不进行任何操作 group 指定用户所在的基本组 groups 指定用户所在的附加组如果用户已经存在并且已经拥有多个附加组，默认情况下，使用此参数设置附加组时，用户原来的附加组会被覆盖如果不想覆盖原来的附加组，在其基础上添加附加组，结合append=yes使用 hidden （no、yes） 仅限于Darwin/OS X系统，是否在登录窗口隐藏用户 home 指定用户的家目录路径 local no（yes） 使用luseradd代替useradd远程主机上必须可以使用luseradd命令 move_home no（yes） 如果用户有家目录，可以用过此参数则迁移用户的家目录。 name（必选） 指定操作的用户名称 password 指定用户的密码这个密码不能是明文密码，必须是对明文密码加密后的字符串可以使用python passlib模块先进行哈希加密 password_lock （no、yes） 是否锁定密码，不会禁用用户登录，只是不能在同一个task来修改密码 remove no（yes） 删除用户时，是否删除用户的家目录等信息当state=absent并且remove=yes时，相当于执行userdel –remove命令 shell 指定用户的默认shell ssh_key_bits default set by ssh-keygen 当generate_ssh_key为yes时，指定要创建的ssh密钥中的位数 ssh_key_comment ansible-generated on $HOSTNAME 当generate_ssh_key为yes时，指定公钥中的注释信息如果同名的密钥对已经存在不会进行任何操作 ssh_key_file .ssh/id_rsa 当generate_ssh_key为yes时，指定ssh私钥的路径和名称，对应的公钥会在同路径下生成 ssh_key_passphrase 当generate_ssh_key为yes时，指定ssh私钥的密码如果同名的密钥对已经存在不会进行任何操作 ssh_key_type rsa 当generate_ssh_key为yes时，指定密钥对的类型如果同名的密钥对已经存在不会进行任何操作 state present（absent） present：用户需要存在absent：删除用户 system no（yes） 是否创建为系统用户，不能在已存在的用户上进行更改 uid 指定用户uid号 update_password always（on_create） always：如果password参数的值与用户当前的加密过的密码字符串不一致，则直接更新用户的密码on_create：如果password参数的值与用户当前的加密过的密码字符串不一致，则不会更新用户的密码字符串，保持之前的密码设定 例： name、comment、password、uid用法 123456789# 首先要生成密码的加密字符串&gt; python -c "from passlib.hash import sha512_crypt; import getpass; print(sha512_crypt.using(rounds=5000).hash(getpass.getpass()))"# 输入密码（不会显示密码）Password: 123456$6$iDBOhZfEfEHw0OhP$I.oHiGUKG9TZgW0BkYcOFjeL15h.LG5bi/gKNlpLP2wY8ux3zx2sB8fMZyO47kmBld1tUyrwyLM/7oYqcDJ9m1# 创建用户名为gp、密码为123456、信息为gaopeng、uid为1993的用户&gt; ansible env2 -m user -a 'name=gp comment="GaoPeng" uid=1993 password=$6$iDBOhZfEfEHw0OhP$I.oHiGUKG9TZgW0BkYcOFjeL15h.LG5bi/gKNlpLP2wY8ux3zx2sB8fMZyO47kmBld1tUyrwyLM/7oYqcDJ9m1'# 查看远程主机，此时用户已经被创建gp:x:1993:1993:GaoPeng:/home/gp:/bin/bash state，remove用法 12# 删除用户，默认情况下删除用户也不会移除家目录，使用remove=yes在删除用户的同时也一并移除家目录&gt; ansible env2 -m user -a 'name=gp state=absent remove=yes' group、groups、append用法 12# 指定用户属组为root、并不覆盖原有的附加组添加附加组为ansible&gt; ansible env2 -m user -a 'name=gp group=root groups=ansible append=yes' expires用法 1234567891011121314# 先生成时间戳&gt; date -d 2018-08-31 +%s1535644800# 创建临时用户，设置过期时间为2018年8月31日ansible env2 -m user -a 'name=guest expires=1535644800 password=$6$iDBOhZfEfEHw0OhP$I.oHiGUKG9TZgW0BkYcOFjeL15h.LG5bi/gKNlpLP2wY8ux3zx2sB8fMZyO47kmBld1tUyrwyLM/7oYqcDJ9m1'# 查看用户过期信息&gt; chage -l guestLast password change : Aug 02, 2018Password expires : neverPassword inactive : neverAccount expires : Aug 30, 2018Minimum number of days between password change : 0Maximum number of days between password change : 99999Number of days of warning before password expires : 7 generate_ssh_key、ssh_key_comment、ssh_key_file、ssh_key_passphrase、ssh_key_type用法 12# 为用户创建秘钥对，注释信息为GaoPeng，路径为/opt/id_rsa_gp，私钥密码为123456，加密类型为dsa&gt; ansible env2 -m user -a 'name=gp generate_ssh_key=yes ssh_key_comment=GaoPeng ssh_key_file=/opt/id_rsa_gp ssh_key_passphrase=123456 ssh_key_type=dsa' group管理远程主机上的组。 远程主机上必须有groupadd、groupdel、groupmod模块。 1ansible-doc group -s 参数 默认（可选） 说明 gid 指定组的gid local no 使用lgroupadd代替groupadd远程主机上必须可以使用lgroupadd命令 name（必选） 指定组名 state present（absent） 同user system no（yes） 是否创建系统组 例： name、gid用法 12# 创建ansible组，gid为2018&gt; ansible env2 -m group -a 'name=ansible gid=2018' state用法 12# 删除ansible组&gt; ansible env2 -m group -a 'name=ansible state=absent' sysctl控制Linux内核参数，与sysctl命令相同。 1ansible-doc sysctl -s 参数 默认（可选） 说明 ignoreerrors no（yes） 是否忽略未知键的错误 name（必选） 指定需要设置的内核参数 reload yes（no） 设置完成后是否需要执行sysctl -p操作 state present（absent） present：该内核参数必须存在absent：删除该内核参数 sysctl_file /etc/sysctl.conf 指定sysctl.conf的绝对路径 sysctl_set no（yes） 是否验证 Verify token value value 指定需要设置的值 例： name、value用法 12# 设置overcommit_memory=1&gt; ansible env2 -m sysctl -a 'name=vm.overcommit_memory value=1' mount在远程服务器上挂载磁盘，当进行挂载磁盘操作时，如果挂载点指定的路径不存在，将创建该路径。 1ansible-doc mount -s 参数 默认（可选） 说明 backup no（yes） 是否创建备份文件 boot yes（no） 仅适用于Solaris系统，是否开机启动时挂载 dump 0 是否进行转储，state=present时将停止工作 fstab /etc/ fstab 是否使用文件来代替/etc/fstab，慎用 fstype 指定文件系统类型，当state的值为present或mounted时，此参数必选 passno 0（1、2） 是否检验扇区：开机的过程中，系统默认会以fsck检验我们系统是否为完整（clean） opts 指定挂载选项 path（必选） 指定挂载点的路径 src 指定挂载的设备，当state的值为present或mounted时，此参数必选 state（必选） （absent、mounted、present、unmounted） mounted：挂载并正确配置fstab文件unmounted：卸载，不改变fstab文件present：仅配置fstab文件，不进行磁盘操作absent：卸载并删除fstab对应的配置 例： state、path、fstype、src、opts用法 1234# 挂载光驱，并添加fstab配置&gt; ansible env2 -m mount -a 'path=/media src=/dev/cdrom fstype=iso9660 opts=loop state=mounted' # 卸载光驱并删除/etc/fstab相关配置&gt; ansible env2 -m mount -a 'path=/media state=absent' dump用法 1234567# 挂载设备，每一个星期进行转储ansible env2 -m mount -a 'path=/mnt/data src=/dev/sdb1 fstype=xfs dump=7 state=mounted'# 查看对应的fstab文件&gt; cat /etc/fstab | grep /dev/sdb1/dev/sdb1 /mnt/data xfs defaults 7 0# 临时卸载，并不删除配置&gt; ansible env2 -m mount -a 'path=/mnt/data state=unmounted' filesystem格式化磁盘，创建文件系统。 iptables🌐iptables 管理静态防火墙。 firewalld管理动态防火墙 selinux:globe_with_meridians:selinux 管理selinux服务状态。 filesystem初始化文件系统。 :globe_with_meridians:filesystem 安装包相关模块安装包相关模块目前只应用于简单的需求场景，复杂的需求没有用到过，因此只记录简要信息。 yum_repository管理远程主机上的yum仓库。 1ansible-doc yum_repository -s 参数 默认（可选） 说明 name（必选） 指定要操作的唯一的仓库的ID即配置文件中的[]内的仓库ID baseurl 指定yum仓库的baseurl description 指定仓库的注释信息，即仓库的name字段内容 file 指定仓库的配置文件名称不使用此参数，默认会以name参数值作为文件名称前缀 enabled yes（no） 是否激活对应的yum源 gpgcheck （no、yes） 是否开启rpm包验证功能 gpgcakey 当gpgcheck=yes时，需要使用此参数指定验证包所需的公钥 state present（absent） present：此yum源需要存在absent：删除对应yum源 name、baeurl、description、enabled用法 1234567891011# 配置本地yum源，但并不开启它&gt; ansible env2 -m yum_repository -a 'name=local baseurl=file:///media description="local yum" enabled=no'# 查看远程主机配置&gt; cat /etc/yum.repos.d/local.repo[local]baseurl = file:///mediaenabled = 0name = local yum# 配置epel源&gt; ansible env2 -m yum_repository -a 'name=epel description="aliyun EPEL" baseurl="https://mirrors.aliyun.com/epel/$releasever\Server/$basearch/"' gpgcheck、gpgcakey用法 12# 配置本地yum源，开启包验证功能，并开启它&gt; ansible env2 -m yum_repository -a 'name=local baseurl=file:///media description="local yum" gpgcheck=yes gpgcakey=file:///media/RPM-GPG-KEY-CentOS-7' state用法 12# 删除epel源&gt; ansible env2 -m yum_repository -a 'name=epel state=absent' yum在远程主机上用过yum管理软件包。 1ansible-doc yum -s 参数 默认（可选） 说明 name 指定需要管理的软件包 state present（absent、installed、latest、removed） present：确保软件包已经安装installed：同presentlatest：安装yum中最新的版本absent：删除对应的软件包removed：同absent disable_gpg_check no（yes） 用于禁用对rpm包的公钥gpg验证在对应的yum源没有开启gpg验证的情况下，需要将此参数的值设置为yes enablerepo 指定安装包临时启用的yum源如果你不确认yum源是否启用，可以在安装软件包时将此参数设置为yes disabelrepo 临时禁用yum源 例： name、state、disable_gpg_check用法 12345# 安装nginx，跳过gpg验证&gt; ansible env2 -m yum -a 'name=nginx disable_gpg_check=yes'# 卸载nginx&gt; ansible env2 -m yum -a 'name=nginx state=absent' 网络工具模块get_url:globe_with_meridians:get_url wait_for数据库相关模块MySQL相关模块mysql_db:globe_with_meridians:mysql_db mysql_replicationmysql_user:globe_with_meridians:mysql_user mysql_variablesMongoDB相关模块mongodb_parametermongodb_userRedis相关模块redis存储相关模块glusterfs相关gluster_peer:globe_with_meridians:gluster_peer gluster_volume:globe_with_meridians:gluster_volume]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible学习笔记-Ad-Hoc]]></title>
    <url>%2F2018%2F07%2F30%2FAnsible%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-Ad-Hoc%2F</url>
    <content type="text"><![CDATA[Ad-Hoc笔记Ansible提供两种完成任务的方式： ansible — Ad-hoc，解决一些简单或者临时遇到的任务，相当于Linux的系统命令行下的shell命令 ansible-playbook — Playbook，解决复杂或者固化下来的任务，相当于Linux系统的Shell脚本 这次记录Ad-Hoc的使用方法。 Ad-Hoc原理 详细过程： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162[root@env1 tmp]# ansible env1 -m ping -vvvansible 2.6.1# 读取配置文件 config file = /etc/ansible/ansible.cfg# 配置的模块路径 configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']# anisble库、命令的位置及python版本号 ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /usr/bin/ansible python version = 2.7.5 (default, Aug 4 2017, 00:39:18) [GCC 4.8.5 20150623 (Red Hat 4.8.5-16)]# 使用配置文件Using /etc/ansible/ansible.cfg as config file# 解析主机清单中（ini插件解析）Parsed /etc/ansible/hosts inventory source with ini pluginMETA: ran handlers# 链接远程主机成功（ESTABLISH），USER:None表示使用系统当前用户，未指定端口号表示使用ssh默认端口号&lt;env1&gt; ESTABLISH SSH CONNECTION FOR USER: None&lt;env1&gt; SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/a950b0cbe8 env1 '/bin/sh -c '"'"'echo ~ &amp;&amp; sleep 0'"'"''&lt;env1&gt; (0, '/root\n', '')# 创建临时目录&lt;env1&gt; ESTABLISH SSH CONNECTION FOR USER: None&lt;env1&gt; SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/a950b0cbe8 env1 '/bin/sh -c '"'"'( umask 77 &amp;&amp; mkdir -p "` echo /root/.ansible/tmp/ansible-tmp-1532981758.0-52597477914119 `" &amp;&amp; echo ansible-tmp-1532981758.0-52597477914119="` echo /root/.ansible/tmp/ansible-tmp-1532981758.0-52597477914119 `" ) &amp;&amp; sleep 0'"'"''&lt;env1&gt; (0, 'ansible-tmp-1532981758.0-52597477914119=/root/.ansible/tmp/ansible-tmp-1532981758.0-52597477914119\n', '')# put所需运行的脚本Using module file /usr/lib/python2.7/site-packages/ansible/modules/system/ping.py&lt;env1&gt; PUT /root/.ansible/tmp/ansible-local-208102tQ8Kw/tmpT0qXOj TO /root/.ansible/tmp/ansible-tmp-1532981758.0-52597477914119/ping.py&lt;env1&gt; SSH: EXEC sftp -b - -C -o ControlMaster=auto -o ControlPersist=60s -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/a950b0cbe8 '[env1]'&lt;env1&gt; (0, 'sftp&gt; put /root/.ansible/tmp/ansible-local-208102tQ8Kw/tmpT0qXOj /root/.ansible/tmp/ansible-tmp-1532981758.0-52597477914119/ping.py\n', '')# 给脚本添加用户可执行权限&lt;env1&gt; ESTABLISH SSH CONNECTION FOR USER: None&lt;env1&gt; SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/a950b0cbe8 env1 '/bin/sh -c '"'"'chmod u+x /root/.ansible/tmp/ansible-tmp-1532981758.0-52597477914119/ /root/.ansible/tmp/ansible-tmp-1532981758.0-52597477914119/ping.py &amp;&amp; sleep 0'"'"''&lt;env1&gt; (0, '', '')# 运行脚本，并返回结果&lt;env1&gt; ESTABLISH SSH CONNECTION FOR USER: None&lt;env1&gt; SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/a950b0cbe8 -tt env1 '/bin/sh -c '"'"'/usr/bin/python /root/.ansible/tmp/ansible-tmp-1532981758.0-52597477914119/ping.py &amp;&amp; sleep 0'"'"''&lt;env1&gt; (0, '\r\n&#123;"ping": "pong", "invocation": &#123;"module_args": &#123;"data": "pong"&#125;&#125;&#125;\r\n', 'Shared connection to env1 closed.\r\n')# 删除脚本&lt;env1&gt; ESTABLISH SSH CONNECTION FOR USER: None&lt;env1&gt; SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o StrictHostKeyChecking=no -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/a950b0cbe8 env1 '/bin/sh -c '"'"'rm -f -r /root/.ansible/tmp/ansible-tmp-1532981758.0-52597477914119/ &gt; /dev/null 2&gt;&amp;1 &amp;&amp; sleep 0'"'"''# 输出返回结果&lt;env1&gt; (0, '', '')env1 | SUCCESS =&gt; &#123; "changed": false, "invocation": &#123; "module_args": &#123; "data": "pong" &#125; &#125;, "ping": "pong"&#125;META: ran handlersMETA: ran handlers 并发： Ansible使用multiprocessing管理多线程 Ad-Hoc命令用法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798Usage: ansible &lt;host-pattern&gt; [options]Options:# 模块参数# 例：ansible -m command -a 'hostname' -a MODULE_ARGS, --args=MODULE_ARGS# vault密码 --ask-vault-pass# 后台执行命令，超过SECONDS秒之后中止任务 -B SECONDS, --background=SECONDS # 不执行任务，尝试检查可能发生的变化# 例：ansible env1 -m file -a 'path=/tmp/testfile state=touch' -C -C, --check# 当更改（小的）文件和模板时，显示这些文件的差异，大文件使用--check# 例：ansible env1 -m blockinfile -a 'path=/tmp/testfile block=test' -D -D, --diff# 设置外部变量，如果是变量文件在文件前面加'@'# 例：ansible-playbook cmdvar.yml --extra-vars 'pass_var=cmdline pass var' -e EXTRA_VARS, --extra-vars=EXTRA_VARS# 设置并发线程数，默认5个# 例：ansible test -m ping -f 1 -vvv（变为线性执行） -f FORKS, --forks=FORKS# 显示帮助信息 -h, --help# 指定inventory文件# 例： ansible -i /etc/ansible/hosts test -m ping -i INVENTORY, --inventory=INVENTORY, --inventory-file=INVENTORY# 进一步限制所选主机/组模式# 例： ansible test -m ping -l env1（只对这个ip执行） -l SUBSET, --limit=SUBSET# 列出主机列表，不执行任何命令 --list-hosts# 指定执行时使用的模块，不指定时默认为command模块 -m MODULE_NAME, --module-name=MODULE_NAME# 执行模块的存放路径，默认为[u'/root/.ansible/plugins/modules',u'/usr/share/ansible/plugins/modules'] -M MODULE_PATH, --module-path=MODULE_PATH# 压缩输出（输出为一行）# 例：ansible test -m ping -o -o, --one-line# 设置playbook目录 --playbook-dir=BASEDIR# 如果使用了-B，可以使用-P来设置轮询间隔，默认15秒 -P POLL_INTERVAL, --poll=POLL_INTERVAL# 对playbook进行语法检测，不会执行playbook（注意不是检查文件，只是检查语法是否正确）# 例：ansible env1 -m file -a 'path=/opt/ state=touch' --syntax-check --syntax-check# 输出信息至目录下# 例：ansible test -m ping --tree /opt/ansible/ -t TREE, --tree=TREE# 指定valut的id --vault-id=VAULT_IDS# 指定vault密码文件 --vault-password-file=VAULT_PASSWORD_FILES# 详细模式，-vvv表示更多，-vvvv表示启用连接调试 -v, --verbose# 查看ansible版本号 --version Connection Options:# ssh认证密码 -k, --ask-pass ask for connection password# 指定秘钥文件 --private-key=PRIVATE_KEY_FILE, --key-file=PRIVATE_KEY_FILE# 指定远程主机运行命令的用户，默认None（以当前系统用户） -u REMOTE_USER, --user=REMOTE_USER# 指定连接方式，默认smart -c CONNECTION, --connection=CONNECTION# 连接远程主机超时时间，单位秒，默认为10秒 -T TIMEOUT, --timeout=TIMEOUT# 指定sftp/scp/ssh连接时的参数（例如ProxyCommand） --ssh-common-args=SSH_COMMON_ARGS# 指定sftp的额外参数（例如-f,-l） --sftp-extra-args=SFTP_EXTRA_ARGS# 指定scp的额外参数（例如-l） --scp-extra-args=SCP_EXTRA_ARGS# 指定ssh的额外参数（例如-R） --ssh-extra-args=SSH_EXTRA_ARGSPrivilege Escalation Options:# 相当于linuxsudu命令，不建议使用，建议使用become -s, --sudo# 所需的sudo用户，不建议使用，建议使用become -U SUDO_USER, --sudo-user=SUDO_USER# 使用su来运行任务，不建议使用，建议使用become -S, --su # 使用su来作为这个用户运行任务，不建议使用，建议使用become -R SU_USER, --su-user=SU_USER# 在运行命令时告诉可以成为另一个用户，没有密码提示 -b, --become# 指定升级特权时的方法，默认sudo，可以选择的有[ sudo | su | pbrun | pfexec | doas | dzdo | ksu | runas | pmrun | enable | machinectl ] --become-method=BECOME_METHOD# 指定运行任务的用户，默认root用户 --become-user=BECOME_USER# sudo密码，不建议使用，建议使用become --ask-sudo-pass# su密码，不建议使用，建议使用become --ask-su-pass# 请求权限提升的密码，即使用become时所需的密码 -K, --ask-become-pass Ad-Hoc执行任务Ansible执行任务时，不论是Ad-Hoc还是Playbook都需要依赖ansible的各个模块，比如使用anible -m ping时就需要使用ping模块，Ansible的模块非常之多，我会单独写一篇笔记来记录常用的模块，因为模块的笔记不论是Ad-Hoc还是Playbook都将使用到。]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible学习笔记-Inventory]]></title>
    <url>%2F2018%2F07%2F30%2FAnsible%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%BB%E6%9C%BA%E6%B8%85%E5%8D%95%2F</url>
    <content type="text"><![CDATA[Ansible主机清单笔记Inventory是Ansible管理主机信息的配置文件，相当于系统HOSTS文件的功能，默认存放在/etc/ansible/hosts。 Ansible使用时可以用过-i或--inventory-file来指定读取。]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible学习笔记-Config]]></title>
    <url>%2F2018%2F07%2F30%2FAnsible%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Ansible配置文件笔记Ansible读取配置文件的顺序 ANSIBLE_CONFIG（如果设置了环境变量） ansible.cfg（当前目录） ~/.ansible.cfg（当前用户宿主目录） /etc/ansible.cfg 命令语法 1ansible-config [view|dump|list] [--help] [options] [ansible.cfg] options --version — 版本号 -c &lt;CONFIG_FILE&gt;, --config &lt;CONFIG_FILE&gt; — 指定配置文件路径 -v, --verbose — 详细模式（-vvv表示更多，-vvvv表示开启连接调试） view 显示当前的配置文件 dump 显示当前设置，如果指定了配置文件，则覆盖配置文件 --only-changed — 仅显示从默认值更改的配置 list 列出lib/constants.py的所有配置，并显示env和配置文件的名称 配置在此记录一下配置文件各个选项的作用： 以下配置项在实际文件中都被注释，大多数情况下使用默认配置，根据自己的需要修改。 为了显示效果，我把注释都去掉了。 defaults日常可能用到的配置，多数保持默认即可。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219[defaults]# Inventory文件# 默认：/etc/ansible/hostsinventory = /etc/ansible/hosts# lib库存放目录# 默认：~/.ansible/plugins/modules:/usr/share/ansible/plugins/moduleslibrary = /usr/share/my_modules/# 模块目录# 默认：~/.ansible/plugins/module_utils:/usr/share/ansible/plugins/module_utilmodule_utils = /usr/share/my_module_utils/# 远程主机临时文件存放目录# 默认：~/.ansible/tmpremote_tmp = ~/.ansible/tmp# 本地临时文件存放目录# 默认：~/.ansible/tmplocal_tmp = ~/.ansible/tmp# 模块黑名单文件（2.5版本及以后）# 默认：Noneplugin_filters_cfg = /etc/ansible/plugin_filters.yml# 并发数# 默认：5forks = 5# 轮询时间间隔# 默认：15poll_interval = 15# sudo用户# 默认：Nonesudo_user = root# 是否需要sudo密码# 默认：Falseask_sudo_pass = True# 是否需要密码，如果使用秘钥验证不需要修改此配置# 默认：Falseask_pass = True# 传输模式# 默认：smart（smart模式会在ssh和paramiko之间切换，取决于操作系统和ssh版本。# 可选：ssh、paramikotransport = smart# 远程链接端口号# 默认：None（将使用连接插件的默认端口号，比如使用ssh连接时会使用控制端ssh的默认端口号）remote_port = 22# 模块在远程主机执行时使用的语言环境设置（只有module_set_locale=True时才生效）# 默认：defaults（尝试将控制器的LANG环境变量设置为远程主机的LANG环境变量）module_lang = Cmodule_set_locale = False# Gathering Facts的默认策略（每个play在执行时都会先执行一个默认任务：收集远程系统的信息）# 默认：implicit（缓存插件将被忽略，Facts数据将被收集，除非在playbook文件中加上gather_facts: false）# 可选：explicit（不收集Facts数据，除非在playbook文件中加上gather_facts: True）# 可选：smart（智能模式，会收集没有发现Facts数据的主机，如果已经收集了，将不会再进行收集任务）# explicit和smart都可使用缓存插件gathering = implicit# 定义获取Facts信息的子集# 默认：all（获取全部信息）# 可选：network（收集最简信息和网络信息）# 可选：hardware（收集硬件信息）# 可选：virtual（收集最简信息和虚拟信息）# 可选：facter（从facter中导入信息，需要远程主机安装facter包）# 可选：ohai（从ohai中导入信息，需要远程主机安装ohai包）# 可以使用逗号来组合（network,hardware）# 可以使用感叹号来排除（!hardware,!facter,!ohai）# 最简信息总是会被采集gather_subset = all# 收集Facts信息的超时时间，不适用ansible hosts -m setup# 默认：10gather_timeout = 10# 角色路径，可用:分隔不同的路径# 默认：~/.ansible/roles:/usr/share/ansible/roles:/etc/ansible/rolesroles_path = /etc/ansible/roles# 首次链接是否需要检查key认证# 默认：True# 建议改为Falsehost_key_checking = False# 改变默认回调，一次只能启用一个'stdout'类型# 可自定义插件来改变回调显示的格式等# 默认：defaultstdout_callback = skippy# 白名单的回调列表，并非所有回调都需要列入白名单，但Ansible自带的许多回调都需要加入白名单列表，因为不希望他们被默认激活# 默认：[]callback_whitelist = timer, mail# 设定任务是否为静态，2.0版本默认动态。若设置为True，则像1.x版本那样为静态。# 默认：Falsetask_includes_static = Falsehandler_includes_static = False# sudo可执行文件名# 默认：sudosudo_exe = sudo# sudo参数# 默认：-H -S -nsudo_flags = -H -S -n# ssh超时时间# 默认：10timeout = 10# 默认的远程连接用户# 默认：None（当前正在执行任务的用户）remote_user = root# 日志文件# 默认：None（不打印日志）log_path = /var/log/ansible.log# ansible默认的执行模块# 默认：commandmodule_name = command# 执行脚本的解释器# 默认：/bin/shexecutable = /bin/sh# 变量重叠式，是优先级高的替换优先级低的还是合并（python中的字典），不建议设置# 默认：replacehash_behaviour = replace# 使角色中的变量无法从其他角色中访问。默认情况下，角色中的变量将在全局变量范围中可见。为了防止这种情况，可以启用以下选项，只有tasks的任务和handlers得任务可以看到角色变量# 默认：Falseprivate_role_vars = yes# jinja2的扩展应用# 默认：[]jinja2_extensions = jinja2.ext.do,jinja2.ext.i18n# 私钥文件路径# 默认：Noneprivate_key_file = /path/to/file# vault密码文件路径# 默认：Nonevault_password_file = /path/to/vault_password_file# 定义一个Jinja2变量，可以插入到Ansible配置模版系统生成的文件中# 默认：Ansible managedansible_managed = Ansible managed# 是否显示跳过的任务状态，不管任务是否被跳过，任务头仍然会显示出来# 默认：Truedisplay_skipped_hosts = True# 清楚情况下playbook会为每个运行的任务打印一个标题。如果指定了标题名，则会打印这个标题；如果没有指定标题，playbook会根据任务的行动来判断当前的任务是什么。如果设置了这个变量，那么playbook会在标题中包含任务的参数。# 默认：Falsedisplay_args_to_stdout = False# 如果所引用的变量名称错误的话, 是否让ansible在执行步骤上失败# 默认：Trueerror_on_undefined_vars = False# 是否禁用系统可能出现的问题相关的警告# 默认：Truesystem_warnings = True# 将不再使用的语言特性显示弃用警告，并将在以后的版本中删除# 默认：Truedeprecation_warnings = True# 默认情况下，当使用shell或命令模块时，该命令将发出警告。这些警告可以通过调整这个设置为False来关闭，还可以使用模块选项警告在任务级别控制它（warn=yes|no）# 默认：Truecommand_warnings = False# 以下为插件存放目录# 默认：~/.ansible/plugins/插件名:/usr/share/ansible/plugins/插件名action_plugins = /usr/share/ansible/plugins/actioncache_plugins = /usr/share/ansible/plugins/cachecallback_plugins = /usr/share/ansible/plugins/callbackconnection_plugins = /usr/share/ansible/plugins/connectionlookup_plugins = /usr/share/ansible/plugins/lookupinventory_plugins = /usr/share/ansible/plugins/inventoryvars_plugins = /usr/share/ansible/plugins/varsfilter_plugins = /usr/share/ansible/plugins/filtertest_plugins = /usr/share/ansible/plugins/testterminal_plugins = /usr/share/ansible/plugins/terminalstrategy_plugins = /usr/share/ansible/plugins/strategy# play的默认策略# 默认：linear（线性）# 可选：free（尽快）以及其他的策略插件strategy = free# 控制callback插件（前面有设置插件的路径）是否在运行/usr/bin/ansible的时候被加载# 默认：Falsebin_ansible_callbacks = False# 是否开启cowsay功能，不想使用可以通过export ANSIBLE_NOCOWS=1来关闭# 默认：Falsenocows = 1# 指定cowsay还是随机选择模块（cow_whitelist选项中的模块）# 默认：defaultcow_selection = random# cowsay白名单# 默认：[‘bud-frogs’, ‘bunny’, ‘cheese’, ‘daemon’, ‘default’, ‘dragon’, ‘elephant-in-snake’, ‘elephant’, ‘eyes’, ‘hellokitty’, ‘kitty’, ‘luke-koala’, ‘meow’, ‘milk’, ‘moofasa’, ‘moose’, ‘ren’, ‘sheep’, ‘small’, ‘stegosaurus’, ‘stimpy’, ‘supermilker’, ‘three-eyes’, ‘turkey’, ‘turtle’, ‘tux’, ‘udder’, ‘vader-koala’, ‘vader’, ‘www’]cow_whitelist=bud-frogs,bunny,cheese,daemon,default,dragon,elephant-in-snake,elephant,eyes,hellokitty,kitty,luke-koala,meow,milk,moofasa,moose,ren,sheep,small,stegosaurus,stimpy,supermilker,three-eyes,turkey,turtle,tux,udder,vader-koala,vader,www# 默认ansible会为输出结果加上颜色,用来更好的区分状态信息和失败信息.如果你想关闭这一功能,可以把nocolor设置为1# 默认：Falsenocolor = 1# fact值默认存储在内存中，可以设置存储在redis等其他缓存型数据库中# 默认：memoryfact_caching = memory# 当playbook失败的情况下，是否创建一个重试文件# 默认：Trueretry_files_enabled = False# 重试文件的路径# 默认：None（当前执行目录）retry_files_save_path = ~/.ansible-retry# Ansible可以优化在循环时使用列表参数调用模块的操作。 而不是每个with_项调用模块一次，该模块会一次调用所有项目一次。该参数记录哪些action是这样操作的。# 默认：apk, apt, dnf, homebrew, openbsd_pkg, pacman, pkgng, yum, zyppersquash_actions = apk,apt,dnf,homebrew,pacman,pkgng,yum,zypper# 任务数据的日志记录# 默认：Falseno_log = False# 在远程主机上是否禁止任务的日志记录，但在控制主机上仍然会记录# 默认：Falseno_target_syslog = False# （不知）allow_world_readable_tmpfiles = False# 控制发送到工作进程的变量的压缩级别，此值必须是从0到9的整数。# 默认：0（不压缩）var_compression_level = 9# 压缩方法# 默认：ZIP_DEFLATED（zip压缩）# 可选：ZIP_STORED（无压缩）module_compression = 'ZIP_DEFLATED'# 控制--diff文件上的截止点（以字节为单位），设置0则为无限制（可能对内存有影响）# 默认：104448max_diff_size = 1048576# 将在2.8本本删除不讨论merge_multiple_cli_flags = True# 是否显示定制的stats信息# 默认：Falseshow_custom_stats = True# 当使用目录作为库存源时忽略的扩展列表# 默认：&#123;&#123;(BLACKLIST_EXTS + ( ‘~’, ‘.orig’, ‘.ini’, ‘.cfg’, ‘.retry’))&#125;&#125;inventory_ignore_extensions = ~, .orig, .bak, .ini, .cfg, .retry, .pyc, .pyo# 针对网络设备优化的备选执行路径，除非了解它是怎么工作的，否则不要改动# 默认：ult: [‘eos’, ‘nxos’, ‘ios’, ‘iosxr’, ‘junos’, ‘enos’, ‘ce’, ‘vyos’, ‘sros’, ‘dellos9’, ‘dellos10’, ‘dellos6’, ‘asa’, ‘aruba’, ‘aireos’, ‘bigip’, ‘ironware’, ‘onyx’]network_group_modules = eos, nxos, ios, iosxr, junos, vyos# 是否允许忽略不安全的数据（通过诸如查找（'foo'）之类的变量，或者作为带有'withfoo'的循环使用的变量）# 默认：Falseallow_unsafe_lookups = False# 为所有的paly设置默认错误# 默认：Falseany_errors_fatal = False inventory主机清单的相关配置。 1234567891011[inventory]# 主机清单插件，也决定使用的顺序# 默认：[‘host_list’, ‘script’, ‘yaml’, ‘ini’, ‘auto’]enable_plugins=host_list, virtualbox, yaml, constructed# 同defaults中inventory_ignore_extensions选项ignore_extensions = .pyc, .pyo, .swp, .bak, ~, .rpm, .md, .txt, ~, .orig, .ini, .cfg, .retry# 在使用目录作为库存源时忽略的模式# 默认：[]ignore_patterns =# 如果为True，未解析的主机为error，否则为警告unparsed_is_failed = False privilege_escalationsudo用户提权的相关配置。 123456789101112# 是否允许sudo命令成为另外一个用户# 默认：falsebecome=True# sudo的方式# 默认：sudobecome_method=sudo# 特权升级时称为的用户# 默认：rootbecome_user=root# sudo后是否验证密码# 默认：Falsebecome_ask_pass=False paramiko_connectionparamiko连接相关配置，目前已经不常用，基本适用ssh连接。 12345678910# 该部分已经不怎么用，简单了解[paramiko_connection]# 不记录新主机的key以提升效率record_host_keys=False# 禁用sudo功能pty=False# 是否显示公钥文件look_for_keys = False# 是否自动添加公钥文件host_key_auto_add = True ssh_connectionssh连接相关配置。 12345678910111213141516171819202122232425262728293031323334[ssh_connection]# ssh连接时的参数# 默认：-C -o ControlMaster=auto -o ControlPersist=60sssh_args = -C -o ControlMaster=auto -o ControlPersist=60s# 保存ControlPath套接字的目录# 默认：~/.ansible/pccontrol_path_dir = ~/.ansible/cp# 保存ControlPath套接字位置# 默认：Nonecontrol_path =# 默认情况下，ansible的执行流程是把生成好的本地python脚本put到远程服务器然后运行。如果开启了pipelining，整个流程少了一个put脚本到远程服务器的步骤，直接在SSH的会话中进行，可以提高整个执行效率。# 默认：False# 可选：True(需要被控的远程服务器将/etc/sudoers中的Defaults requiretty注释掉)pipelining = False# 控制文件传输机制（旧）# 默认：smart（先尝试使用sftp，再尝试使用scp）# 可选：True（只使用scp传输文件）# 可选：False（只使用sftp传输文件）scp_if_ssh = smart# 控制文件传输的机制(新)# 默认：smart（按顺序尝试使用sftp、scp、dd）# 可选：sftp（只使用stfp传输文件）# 可选：scp（只使用scp传输文件）# 可选：piped（只使用dd传输文件）transfer_method = smart# sftp是否使用批处理模式来传输文件# 默认：True# 可选：False（只有在您的sftp版本在批处理模式上有问题时才应禁用）sftp_batch_mode = False# 不知use_tty = True# 在不可到达的情况下，重试SSH连接到主机的次数。对于每次重试尝试，都有一个指数级的回退，所以在第一次尝试之后，有1s等待，然后是2s、4s等，直到30s（max）。# 默认：0retries = 3 persistent_connection持久化连接的相关配置。 12345678910[persistent_connection]# 持久连接超时值，如果在超时值过期之前没有收到新的请求，则连接关闭# 默认：30connect_timeout = 30# 重试超时值，这个值必须大于command_timeout值，并且小于connect_timeout# 默认：30connect_retry_timeout = 15# 响应超时时间值# 默认：10command_timeout = 10 accelerateaccelerate模式（加速模式）的相关配置。 123456789101112131415# 加速连接端口# 默认：5099accelerate_port = 5099# 命令执行超时时间，单位秒# 默认：30accelerate_timeout = 30# 连接超时时间，单位秒# 默认：5.0accelerate_connect_timeout = 5.0# 进程超时时间，单位分钟# 默认：30accelerate_daemon_timeout = 30# 是否允许加载多个私钥# 默认：no#accelerate_multi_key = yes selinuxselinux的相关配置。 1234567[selinux]# 文件系统在处理安全上下文时需要特殊处理，定义复制现有上下文的文件系统# 默认：fuse, nfs, vboxsf, ramfs, 9pspecial_context_filesystems = nfs,vboxsf,fuse,ramfs,9p# 是否以允许libvirt_lxc连接在没有SELinux的情况下工作# 默认：Falselibvirt_lxc_noseclabel = yes colors颜色的相关配置。 123456789101112131415# 这部分根据自己需要修改，基本不会进行配置。[colors]highlight = whiteverbose = bluewarn = bright purpleerror = reddebug = dark graydeprecate = purpleskip = cyanunreachable = redok = greenchanged = yellowdiff_add = greendiff_remove = reddiff_lines = cyan diff打印差异的相关配置。 12345678[diff]# 是否总是打印改变状态是的差异# 默认：no# 可选：yes（相当于--diff）always = no# 打印文件差异时，需要显示上下文的行数# 默认：3context = 3]]></content>
      <categories>
        <category>Ansible</category>
      </categories>
      <tags>
        <tag>Ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos 内存信息]]></title>
    <url>%2F2018%2F07%2F23%2FCentos%20%E5%86%85%E5%AD%98%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[1 Free1.1 CentOS6及以前在CentOS6及以前的版本中，free命令输出是这样的： 123456$free -m total used free shared buffers cachedMem: 1002 769 233 0 62 421-/+ buffers/cache: 286 716Swap: 1153 0 1153 第一行： ​ 系统内存主要分为四部分：used(程序已使用内存)，free(空闲内存)，buffers(buffer cache)， cached(Page cache)。 ​ 系统总内存total = used + free； buffers和cached被算在used里，因此第一行系统已使用内存used = buffers + cached + 第二行系统已使用内存used。 ​ 由于buffers和cached在系统需要时可以被回收使用，因此系统可用内存 = free + buffers + cached； ​ shared为程序共享的内存空间，往往为0。 第二行： 正因为buffers和cached在系统需要时可以被回收使用，因此buffer和cached其实可以可以算作可用内存，因此： 系统可用内存，即第二行的free = 第一行的free + buffers + cached。 系统已使用内存，即第二行的used = total - 第二行free。 第三行： ​ swap内存交换空间使用情况。 1.2 CentOS7及以后 CentOS7及以后free命令的输出如下： 1234# free -m total used free shared buff/cache availableMem: 3440 213 2276 168 950 2778Swap: 0 0 0 buffer和cached被合成一组，加入了一个available，关于此available，文档上的说明如下： MemAvailable: An estimate of how much memory is available for starting new applications, without swapping. 即系统可用内存，之前说过由于buffer和cache可以在需要时被释放回收，系统可用内存即 free + buffer + cache，在CentOS 7之后这种说法并不准确，因为并不是所有的buffer/cache空间都可以被回收。 即available = free + buffer/cache - 不可被回收内存(共享内存段、tmpfs、ramfs等)。 因此在CentOS7之后，用户不需要去计算buffer/cache，即可以看到还有多少内存可用，更加简单直观。 1.3 buffer/cache相关介绍什么是buffer/cache？ buffer 和 cache 是两个在计算机技术中被用滥的名词，放在不通语境下会有不同的意义。在 Linux 的内存管理中，这里的 buffer 指 Linux 内存的： Buffer cache 。这里的 cache 指 Linux 内存中的： Page cache 。翻译成中文可以叫做缓冲区缓存和页面缓存。在历史上，它们一个（ buffer ）被用来当成对 io 设备写的缓存，而另一个（ cache ）被用来当作对 io 设备的读缓存，这里的 io 设备，主要指的是块设备文件和文件系统上的普通文件。但是现在，它们的意义已经不一样了。在当前的内核中， page cache 顾名思义就是针对内存页的缓存，说白了就是，如果有内存是以 page 进行分配管理的，都可以使用 page cache 作为其缓存来管理使用。当然，不是所有的内存都是以页（ page ）进行管理的，也有很多是针对块（ block ）进行管理的，这部分内存使用如果要用到 cache 功能，则都集中到 buffer cache 中来使用。（从这个角度出发，是不是 buffer cache 改名叫做 block cache 更好？）然而，也不是所有块（ block ）都有固定长度，系统上块的长度主要是根据所使用的块设备决定的，而页长度在 X86 上无论是 32 位还是 64 位都是 4k 。 明白了这两套缓存系统的区别，就可以理解它们究竟都可以用来做什么了。 什么是 page cache Page cache 主要用来作为文件系统上的文件数据的缓存来用，尤其是针对当进程对文件有 read ／ write 操作的时候。如果你仔细想想的话，作为可以映射文件到内存的系统调用： mmap 是不是很自然的也应该用到 page cache ？在当前的系统实现里， page cache 也被作为其它文件类型的缓存设备来用，所以事实上 page cache 也负责了大部分的块设备文件的缓存工作。 什么是 buffer cache Buffer cache 则主要是设计用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用。这意味着某些对块的操作会使用 buffer cache 进行缓存，比如我们在格式化文件系统的时候。一般情况下两个缓存系统是一起配合使用的，比如当我们对一个文件进行写操作的时候， page cache 的内容会被改变，而 buffer cache 则可以用来将 page 标记为不同的缓冲区，并记录是哪一个缓冲区被修改了。这样，内核在后续执行脏数据的回写（ writeback ）时，就不用将整个 page 写回，而只需要写回修改的部分即可。 如何回收 cache ？ Linux 内核会在内存将要耗尽的时候，触发内存回收的工作，以便释放出内存给急需内存的进程使用。一般情况下，这个操作中主要的内存释放都来自于对 buffer ／ cache 的释放。尤其是被使用更多的 cache 空间。既然它主要用来做缓存，只是在内存够用的时候加快进程对文件的读写速度，那么在内存压力较大的情况下，当然有必要清空释放 cache ，作为 free 空间分给相关进程使用。所以一般情况下，我们认为 buffer/cache 空间可以被释放，这个理解是正确的。 但是这种清缓存的工作也并不是没有成本。理解 cache 是干什么的就可以明白清缓存必须保证 cache 中的数据跟对应文件中的数据一致，才能对 cache 进行释放。所以伴随着 cache 清除的行为的，一般都是系统 IO 飙高。因为内核要对比 cache 中的数据和对应硬盘文件上的数据是否一致，如果不一致需要写回，之后才能回收。 在系统中除了内存将被耗尽的时候可以清缓存以外，我们还可以使用下面这个文件来人工触发缓存清除的操作： 12[root@tencent64 ~]# cat /proc/sys/vm/drop_caches 1 方法是： 1echo 1 &gt; /proc/sys/vm/drop_caches 当然，这个文件可以设置的值分别为 1 、 2 、 3 。它们所表示的含义为： echo 1 &gt; /proc/sys/vm/drop_caches:表示清除 pagecache 。 echo 2 &gt; /proc/sys/vm/drop_caches:表示清除回收 slab 分配器中的对象（包括目录项缓存和 inode 缓存）。 slab 分配器是内核中管理内存的一种机制，其中很多缓存数据实现都是用的 pagecache 。 echo 3 &gt; /proc/sys/vm/drop_caches:表示清除 pagecache 和 slab 分配器中的缓存对象。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-staticmethod、classmethod]]></title>
    <url>%2F2018%2F07%2F14%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-staticmethod%E3%80%81classmethod%2F</url>
    <content type="text"><![CDATA[网上看到的 写的很清晰之后再整理 1234567891011121314151617181920212223# 静态函数，类函数，成员函数 # 静态函数(@staticmethod): 即静态方法,主要处理与这个类的逻辑关联, 如验证数据; # 类函数(@classmethod):即类方法, 更关注于从类中调用方法, 而不是在实例中调用方法, 如构造重载; # 成员函数: 实例的方法, 只能通过实例进行调用; class Person: grade=1 def __init__(self,name): self.name = name def sayHi(self):#加 self 区别于普通函数 print 'Hello, your name is?',self.name # 声明静态，去掉则编译报错;还有静态方法不能访问类变量和实例变量 # 好处：不需要定义实例即可使用这个方法。另外，多个实例共享此静态方法。 @staticmethod def sayName():#使用了静态方法，则不能再使用 self print "my name is king"#,grade,#self.name # 类方法：一个类方法就可以通过类或它的实例来调用的方法, # 不管你是用类来调用这个方法还是类实例调用这个方法,该方法的第一个参数总是定义该方法的类对象。 @classmethod def classMethod(cls): print("class method") 顺便提一下类属性和实例属性： 类属性可用于为实例属性提供默认值。 类属性就是类中不加self的属性： 12345678910class Vector2d: typecode = 'd' def __init__(self, x, y): self.__x = float(x) self.__y = float(y) def __bytes__(self): return (bytes([ord(self.typecode)]) + bytes(array(self.typecode, self))) 123456789101112131415161718Vector2d.typecodeOut[11]: 'd' # 创建实例时，类属性会被当成实例属性并为实例属性提供默认值v1 = Vector2d(1.1, 2.2)dumpd = bytes(v1)dumpdOut[14]: b'd\x9a\x99\x99\x99\x99\x99\xf1?\x9a\x99\x99\x99\x99\x99\x01@' # 修改实例属性v1.typecode = 'f'dumpf = bytes(v1)dumpfOut[18]: b'f\xcd\xcc\x8c?\xcd\xcc\x0c@' # 类属性没有变Vector2d.typecodeOut[19]: 'd' 这种类型可以直接通过类来访问类属性； 创建实例时，类属性会被当成实例属性并为实例属性提供默认值。 所以说添加方法时也可以使用self.typecode来获取实例属性。 如果想修改类属性的值，必须直接在类上修改，不能通过实例修改。 1Vector2d.typecode = 'f' 有种修改方法更符合Python风格，而且效果更持久，也更有针对性。类属性是公开的，因此会被子类继承，于是经常会创建一个子类，只用于定制类的数据属性。 Django基于类的视图就大量使用了这个技术。 123from vector2d_v1 import Vector2dclass ShortVector2d(Vector2d): typecode = 'f']]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-time模块]]></title>
    <url>%2F2018%2F07%2F13%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-time%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[在Python中，与时间处理有关的模块包括：time，datetime以及calendar。 Time模块-时间获取和转换一些术语和约定的解释 UTC（Coordinated Universal Time，世界协调时 ），也叫格林威治天文时间，是世界标准时间。在中国为UTC+8 DST（Daylight Saving Time ），即夏令时，是一种为节约能源而人为规定地方时间的制度，一般在天亮早的夏季人为将时间提前一小时。在中国1992年已经停止 一些实时函数的计算精度可能低于它们建议的值或参数，例如在大部分Unix系统，时钟一秒钟”滴答“50~100次 两种表现形式时间戳（timestamp）： 通常来说，时间戳表示的是从1970年1月1日00:00:00开始按秒计算的偏移量（time.gmtime(0)），此模块中的函数无法处理1970纪元年以前的日期和时间或太遥远的未来（处理极限取决于C函数库，对于32位的系统来说是2038年） 时间元组（time.struct_time）： gmtime()，localtime()和strptime()以时间元组（struct_time）的形式返回。 索引值（Index） 属性（Attribute） 值（Values） 0 tm_year（年） （例如：2015） 1 tm_mon（月） 1 ~ 12 2 tm_mday（日） 1 ~ 31 3 tm_hour（时） 0 ~ 23 4 tm_min（分） 0 ~ 59 5 tm_sec（秒） 0 ~ 61（见下方注1） 6 tm_wday（星期几） 0 ~ 6（0 表示星期一） 7 tm_yday（一年中的第几天） 1 ~ 366 8 tm_isdst（是否为夏令时） 0， 1， -1（-1 代表夏令时） 注1：60 代表闰秒，61 是基于历史原因保留。 属性timezonetime.timezone 属性返回当地时间与标准UTC时间的误差，以秒计 （美洲 &gt;0；大部分欧洲，亚洲，非洲 &lt;= 0 1234&gt;&gt;&gt; time.timezone-28800&gt;&gt;&gt; -8*60*60-28800 altzonetime.altzone属性返回当地夏令时时间与标准UTC时间的误差，以秒计。 1234&gt;&gt;&gt; time.altzone-32400&gt;&gt;&gt; -9*60*60-32400 daylight 当地时间是否反映夏令时，默认为0 12&gt;&gt;&gt; time.daylight0 time.tznametime.tzname属性包含一对根据情况的不同而不同的字符串，分别是带夏令时的本地时区名称，和不带的。 (标准时区名称,夏令时时区名称的元组) 12&gt;&gt;&gt; time.tzname('ÖÐ¹ú±ê×¼Ê±¼ä', 'ÖÐ¹úÏÄÁîÊ±') 函数timetime() 返回当前时间的时间戳（1970纪元年后经过的浮点秒数） 12&gt;&gt;&gt; time.time()1531461304.5539289 asctime asctime(…) asctime([tuple]) -&gt; string 接受时间元组并返回一个可读的形式为”Tue Dec 11 18:07:14 2015”（2015年12月11日 周二 18时07分14秒）的 24 个字符的字符串。 12345&gt;&gt;&gt; t = time.localtime()&gt;&gt;&gt; ttime.struct_time(tm_year=2018, tm_mon=7, tm_mday=13, tm_hour=13, tm_min=38, tm_sec=44, tm_wday=4, tm_yday=194, tm_isdst=0)&gt;&gt;&gt; time.asctime(t)'Fri Jul 13 13:38:44 2018' clock clock(…) clock() -&gt; floating point number 用以浮点数计算的秒数返回当前的 CPU 时间。用来衡量不同程序的耗时，比 time.time() 更有用。 Python 3.3 以后不被推荐，由于该方法依赖操作系统，建议使用 perf_counter() 或 process_time() 代替（一个返回系统运行时间，一个返回进程运行时间，请按照实际需求选择） 在不同的系统上含义不同。在UNIX系统上，它返回的是“进程时间”，它是用秒表示的浮点数（时间戳）。而在Windows中，第一次调用，返回的是进程运行的实际时间。而第二次之后的调用是自第一次调用以后到现在的运行时间。（实际上是以WIN32上QueryPerformanceCounter()为基础，它比毫秒表示更为精确） 12345678910# 在Windows中，第一次调用，返回的是进程运行的实际时间。&gt;&gt;&gt; print('clock_1: %s' % time.clock() )clock_1: 4.276543543552533e-07# 第二次之后的调用是自第一次调用以后到现在的运行时间。&gt;&gt;&gt; print('clock_1: %s' % time.clock() )clock_1: 2.2088326019731115&gt;&gt;&gt; print('clock_2: %s' % time.clock() )clock_2: 8.735795567533678&gt;&gt;&gt; print('clock_3: %s' % time.clock() )clock_3: 13.871665204801532 ctime ctime(…) ctime(seconds) -&gt; string 把一个时间戳（按秒计算的浮点数）转化为time.asctime()的形式。如果参数未给或者为None的时候，将会默认time.time()即当前时间戳为参数。它的作用相当于time.asctime(time.localtime(secs))。 123456&gt;&gt;&gt; time.ctime()'Fri Jul 13 14:36:18 2018'&gt;&gt;&gt; time.time()1531463798.9535468&gt;&gt;&gt; time.ctime(1531463798.9535468)'Fri Jul 13 14:36:38 2018' localtime localtime(…) localtime([seconds]) -&gt; (tm_year,tm_mon,tm_mday,tm_hour,tm_min, tm_sec,tm_wday,tm_yday,tm_isdst) 将一个时间戳转换为当前时区的struct_time。secs参数未提供，则以当前时间为准。 123456&gt;&gt;&gt; time.localtime()time.struct_time(tm_year=2018, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=45, tm_sec=41, tm_wday=4, tm_yday=194, tm_isdst=0)&gt;&gt;&gt; time.time()1531464356.65697&gt;&gt;&gt; time.localtime(1531464356.65697)time.struct_time(tm_year=2018, tm_mon=7, tm_mday=13, tm_hour=14, tm_min=45, tm_sec=56, tm_wday=4, tm_yday=194, tm_isdst=0) gmtime gmtime(…) gmtime([seconds]) -&gt; (tm_year, tm_mon, tm_mday, tm_hour, tm_min, tm_sec, tm_wday, tm_yday, tm_isdst) 和localtime()方法类似，gmtime()方法是将一个时间戳转换为UTC时区（0时区）的struct_time。 12&gt;&gt;&gt; time.gmtime()time.struct_time(tm_year=2018, tm_mon=7, tm_mday=13, tm_hour=6, tm_min=47, tm_sec=35, tm_wday=4, tm_yday=194, tm_isdst=0) mktime mktime(…) mktime(tuple) -&gt; floating point number 将一个struct_time转化为时间戳。 12&gt;&gt;&gt; time.mktime(time.localtime())1531464557.0 perf_counter perf_counter(…) perf_counter() -&gt; float 返回计时器的精准时间（系统的运行时间），包含整个系统的睡眠时间。由于返回值的基准点是未定义的，所以，只有连续调用的结果之间的差才是有效的。 123456&gt;&gt;&gt; t0 = time.perf_counter()&gt;&gt;&gt; t01420.9757357472427&gt;&gt;&gt; t1 = time.perf_counter()&gt;&gt;&gt; t1 - t023.103979024409227 process_time(未理解) process_time(…) process_time() -&gt; float 返回当前进程执行 CPU 的时间总和，不包含睡眠时间。由于返回值的基准点是未定义的，所以，只有连续调用的结果之间的差才是有效的。 12&gt;&gt;&gt; time.process_time()0.125 sleep sleep(…) sleep(seconds) 线程推迟指定的时间运行，单位为秒。 1sleep(2) strfimestrftime(…) strftime(format[, tuple]) -&gt; string 把一个代表时间的元组或者struct_time（如由time.localtime()和time.gmtime()返回）转化为格式化的时间字符串。如果t未指定，将传入time.localtime()。如果元组中任何一个元素越界，ValueError的错误将会被抛出。 format 格式如下： 格式 含义 备注 %a 本地（locale）简化星期名称 %A 本地完整星期名称 %b 本地简化月份名称 %B 本地完整月份名称 %c 本地相应的日期和时间表示 %d 一个月中的第几天（01 - 31） %H 一天中的第几个小时（24 小时制，00 - 23） %l 一天中的第几个小时（12 小时制，01 - 12） %j 一年中的第几天（001 - 366） %m 月份（01 - 12） %M 分钟数（00 - 59） %p 本地 am 或者 pm 的相应符 注1 %S 秒（01 - 61） 注2 %U 一年中的星期数（00 - 53 星期天是一个星期的开始）第一个星期天之前的所有天数都放在第 0 周 注3 %w 一个星期中的第几天（0 - 6，0 是星期天） 注3 %W 和 %U 基本相同，不同的是 %W 以星期一为一个星期的开始 %x 本地相应日期 %X 本地相应时间 %y 去掉世纪的年份（00 - 99） %Y 完整的年份 %z 用 +HHMM 或 -HHMM 表示距离格林威治的时区偏移（H 代表十进制的小时数，M 代表十进制的分钟数） %Z 时区的名字（如果不存在为空字符） %% %号本身 注1：“%p”只有与“%I”配合使用才有效果。 注2：60 代表闰秒，61 是基于历史原因保留。 注3：当使用 strptime() 函数时，只有当在这年中的周数和天数被确定的时候 %U 和 %W 才会被计算。 12&gt;&gt;&gt; time.strftime('%Y-%m-%d %X')'2018-07-13 15:06:39' strptime strptime(…) strptime(string, format) -&gt; struct_time 把一个格式化时间字符串转化为 struct_time。实际上它和 strftime() 是逆操作。 123456&gt;&gt;&gt; time_now = time.strftime('%y-%m-%d %X') &gt;&gt;&gt; time_now '18-07-13 15:09:04' &gt;&gt;&gt; time.strptime(time_now, '%y-%m-%d %X') time.struct_time(tm_year=2018, tm_mon=7, tm_mday=13, tm_hour=15, tm_min=9, tm_sec=4, tm_wday=4, tm_yday=194, tm_isdst=-1) 总结根据之前描述，在Python中共有三种表达方式： timestamp tuple或者struct_time 格式化字符串。它们之间的转化如图所示：]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-函数内省]]></title>
    <url>%2F2018%2F07%2F13%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E5%87%BD%E6%95%B0%E5%86%85%E7%9C%81%2F</url>
    <content type="text"><![CDATA[学习《流畅的Python时》一等函数的函数内省时，有一些疑问： 什么是函数内省？ 函数内省有什么作用？ 书上没有涉及内省的概念，所以在网上查阅了一下资料来记录一下函数内省。 这篇笔记抄录于《Python自省指南》，由于这篇文章使用的是Python 2，我用Python 3再抄录一遍。 以及后面有我自己的补充，主要是加上书上的内容以及自己的理解。 什么是自省 在日常生活中，自省（introspection）是一种自我检查行为。自省是指对某人自身思想、情绪、动机和行为的检查。伟大的哲学家苏格拉底将生命中的大部分时间用于自我检查，并鼓励他的雅典朋友们也这样做。他甚至对自己作出了这样的要求：“未经自省的生命不值得存在。” 在计算机编程中，自省是指这种能力：检查某些事物以确定它是什么，它知道什么以及它能做什么。自省向程序员提供了极大的灵活性和控制力。一旦您使用了支持自省的编程预言，就会产生类似这样的感觉：“未经检查的对象不值得实例化”。 本文介绍了 Python 编程语言的自省能力。整个 Python 语言对自省提供了深入而广泛的支持。实际上，很难想象假如 Python 语言没有其自省特性是什么样子。在读完本文时，您应该能够非常轻松地洞察到自己 Python 对象的“灵魂”。 自省和反射 在网上有很多资料把自省和反射理解为一个概念，也有理解为不同概念的，在此讨论一下。 自省在wiki上的解释：在计算机科学中，内省是指计算机程序在运行时（Run time）检查对象（Object）类型的一种能力，通常也可以称作运行时类型检查。 不应该将内省和反射混淆。相对于内省，反射更进一步，是指计算机程序在运行时（Run time）可以访问、检测和修改它本身状态或行为的一种能力。 搜索自省和反射的区别，大部分都是讲java的，而python几乎都把自省和反射理解为一个概念。 有一个博客写到——我感 反射与自省其实是一回事 只是在JAVA中把自省说成反射，在PYTHON中把反射说成自省。 我也是这么觉得，所以目前姑且先把自省和反射理解为一个概念。以后有新的想法再来解释。 在深入研究更高级的技术之前，我们尽可能用最普通的方式来研究 Python 自省。有些读者甚至可能会争论说：我们开始时所讨论的特性不应称之为“自省”。我们必须承认，它们是否属于自省的范畴还有待讨论。但从本节的意图出发，我们所关心的只是找出有趣问题的答案。 现在让我们以交互方式使用 Python 来开始研究。这是前面已经在使用的一种方式。 联机帮助在交互模式下，用 help 向 Python 请求帮助。 1234567891011121314151617&gt;&gt;&gt; help()Welcome to Python 3.6&apos;s help utility!If this is your first time using Python, you should definitely check outthe tutorial on the Internet at https://docs.python.org/3.6/tutorial/.Enter the name of any module, keyword, or topic to get help on writingPython programs and using Python modules. To quit this help utility andreturn to the interpreter, just type &quot;quit&quot;.To get a list of available modules, keywords, symbols, or topics, type&quot;modules&quot;, &quot;keywords&quot;, &quot;symbols&quot;, or &quot;topics&quot;. Each module also comeswith a one-line summary of what it does; to list the modules whose nameor summary contain a given string such as &quot;spam&quot;, type &quot;modules spam&quot;.help&gt; 这时候就进入了联机帮助状态，根据提示输入keywords： 12345678910111213help&gt; keywordsHere is a list of the Python keywords. Enter any keyword to get more help.False def if raiseNone del import returnTrue elif in tryand else is whileas except lambda withassert finally nonlocal yieldbreak for notclass from orcontinue global pass 现在显示出了Python关键词的列表。按照说明我们继续输入关键字获取关键字的相关文档。 12345678910111213141516171819202122232425262728293031323334353637help&gt; in Membership test operations ************************** The operators &quot;in&quot; and &quot;not in&quot; test for membership. &quot;x in s&quot; evaluates to &quot;True&quot; if *x* is a member of *s*, and &quot;False&quot; otherwise. &quot;x not in s&quot; returns the negation of &quot;x in s&quot;. All built-in sequences and set types support this as well as dictionary, for which &quot;in&quot; tests whether the dictionary has a given key. For container types such as list, tuple, set, frozenset, dict, or collections.deque, the expression &quot;x in y&quot; is equivalent to &quot;any(x is e or x == e for e in y)&quot;. For the string and bytes types, &quot;x in y&quot; is &quot;True&quot; if and only if *x* is a substring of *y*. An equivalent test is &quot;y.find(x) != -1&quot;. Empty strings are always considered to be a substring of any other string, so &quot;&quot;&quot; in &quot;abc&quot;&quot; will return &quot;True&quot;. For user-defined classes which define the &quot;__contains__()&quot; method, &quot;x in y&quot; returns &quot;True&quot; if &quot;y.__contains__(x)&quot; returns a true value, and &quot;False&quot; otherwise. For user-defined classes which do not define &quot;__contains__()&quot; but do define &quot;__iter__()&quot;, &quot;x in y&quot; is &quot;True&quot; if some value &quot;z&quot; with &quot;x == z&quot; is produced while iterating over &quot;y&quot;. If an exception is raised during the iteration, it is as if &quot;in&quot; raised that exception. Lastly, the old-style iteration protocol is tried: if a class defines &quot;__getitem__()&quot;, &quot;x in y&quot; is &quot;True&quot; if and only if there is a non- negative integer index *i* such that &quot;x == y[i]&quot;, and all lower integer indices do not raise &quot;IndexError&quot; exception. (If any other exception is raised, it is as if &quot;in&quot; raised that exception). The operator &quot;not in&quot; is defined to have the inverse true value of &quot;in&quot;. Related help topics: SEQUENCEMETHODS 正如您从这个示例可以看到的，Python 的联机帮助实用程序会显示关于各种主题或特定对象的信息。帮助实用程序很有用，并确实利用了 Python 的自省能力。但仅仅使用帮助不会揭示帮助是如何获得其信息的。而且，因为本文的目的是揭示 Python 自省的所有秘密，所以我们必须迅速地跳出对帮助实用程序的讨论。 最后让我们来获得一个可用模块的列表。 模块只是包含 Python 代码的文本文件，其名称后缀是 .py 。如果在 Python 提示符下输入help(&#39;modules&#39;) ，或在 help 提示符下输入 modules ，则会看到一长列可用模块，类似于下面所示的部分列表。自己尝试它以观察您的系统中有哪些可用模块，并了解为什么会认为 Python 是“自带电池”的。 就是说 Python 在被安装时，就带了很多模块，这些模块是你以后开发中会用到的，比喻成电池，好比开发的助力工具），或者说是 Python 一被安装，就已经包含有的模块，不用我们费力再安装了。 1234567891011121314help&gt; modulesPlease wait a moment while I gather a list of all available modules...D:\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\IPython\kernel\__init__.py:13: ShimWarning: The `IPython.kernel` package has been deprecated since IPython 4.0.You should import from ipykernel or jupyter_client instead. &quot;You should import from ipykernel or jupyter_client instead.&quot;, ShimWarning)IPython asynchat idna sched__future__ asyncio imaplib secrets_ast asyncore imghdr select...ast idlelib runpyEnter any module name to get more help. Or, type &quot;modules spam&quot; to searchfor modules whose name or summary contain the string &quot;spam&quot;. dir()函数尽管查找和导入模块相对容易，但要记住每个模块包含什么却不是这么简单。你或许并不希望总是必须查看源代码来找出答案。幸运的是，Python 提供了一种方法，可以使用内置的 dir() 函数来检查模块（以及其它对象）的内容。 其实，这个东西我们已经一直在使用。 dir() 函数可能是 Python 自省机制中最著名的部分了。它返回传递给它的任何对象的属性名称经过排序的列表。如果不指定对象，则 dir() 返回当前作用域中（理解为某个范围）的名称。让我们将 dir() 函数应用于 keyword 模块，并观察它揭示了什么： 123&gt;&gt;&gt; import keyword&gt;&gt;&gt; dir(keyword)['__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'iskeyword', 'kwlist', 'main'] 如果不带任何参数，则 dir() 返回当前作用域中的名称。请注意，因为我们先前导入了 keyword ，所以它们出现在列表中。导入模块将把该模块的名称添加到当前作用域： 12&gt;&gt;&gt; dir()['__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'keyword'] dir() 函数是内置函数，这意味着我们不必为了使用该函数而导入模块。不必做任何操作，Python 就可识别内置函数。 再观察，看到调用 dir() 后返回了这个名称 __builtins__ 。也许此处有连接。让我们在 Python 提示符下输入名称 __builtins__ ，并观察 Python 是否会告诉我们关于它的任何有趣的事情： 12&gt;&gt;&gt; __builtins__&lt;module '__builtin__' (built-in)&gt; 因此 __builtins__ 看起来象是当前作用域中绑定到名为 __builtin__ 的模块对象的名称。（因为模块不是只有多个单一值的简单对象，所以 Python 改在尖括号中显示关于模块的信息。） 注：如果您在磁盘上寻找 __builtin__.py 文件，将空手而归。这个特殊的模块对象是 Python 解释器凭空创建的，因为它包含着解释器始终可用的项。尽管看不到物理文件，但我们仍可以将 dir() 函数应用于这个对象，以观察所有内置函数、错误对象以及它所包含的几个杂项属性。 12&gt;&gt;&gt; dir(__builtins__)['ArithmeticError', 'AssertionError', 'AttributeError', 'BaseException', 'BlockingIOError', 'BrokenPipeError', 'BufferError', 'BytesWarning', 'ChildProcessError', 'ConnectionAbortedError', 'ConnectionError', 'ConnectionRefusedError', 'ConnectionResetError', 'DeprecationWarning', 'EOFError', 'Ellipsis', 'EnvironmentError', 'Exception', 'False', 'FileExistsError', 'FileNotFoundError', 'FloatingPointError', 'FutureWarning', 'GeneratorExit', 'IOError', 'ImportError', 'ImportWarning', 'IndentationError', 'IndexError', 'InterruptedError', 'IsADirectoryError', 'KeyError', 'KeyboardInterrupt', 'LookupError', 'MemoryError', 'ModuleNotFoundError', 'NameError', 'None', 'NotADirectoryError', 'NotImplemented', 'NotImplementedError', 'OSError', 'OverflowError', 'PendingDeprecationWarning', 'PermissionError', 'ProcessLookupError', 'RecursionError', 'ReferenceError', 'ResourceWarning', 'RuntimeError', 'RuntimeWarning', 'StopAsyncIteration', 'StopIteration', 'SyntaxError', 'SyntaxWarning', 'SystemError', 'SystemExit', 'TabError', 'TimeoutError', 'True', 'TypeError', 'UnboundLocalError', 'UnicodeDecodeError', 'UnicodeEncodeError', 'UnicodeError', 'UnicodeTranslateError', 'UnicodeWarning', 'UserWarning', 'ValueError', 'Warning', 'WindowsError', 'ZeroDivisionError', '_', '__build_class__', '__debug__', '__doc__', '__import__', '__loader__', '__name__', '__package__', '__spec__', 'abs', 'all', 'any', 'ascii', 'bin', 'bool', 'bytearray', 'bytes', 'callable', 'chr', 'classmethod', 'compile', 'complex', 'copyright', 'credits', 'delattr', 'dict', 'dir', 'divmod', 'enumerate', 'eval', 'exec', 'exit', 'filter', 'float', 'format', 'frozenset', 'getattr', 'globals', 'hasattr', 'hash', 'help', 'hex', 'id', 'input', 'int', 'isinstance', 'issubclass', 'iter', 'len', 'license', 'list', 'locals', 'map', 'max', 'memoryview', 'min', 'next', 'object', 'oct', 'open', 'ord', 'pow', 'print', 'property', 'quit', 'range', 'repr', 'reversed', 'round', 'set', 'setattr', 'slice', 'sorted', 'staticmethod', 'str', 'sum', 'super', 'tuple', 'type', 'vars', 'zip'] dir() 函数适用于所有对象类型，包括字符串、整数、列表、元组、字典、函数、定制类、类实例和类方法。 例如将 dir() 应用于字符串对象，如您所见，即使简单的 Python 字符串也有许多属性 ： 12&gt;&gt;&gt; dir("This is a str.")['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill'] 我们来看一下其他的对象类型： 12&gt;&gt;&gt; dir(43)['__abs__', '__add__', '__and__', '__bool__', '__ceil__', '__class__', '__delattr__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floor__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__le__', '__lshift__', '__lt__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__trunc__', '__xor__', 'bit_length', 'conjugate', 'denominator', 'from_bytes', 'imag', 'numerator', 'real', 'to_bytes'] 12&gt;&gt;&gt; dir(&#123;&#125;)['__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'items', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values'] 12&gt;&gt;&gt; dir(set)['__and__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__iand__', '__init__', '__init_subclass__', '__ior__', '__isub__', '__iter__', '__ixor__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__or__', '__rand__', '__reduce__', '__reduce_ex__', '__repr__', '__ror__', '__rsub__', '__rxor__', '__setattr__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__xor__', 'add', 'clear', 'copy', 'difference', 'difference_update', 'discard', 'intersection', 'intersection_update', 'isdisjoint', 'issubset', 'issuperset', 'pop', 'remove', 'symmetric_difference', 'symmetric_difference_update', 'union', 'update'] 12&gt;&gt;&gt; dir(list)['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort'] 通过dir()函数获取对象的属性以及方法，我们就可以随时的了解我们创建的对象包含了什么，以及可以做些什么。 文档字符串在许多 dir() 示例中，您可能会注意到的一个属性是 __doc__ 属性。这个属性是一个字符串，它包含了描述对象的注释。Python 称之为文档字符串或 docstring 。 如果模块、类、方法或函数定义的第一条语句是字符串，那么该字符串会作为对象的 __doc__ 属性与该对象关联起来。例如，看一下 str 类型对象的文档字符串。因为文档字符串通常包含嵌入的换行 \n ，我们将使用 Python 的 print 语句，以便输出更易于阅读： 1234567891011&gt;&gt;&gt; print(str.__doc__)str(object='') -&gt; strstr(bytes_or_buffer[, encoding[, errors]]) -&gt; strCreate a new string object from the given object. If encoding orerrors is specified, then the object must expose a data bufferthat will be decoded using the given encoding and error handler.Otherwise, returns the result of object.__str__() (if defined)or repr(object).encoding defaults to sys.getdefaultencoding().errors defaults to 'strict'. 利用自省检查 Python 对象前面已经好几次提到了“对象（object）”这个词，但一直没有真正定义它。编程环境中的对象很像。现实世界中的对象。实际的对象有一定的形状、大小、重量和其它特征。实际的对象还能够对其环境进行响应、与其它对象交互或执行任务。计算机中的对象试图模拟我们身边现实世界中的对象，包括象文档、日程表和业务过程这样的抽象对象。 其实，我总觉得把 object 翻译成对象，让人感觉很没有具象的感觉，因为在汉语里面，对象是一个很笼统的词汇。另外一种翻译，流行于台湾，把它称为“物件”，倒是挺不错的理解。当然，名词就不纠缠了，关键是理解内涵。关于面向对象编程，可以阅读维基百科的介绍——面向对象程序设计——先了解大概。 类似于实际的对象，几个计算机对象可能共享共同的特征，同时保持它们自己相对较小的变异特征。想一想您在书店中看到的书籍。书籍的每个物理副本都可能有污迹、几张破损的书页或唯一的标识号。尽管每本书都是唯一的对象，但都拥有相同标题的每本书都只是原始模板的实例，并保留了原始模板的大多数特征。 对于面向对象的类和类实例也是如此。例如，可以看到每个 Python 子符串都被赋予了一些属性， dir() 函数揭示了这些属性。 于是在计算机术语中，对象是拥有标识和值的事物，属于特定类型、具有特定特征和以特定方式执行操作。 并且，对象从一个或多个父类继承了它们的许多属性。除了关键字和特殊符号（象运算符，如 + 、 - 、 * 、 ** 、 / 、 % 、 &lt; 、 &gt; 等）外，Python 中的所有东西都是对象。Python 具有一组丰富的对象类型：字符串、整数、浮点、列表、元组、字典、函数、类、类实例、模块、文件等。 当您有一个任意的对象（也许是一个作为参数传递给函数的对象）时，可能希望知道一些关于该对象的情况。如希望 Python 告诉我们： 对象的名称是什么？ 这是哪种类型的对象？ 对象知道些什么？ 对象能做些什么？ 对象的父对象是谁？ 名称并非所有对象都有名称，但那些有名称的对象都将名称存储在其 __name__ 属性中。注：名称是从对象而不是引用该对象的变量中派生的。 1234567891011121314&gt;&gt;&gt; l = list# dir(l)和dir(list)结果相同&gt;&gt;&gt; dir(l)['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']&gt;&gt;&gt; dir(list)['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']&gt;&gt;&gt; l.__name__ 'list'&gt;&gt;&gt; list.__name__'list'# 我之前讲过__name__的用法&gt;&gt;&gt; __name__ #这是不一样的'__main__' 模块拥有名称，Python 解释器本身被认为是顶级模块或主模块。当以交互的方式运行 Python 时，局部 __name__ 变量被赋予值 &#39;__main__&#39; 。同样地，当从命令行执行 Python 模块，而不是将其导入另一个模块时，其 __name__ 属性被赋予值 &#39;__main__&#39; ，而不是该模块的实际名称。这样，模块可以查看其自身的 __name__ 值来自行确定它们自己正被如何使用，是作为另一个程序的支持，还是作为从命令行执行的主应用程序。因此，下面这条惯用的语句在 Python 模块中是很常见的： 12345678if __name__ == '__main__': # Do something appropriate here, like calling a # main() function defined elsewhere in this module. main()else: # Do nothing. This module has been imported by another # module that wants to make use of the functions, # classes and other useful bits it has defined. 类型type() 函数有助于我们确定对象是字符串还是整数，或是其它类型的对象。它通过返回类型对象来做到这一点。（其实就相当于访问对象__class__属性，个人理解） 12345678910111213141516&gt;&gt;&gt; list.__class__&lt;class 'type'&gt;&gt;&gt;&gt; type(list)&lt;class 'type'&gt;&gt;&gt;&gt; [].__class__&lt;class 'list'&gt;&gt;&gt;&gt; type([])&lt;class 'list'&gt;&gt;&gt;&gt; &#123;&#125;.__class__&lt;class 'dict'&gt;&gt;&gt;&gt; type(&#123;&#125;)&lt;class 'dict'&gt;&gt;&gt;&gt; 'python'.__class__&lt;class 'str'&gt;&gt;&gt;&gt; type('python')&lt;class 'str'&gt; 标识先前说过，每个对象都有标识、类型和值。值得注意的是，可能有多个变量引用同一对象，同样地，变量可以引用看起来相似（有相同的类型和值），但拥有截然不同标识的多个对象。当更改对象时（如将某一项添加到列表），这种关于对象标识的概念尤其重要，如在下面的示例中， list_1 和 list_2 变量引用同一个列表对象。正如您在示例中所见， id() 函数给任何给定对象返回唯一的标识符。 1234567891011121314151617181920&gt;&gt;&gt; print(id.__doc__)Return the identity of an object.This is guaranteed to be unique among simultaneously existing objects.(CPython uses the object's memory address.)# list_1和list_2标识不同&gt;&gt;&gt; list_1 = [1, 2, 3]&gt;&gt;&gt; list_2 = [1, 2, 3]&gt;&gt;&gt; id(list_1)1740112651912&gt;&gt;&gt; id(list_2)1740117536520# a和b标识相同&gt;&gt;&gt; a = 1&gt;&gt;&gt; b = 1&gt;&gt;&gt; id(a)1653632016&gt;&gt;&gt; id(b)1653632016 Python中会为每个出现的对象分配内存，哪怕他们的值完全相等（注意是相等不是相同）。 属性对象拥有属性，并且 dir() 函数会返回这些属性的列表。但是，有时我们只想测试一个或多个属性是否存在。 如果对象具有我们正在考虑的属性，那么通常希望只检索该属性。这个任务可以由hasattr()和 getattr() 函数来完成。 123456789101112131415161718192021222324252627&gt;&gt;&gt; print(hasattr.__doc__)#判断一个对象里面是否有name属性或者name方法，返回BOOL值，有name特性返回True， 否则返回False。需要注意的是name要用括号括起来Return whether the object has an attribute with the given name.This is done by calling getattr(obj, name) and catching AttributeError.&gt;&gt;&gt; print(getattr.__doc__)# 获取对象object的属性或者方法，如果存在打印出来，如果不存在，打印出默认值，默认值可选。# 需要注意的是，如果是返回的对象的方法，返回的是方法的内存地址，如果需要运行这个方法，可以在后面添加一对括号。getattr(object, name[, default]) -&gt; valueGet a named attribute from an object; getattr(x, 'y') is equivalent to x.y.When a default argument is given, it is returned when the attribute doesn'texist; without it, an exception is raised in that case.&gt;&gt;&gt; hasattr(list, '__add__')True&gt;&gt;&gt; hasattr(dict, '__add__')False&gt;&gt;&gt; list_1 = [1, 3, 2]&gt;&gt;&gt; getattr(list_1, 'sort')&lt;built-in method sort of list object at 0x0000019526CAEAC8&gt;&gt;&gt;&gt; getattr(list_1, 'sort')()&gt;&gt;&gt; list_1[1, 2, 3] 此外还有setattr——为对象添加变量或方法 、delattr——删除对象中的变量（不能用于删除方法）。 可调用Python数据类型文档列出了7种可调用对象。 用户定义的函数 使用def语句或lambda表达式创建。 内置函数 使用C语言（CPython）实现的函数，如len或time.strftime。 内置方法 使用C语言实现的方法，如dict.get。 方法 在类的定义体中定义的函数 类 调用类时会运行类的__new__方法创建一个实例，然后运行__init__方法，初始化实例，最后把实例返回给对方。 类的实例 如果类定义了__call__方法，那么它的实例可以作为函数调用 生成器函数 使用yield关键字的函数或方法。调用生成器函数返回的是生成器对象。 Python中有各种各样可调用类型，因此判断对象能否调用，最安全的方法是使用内置的callable()函数： 12345678910&gt;&gt;&gt; print(callable.__doc__)Return whether the object is callable (i.e., some kind of function).Note that classes are callable, as are instances of classes with a__call__() method.&gt;&gt;&gt; abs, str, 13(&lt;built-in function abs&gt;, &lt;class 'str'&gt;, 13)&gt;&gt;&gt; [callable(obj) for obj in (abs, str, 13)][True, True, False] 实例在 type() 函数提供对象的类型时，还可以使用 isinstance() 函数测试对象，以确定它是否是某个特定类型或定制类的实例： 1234567891011121314151617&gt;&gt;&gt; print(isinstance.__doc__)Return whether an object is an instance of a class or of a subclass thereof.A tuple, as in ``isinstance(x, (A, B, ...))``, may be given as the target tocheck against. This is equivalent to ``isinstance(x, A) or isinstance(x, B)or ...`` etc.&gt;&gt;&gt; isinstance(2, int)True&gt;&gt;&gt; isinstance(2, str)False&gt;&gt;&gt; isinstance([1, 2, 3], list)True&gt;&gt;&gt; isinstance(&#123;1, 2, 3&#125;, set)True&gt;&gt;&gt; isinstance(&#123;1, 2, 3&#125;, dict)False 子类关于类的问题，有一个“继承”概念，有继承就有父子问题，这是在现实生活中很正常的，在编程语言中也是如此。 在类这一级别，可以根据一个类来定义另一个类，同样地，这个新类会按照层次化的方式继承属性。Python 甚至支持多重继承，多重继承意味着可以用多个父类来定义一个类，这个新类继承了多个父类。 issubclass() 函数使我们可以查看一个类是不是继承了另一个类： 123456789101112131415&gt;&gt;&gt; print(issubclass.__doc__)Return whether 'cls' is a derived from another class or is the same class.A tuple, as in ``issubclass(x, (A, B, ...))``, may be given as the target tocheck against. This is equivalent to ``issubclass(x, A) or issubclass(x, B)or ...`` etc.&gt;&gt;&gt; from abc import ABC, abstractmethod&gt;&gt;&gt; class Test(ABC): ... @abstractmethod ... def test(self): ... """test""" ... &gt;&gt;&gt; issubclass(Test, ABC) True 函数内省 我们使用实例来讲述函数内省的具体作用。 这部分参考与《流畅的Python》第五章。 首先定义一个函数： 1234567891011121314def clip(text:str, max_len:'int &gt; 0'=80) -&gt; str: """在max_len前面或后面的第一个空格处截断文本""" end = None if len(text) &gt; max_len: space_before = text.rfind(' ', 0, max_len) if space_before &gt;= 0: end = space_before else: space_after = text.rfind(' ', max_len) if space_after &gt;= 0: end = space_after if end is None: end = len(text) return text[:end].rstrip() 使用dir()获取函数的属性使用dir函数可以探知clip具有下述属性： 1['__annotations__', '__call__', '__class__', '__closure__', '__code__', '__defaults__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__globals__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__kwdefaults__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__'] 其中大多数属性是Python对象共有的。我们重点说明函数专有而用户定义的一般对象没有的属性。 计算两个属性集合的差集便能得到函数专有的属性列表： 1234567&gt;&gt;&gt; class C: pass...&gt;&gt;&gt; obj = C()&gt;&gt;&gt; def func(): pass...&gt;&gt;&gt; sorted(set(dir(func)) - set(dir(obj)))['__annotations__', '__call__', '__closure__', '__code__', '__defaults__', '__get__', '__globals__', '__kwdefaults__', '__name__', '__qualname__'] 列出这些属性的简要说明： 名称 类型 说明 __annotations__ dict 参数和返回值的注解 __call__ method-wrapper 实现（）运算符，即可调用对象协议 __closure__ tuple 函数闭包，即自由变量的绑定（通常是None） __code__ code 编译成字节码的函数元数据和函数定义体 __defaults__ tuple 形式参数的默认值 __get__ method-wrapper 实现制度描述符协议 __globals__ dict 函数所在模块中的全局变量 __kwdefaults__ dict 仅限关键字形式参数的默认值 __name__ str 函数名称 __qualname__ str 函数的限定名称，如Random.choice 获取函数注解123&gt;&gt;&gt; from clip import clip&gt;&gt;&gt; clip.__annotations__&#123;'text': &lt;class 'str'&gt;, 'max_len': 'int &gt; 0', 'return': &lt;class 'str'&gt;&#125; 函数声明中的各个参数可以在：之后增加注解表达式。如果参数有默认值，注解放在参数名和=号之间。如果想注解返回值，在）和函数声明末尾的：之间添加-&gt;表达式。这个表达式可以是任何类型。注解中最常用的类型是类（如str或int）和字符串（如’int&gt;0’）。 获取函数文档字符串12&gt;&gt;&gt; clip.__doc__'在max_len前面或后面的第一个空格处截断文本' 获取函数参数信息12345678&gt;&gt;&gt; clip.__defaults__ (80,) &gt;&gt;&gt; clip.__code__ &lt;code object clip at 0x000001ABE1308780, file "C:\Users\Peng.Gao\Desktop\fluency_python\c5\clip.py", line 1&gt; &gt;&gt;&gt; clip.__code__.co_varnames ('text', 'max_len', 'end', 'space_before', 'space_after') &gt;&gt;&gt; clip.__code__.co_argcount 2 可以看出这种组织信息的方式并不是很便利的。 之后再介绍更好的方式inspect模块。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现统计文本当中单词数量]]></title>
    <url>%2F2018%2F06%2F30%2FPython%E5%AE%9E%E7%8E%B0%E7%BB%9F%E8%AE%A1%E6%96%87%E6%9C%AC%E5%BD%93%E4%B8%AD%E5%8D%95%E8%AF%8D%E6%95%B0%E9%87%8F%2F</url>
    <content type="text"><![CDATA[关于用实现统计文本当中单词数量这个功能，代码进行一步一步的升级。 我做个回顾，或许以后还能写出更符合标准的代码。 1 刚看完《python编程：从入门到实践》的时候写的代码学习python的时候在《python编程：从入门到实践》书中第10章中学习了分析文本，当时写出了统计一个单词出现的频率： 1234567891011121314151617181920# 10-10 常见单词def row_count(filename): try: with open(filename) as f_obj: content = f_obj.read() except FileNotFoundError: msg = "The file " + filename + " does not exist." print(msg) else: content = content.replace(',', ' ') content = content.replace('.', ' ') content = content.replace('-', ' ') content = content.strip().lower() words = content.split() # 统计row单词出现在文本中的次数 number = words.count('row') print('row : %d' % number)filename = 'Heart of Darkness.txt'row_count(filename) 运行结果为： 1row : 9 这个代码只是实现一个单词的出现次数的统计。 并且还有一些问题。比如还有[a、(b这样的标点符号出现在单词中。 2 写完一个单词的统计，又扩展了对所有单词都进行统计并排序当时写完课后作业时，想到了能不能对所有单词都进行统计并进行排序呢，于是上网查了一些资料，写出了下面的代码： 123456789101112131415161718192021222324252627282930313233343536373839from operator import itemgetterdef words_list(filename): try: with open(filename) as f_obj: content = f_obj.read() except FileNotFoundError: msg = "The file " + filename + " does not exist." print(msg) else: content = content.replace(',', ' ') content = content.replace('.', ' ') content = content.replace('!', ' ') content = content.replace('-', ' ') content = content.replace('_', ' ') content = content.replace('(', ' ') content = content.replace(')', ' ') content = content.strip() words = [word.lower() for word in content.split()] return wordsdef count_results(filename): words_count = &#123;&#125; words = words_list(filename) words_count = words_count.fromkeys(words) for word in words_count.keys(): number = words.count(word) words_count[word] = number words_count = sorted(words_count.items(), key=itemgetter(1), reverse=True) return words_countif __name__ == '__main__': filename = 'Heart of Darkness.txt' words_count = count_results(filename) for word, word_count in words_count[:10]: print('&#123;0:&lt;10&#125; : &#123;1&#125;'.format(word, word_count)) 运行结果为： 12345678910the : 2440of : 1492a : 1205and : 1045i : 1039to : 967was : 671in : 668he : 563had : 503 还是有问题，还是有标点符号没有彻底清除，类似这种[a]、_a_。 3 学完正则表达式以后使用正则表达式进行分割单词学会使用正则以后，试着用正则来清除标点符号： 1234567891011121314151617181920212223242526272829303132import refrom operator import itemgetterdef words_list(filename): try: with open(filename) as f_obj: content = f_obj.read() except FileNotFoundError: msg = "The file " + filename + " does not exist." print(msg) else: WORD_RE = re.compile(r'\W+') words = WORD_RE.split(content.lower()) return wordsdef count_results(filename): words_count = &#123;&#125; words = words_list(filename) words_count = words_count.fromkeys(words) for word in words_count.keys(): number = words.count(word) words_count[word] = number words_count = sorted(words_count.items(), key=itemgetter(1), reverse=True) return words_countif __name__ == '__main__': filename = 'Heart of Darkness.txt' words_count = count_results(filename) for word, word_count in words_count[:10]: print('&#123;0:&lt;10&#125; : &#123;1&#125;'.format(word, word_count)) 统计结果为： 12345678910the : 2468of : 1496a : 1209i : 1153and : 1062to : 974in : 673was : 672he : 596it : 515 还是有问题，\w+包含[A-Za-z0-9_]，所以包含’_‘的单词也会被统计出来，比如说‘_that_’也会被统计，并且区别于that。 首先想到就是先把下划线(_)替换为空格即可，不过也可以把_that_和that当做是2种单词。 4 还想到了一个不需要count的解法今天看另一本书《流畅的python》，在学习字典和集合的时候想到了另一种解法： 123456789101112131415161718192021222324252627def count_results(filename): try: with open(filename) as f_obj: content = f_obj.read() except FileNotFoundError: msg = "The file " + filename + " does not exist." print(msg) else: WORD_RE = re.compile(r'\w+') words_count = &#123;&#125; for match in WORD_RE.finditer(content.lower()): word = match.group() # occurrences = words_count.get(word, []) # occurrences.append(word) # words_count[word] = occurrences words_count.setdefault(word, []).append(word) words_count = &#123;word:len(value) for word, value in words_count.items()&#125; words_count = sorted(words_count.items(), key=itemgetter(1), reverse=True) return words_countif __name__ == '__main__': filename = 'Heart of Darkness.txt' words_count = count_results(filename) for word, word_count in words_count[:10]: print('&#123;0:&lt;10&#125; : &#123;1&#125;'.format(word, word_count)) 12345678910the : 2468of : 1496a : 1209i : 1153and : 1062to : 974in : 673was : 672he : 596it : 515 这个方法比使用count的方法速度要快上不少。 5 count方法和len方法速度比较12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import refrom operator import itemgetterdef words_list(filename): try: with open(filename) as f_obj: content = f_obj.read() except FileNotFoundError: msg = "The file " + filename + " does not exist." print(msg) else: WORD_RE = re.compile(r'\W+') words = WORD_RE.split(content.lower()) return wordsdef count_results(filename): words_count = &#123;&#125; words = words_list(filename) words_count = words_count.fromkeys(words) for word in words_count.keys(): number = words.count(word) words_count[word] = number words_count = sorted(words_count.items(), key=itemgetter(1), reverse=True) return words_countdef count_words(filename): try: with open(filename) as f_obj: content = f_obj.read() except FileNotFoundError: msg = "The file " + filename + " does not exist." print(msg) else: WORD_RE = re.compile(r'\w+') words_count = &#123;&#125; for match in WORD_RE.finditer(content.lower()): word = match.group() # occurrences = words_count.get(word, []) # occurrences.append(word) # words_count[word] = occurrences words_count.setdefault(word, []).append(word) words_count = &#123;word: len(value) for word, value in words_count.items()&#125; words_count = sorted(words_count.items(), key=itemgetter(1), reverse=True) return words_countif __name__ == '__main__': import timeit def test_count_results(): filename = 'Heart of Darkness.txt' words_count = count_results(filename) return words_count def test_count_words(): filename = 'Heart of Darkness.txt' words_count = count_words(filename) return words_count time_1 = timeit.Timer('test_count_results()', setup="from __main__ import test_count_results") time_2 = timeit.Timer('test_count_words()', setup="from __main__ import test_count_words") print(time_1.timeit(number=10)) print(time_2.timeit(number=10)) 输出结果为： 1254.6624033767616740.5711055088342789 结果发现差距还是挺大的。 说一个我认为的原因： count需要遍历列表，遍历列表是很消耗时间的，时间复杂度为O(n)； 而使用len()方法时，CPython会直接从内存来读取属性值，当然要快很多。 以下摘自《流畅的python》第一章 如何使用特殊方法 首先明确一点，特殊方法的存在是为了被Python解释器调用的，你自己并不需要调用它们。也就是说没有my_object.__len()__这种写法，而应该使用len(my_object)。在执行len(my_object)的时候，如果my_object是一个自定义类的对象，那么Python会自己去调用其中由你实现的__len__方法。 然而如果是Python内置的类型，比如列表（list）、字符串（str）、字节系列（bytearray）等，那么CPython会抄个近路，__len__实际上会直接返回PyVarObject里的ob_size属性。PyVarObject是表示内存中长度可变的内置对象的C语言结构体。直接读取这个值比调用一个方法要快很多。 目前知识掌握的不太多，还不能很肯定。 说不定以后还有其他的改进，可能未完待续。 6 使用计数器（Counter）方法来实现（目前为止个人觉得最佳解法）好吧，今天在学习《流畅的python》时，又发现了目前为止我认为最佳解决方式。 collections.Counter 这个映射类型会给键准备一个整体计数器。 每次更新一个键的时候会增加这个计数器。所以这个类型可以用来给可散列表对象计数，或者是当成多重集合来用——多重集合就是集合里的元素可以出现不止一次。 Counter实现了+和-运算符用来合并记录，还有像most_common([n])这类很有用的方法。most_common([n])会按照次序返回映射里最常见的n个键和他们的计数，详情参阅文档（https://docs.python.org/3/library/collections.html#collections.Counter）。 看一下使用Counter类来实现对单词进行统计的代码： 123456789101112131415161718192021222324252627282930313233import reimport collectionsdef words_list(filename): try: with open(filename) as f_obj: content = f_obj.read() except FileNotFoundError: msg = "The file " + filename + " does not exist." print(msg) else: WORD_RE = re.compile(r'\W+') words = WORD_RE.split(content.lower()) return words def words_counter(filename): try: with open(filename) as f_obj: content = f_obj.read() except FileNotFoundError: msg = "The file " + filename + " does not exist." print(msg) else: words = words_list(filename) words_count = collections.Counter(words) return words_countif __name__ != '__main__': filename = 'Heart of Darkness.txt' words_count = words_counter(filename) # 注意这个显示前10需要使用words_count.most_common(10)方法 for word, word_count in words_count.most_common(10): print('&#123;0:&lt;20&#125; : &#123;1&#125;'.format(word, word_count)) 结果为： 12345678910the : 2468of : 1496a : 1209i : 1153and : 1062to : 974in : 673was : 672he : 596it : 515 连排序（sorted）都省了，理论上应该更快。我们来测试一下速度。 1234567891011121314151617181920212223242526272829if __name__ == '__main__': import timeit def test_count_results(): filename = 'Heart of Darkness.txt' words_count = count_results(filename) return words_count def test_count_words(): filename = 'Heart of Darkness.txt' words_count = count_words(filename) return words_count def test_words_counter(): filename = 'Heart of Darkness.txt' words_count = words_counter(filename) return words_count time_1 = timeit.Timer('test_count_results()', setup="from __main__ import test_count_results") time_2 = timeit.Timer('test_count_words()', setup="from __main__ import test_count_words") time_3 = timeit.Timer('test_words_counter()', setup="from __main__ import test_words_counter") print(time_1.timeit(number=10)) print(time_2.timeit(number=10)) print(time_3.timeit(number=10)) 结果为： 12336.7605531286150150.379986186752539370.2645591842058579 果然速度更快了。 还能未完待续吗？]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-正则表达式与re模块]]></title>
    <url>%2F2018%2F06%2F28%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1 使用目的 工作中一直在使用Linux，也使用过正则表达式（grep、awk命令），都是现用现查，没有真正去了解它，用完以后就忘。 学习python之后发现使用python处理文本、日志等文件用正则表达式的地方比较多。每次都得浪费挺长时间去设计正则表达式，就是因为没有真正掌握它，以至于用起来没有得心应手的感觉。 我花费一下午加一晚上的时间阅读了一本《正则表达式必知必会》感觉正则表达式不复杂，但是能熟练、准确、高效的使用它并不是一件容易的时间。必须要多使用、多阅读才行。 目前只是属于入门，等以后如果需要学习爬虫的时候应该需要更深入的了解，那时候再去去阅读《精通正则表达式》。 虽说是入门，也想总结一下便以后查阅。 2 正则表达式基础 2.1 正则表达式简单介绍正则表达式并不是Python的一部分。正则表达式是用于处理字符串的强大工具，拥有自己独特的语法以及一个独立的处理引擎，效率上可能不如str自带的方法，但功能十分强大。 得益于这一点，在提供了正则表达式的语言里，正则表达式的语法都是一样的，区别只在于不同的编程语言实现支持的语法数量不同；但不用担心，不被支持的语法通常是不常用的部分。 正则表达式工具 最基础的功能，测试用还行 http://tool.oschina.net/regex/ （目前用过最好的web端，很适合学习的时候使用） http://regex.zjmainstay.cn/ （上个工具的原版） https://regex101.com/ RegexBuddy （客户端，貌似是最全面的，需要收费，没有使用过） 还有很多类似的工具，网上自己搜索就好。 2.2 正则表达式元字符统计先统计一下正则表达式常用的元字符，在举例详细说明。 元字符 说明 一般字符 匹配自身 . 匹配任何一个单个的字符（在绝大多数的正则表达式实现里，只能匹配除换行符以外的任何单个字符） […] 字符集（字符类）。可以是任意字符；可以使用-（连字符）利用集合区间[…]中第一个字符是^表示取反-（连字符）是一个特殊的元字符，作为元字符只能用在[和]之间，在字符集以外的地方-（连字符）只是一个普通字符，只能与-（连字符）本身相匹配，不需要被转义。所有的元字符在字符集中都失去原有的特殊含义，不需要被转义，但转义了也没有坏处。注意[和]也只能匹配一个字符 \ 转义，将特殊字符变为普通字符 元字符（特定的字符类别） 说明 \w 任何一个字母数字或下划线字符等价于[a-zA-Z0-9_] \W 任何一个非字母数字或非下划线字符等价于[^a-zA-Z0-9_] \d 任何一个数字字符等价于[0-9] \D 任何一个非数字字符等价于[^0-9] \s 任何一个空白字符等价于[\f\n\r\t\v ] （注意字符集中有个空格） \S 任何一个非空白字符等价于[^\f\n\r\t\v ] 元字符（数量词） 说明（用在普通字符、元字符、字符集或子表达式之后） *（贪婪型） 前一个字符或字符集合重复出现0次或多次（无限次） +（贪婪型） 前一个字符或字符集合连续出现1次或多次效果等价于{1,} ？（贪婪型） 前一个字符或字符集合出现0次或1次效果等价于{0,1} {}（贪婪型） 设定前一个字符或字符集合的重复次数{m,n}可以为重复匹配次数设定一个区间{m,}表示重复匹配前一个字符重复次数为m至无限次{,n}表示重复匹配前一个字符重复次数为0至n次 *？（非贪婪型） 匹配尽量少的字符，至少0次 +？（非贪婪型） 匹配尽量少的字符，至少一次 ??（非贪婪型） 匹配尽量少的字符，至少0次 {m,n}？（非贪婪型） 匹配尽量少的字符，至少m次 元字符（边界位置） 说明（边界匹配不消耗待匹配字符串中的字符） ^ 字符串开头在多行模式(?m)中匹配每一行的开头 $ 字符串末尾在多行模式(?m)中匹配每一行的末尾 \A 仅匹配字符串开头 \Z 仅匹配字符串末尾 \b 单词边界匹配这样一个位置：位于一个能够用来构成单词的字符（\w）和一个不能用来构成单词的字符（\W）之间 \B 非单词边界的位置[^\b] 元字符（逻辑分组） 说明 \ 代表左右表达式任意匹配一个总是先尝试匹配左边的表达式，一旦成功则跳过匹配右边的表达式 (…) 小括号里包裹指定子表达式（子串），这就是分组。 分组，分组从1开始，从表达式左边开始每遇到一个分组的左括号时编号+1分组表达式被视为一个整体，可以后接数量词。表达式若有丨表示仅在该组中有效 (?P\&lt;name>…) 命名分组可以给分组起一个唯一的名字，然后用这个名字（而不是相对位置）来引用 \&lt;number> 引用编号为\&lt;number>的分组匹配到的字符串 (?P=name) 引用命名为\&lt;name>的分组匹配到的字符串 元字符（特殊构造） 说明（不作为分组） (?:…) （…）不分组版本 (?=…) 向前查找匹配=后面字符的内容非捕获，不消耗字符串内容（通俗讲就是不包含匹配的字符） (?!…) 向前查找不匹配!后面字符的内容非捕获，不消耗字符串内容（通俗讲就是不包含匹配的字符） (?&lt;=…) 向后查找匹配=后面字符的内容非捕获，不消耗字符串内容（通俗讲就是不包含匹配的字符） (?&lt;!…) 向后查找不匹配!后面字符的内容非捕获，不消耗字符串内容（通俗讲就是不包含匹配的字符） (?(1)yes\丨no) 若成功匹配分组1，则匹配竖线前的子表达式，否则匹配后者。 还有一些递归环视等等元字符目前感觉用不上，通常用于很复杂的情况，暂时不统计。 2.3 正则表达式实例例1：使用数量词 RGB值：用一个十六进制数字给出的红、绿、蓝三基色的组合值。 测试文本（粗体字表示需要匹配出来的字符串）： &lt;BODY BGCOLOR=”#336633“ TEXT=”#FFFFFF“ MARGINWIDTH=”0” MARGINHEIGTH=”0” TOPMARGIN=”0” LEFTMARGIN=”0”&gt; 1#[0-9A-Fa-f]&#123;6&#125; Match 1 完整匹配 20-27 `#336633` Match 2 完整匹配 35-42 `#FFFFFF` 例2：使用分组以及条件判断 查找中国固定电话号码 我国的固定电话号码的规律是区号+电话号码： 最开始的位一定是0，表示长途，接着是2位、3位或者组成的区号。 然后是7位或者8位的电话号码，其中首位不为1。 国内习惯的电话格式有： 029 8845 7890 (029)88457890 (029) 88457890 029-8845-7890 029-88457890 020-8845 7890 测试文本（粗体字表示需要匹配出来的字符串）： 022-5731-3255(0701)40566323010-667788990376 622779104356647988(0376-22444567(0378 2234432 1(?m)^(\()?0[1-9]\d&#123;1,2&#125;(?(1)\)|[- ])[2-9]\d&#123;2,3&#125;[-]?\d&#123;4&#125; 解释一下这个正则表达式： (?m) 表示开启多行匹配 ^(\()?0 表示必须以(开头或者0开头，并把(分组，编号为1 (?(1)\)|[- ])表示若成功匹配到分组1(\()，则匹配)，若匹配补刀分组1，则匹配-或者空格。 Match 1 完整匹配 0-13 `022-5731-3255` Match 2 完整匹配 14-28 `(0701)40566323` 分组 1. 14-15 `(` Match 3 完整匹配 29-41 `010-66778899` Match 4 完整匹配 42-54 `0376 6227791` 例3：子表达式的嵌套用法 IPv4地址由4个字节构成，IP地址中的4组数字分别对应着那4个字节。所以IP地址里的每组数字的取值范围也就是单个字节的表示范围，即0~255。 正则表达式不懂计算，只是一种工具，没办法设定取值范围。 一组数字的取值范围需要我们自己来构造： 任何一个1位或2位数字 任何一个以1开头的3位数字 任何一个以2开头、第二位数字在0~4之间的3位数字 任何一个以25开头、第三位数字在0~5之间的3位数字 测试文本： 255.255.255.01.1.1.122.2.222.234192.168.99.101192.168.99.10111124.258.66.88192192.168.99.101 1\b(((\d&#123;1,2&#125;)|(1\d&#123;2&#125;)|(2[0-4]\d)|(25[0-5]))\.)&#123;3&#125;((\d&#123;1,2&#125;)|(1\d&#123;2&#125;)|(2[0-4]\d)|(25[0-5]))\b Match 1 完整匹配 0-13 `255.255.255.0` 分组 1. 8-12 `255.` 分组 2. 8-11 `255` 分组 6. 8-11 `255` 分组 7. 12-13 `0` 分组 8. 12-13 `0` Match 2 完整匹配 14-21 `1.1.1.1` 分组 1. 18-20 `1.` 分组 2. 18-19 `1` 分组 3. 18-19 `1` 分组 7. 20-21 `1` 分组 8. 20-21 `1` Match 3 完整匹配 22-34 `22.2.222.234` 分组 1. 27-31 `222.` 分组 2. 27-30 `222` 分组 3. 25-26 `2` 分组 5. 27-30 `222` 分组 7. 31-34 `234` 分组 10. 31-34 `234` Match 4 完整匹配 35-49 `192.168.99.101` 分组 1. 43-46 `99.` 分组 2. 43-45 `99` 分组 3. 43-45 `99` 分组 4. 39-42 `168` 分组 7. 46-49 `101` 分组 9. 46-49 `101` 所以我们可以把（…）当做子表达式来使用（比如限制|或者跟数量词等），也可以当做分组来使用（比如回溯引用）。 例4：获取匹配与非获取匹配 在正则表达式匹配的过程中，其实存在“消耗字符”的过程，也就是说，一旦一个字符在匹配过程中被检索（消耗）过，后面的匹配就不会再检索这一字符了。 匹配使用的协议 测试文本：1 https://www.baidu.comhttp://www.cnblogs.comftp://ftp.forta.com 我们先看一个普通的（获取匹配，消耗字符串内容） 1.+(:) 这里：分不分组都一样，这里只是清晰的表达获取匹配和非获取匹配 Match 1 完整匹配 0-6 `https:` Match 2 完整匹配 22-27 `http:` Match 3 完整匹配 45-49 `ftp:` 可以看出：也被匹配进去了，我们如果不想匹配：的话就可以使用非获取匹配，不消耗匹配的字符。 1.+(?=:) Match 1 完整匹配 0-5 `https` Match 2 完整匹配 22-26 `http` Match 3 完整匹配 45-48 `ftp` 可以发现匹配的：并没有出现在我们的最终匹配结果中。 同理，(?!…)(?&lt;=…)(?&lt;!…)也是一样的道理。 例5：贪婪型元字符和非贪婪型元字符 贪婪型总是尝试匹配尽可能多的字符；非贪婪型型则相反，总是尝试匹配尽可能少的字符。 贪婪性元字符 非贪婪型元字符 * *? + +? {m,n} {m,n}? 我们看一个例子： 测试文本： &lt;div&gt;test1&lt;/div&gt;bb&lt;div&gt;test2&lt;/div&gt; 1&lt;div&gt;.*&lt;\/div&gt; Match 1 完整匹配 0-34 &lt;div&gt;test1&lt;/div&gt;bb&lt;div&gt;test2&lt;/div&gt; 1&lt;div&gt;.*?&lt;\/div&gt; Match 1 完整匹配 0-16 &lt;div&gt;test1&lt;/div&gt; Match 2 完整匹配 18-34 &lt;div&gt;test2&lt;/div&gt; 如果只匹配一次的话，使用贪婪模式就尽可能多的匹配符合表达式的结果；而使用非贪婪模式匹配符合表达式就立即停止这次匹配，开始下次匹配。 3 Python re模块使用 re 模块使 Python 语言拥有全部的正则表达式功能。 compile 函数根据一个模式字符串和可选的标志参数生成一个正则表达式对象。该对象拥有一系列方法用于正则表达式匹配和替换。 re 模块也提供了与这些方法功能完全一致的函数，这些函数使用一个模式字符串做为它们的第一个参数。 3.1 正则表达式修饰符 - 可选标志正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。多个标志可以通过按位 OR(|) 它们来指定。如 re.I | re.M 被设置成 I 和 M 标志： 修饰符 描述 re.I（IGNORECASE） 使匹配对大小写不敏感 re.L（LOCALE） 做本地化识别（locale-aware）匹配使预定字符类 \w \W \b \B \s \S 取决于当前区域设定 re.M（ MULTILINE ） 多行匹配，改变 ^ 和 $的行为 re.S（DOTALL） 使 . 匹配包括换行在内的所有字符 re.U（ UNICODE ） 根据Unicode字符集解析字符。使预定字符类 \w \W \b \B \s \S \d \D 取决于unicode定义的字符属性 re.X（ VERBOSE ） 详细模式，该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。 3.2 反斜杠的困扰与大多数编程语言相同，正则表达式里使用\作为转义字符，这就可能造成反斜杠困扰。 假如你需要匹配文本中的字符\，那么使用编程语言表示的正则表达式里将需要4个反斜杠”\\\\“：前两个和后两个分别用于在编程语言里转义成反斜杠，转换成两个反斜杠后再在正则表达式里转义成一个反斜杠。 Python里的原生字符串很好地解决了这个问题，这个例子中的正则表达式可以使用r’\‘表示。同样，匹配一个数字的’\d’可以写成r’\d’。有了原生字符串，你再也不用担心是不是漏写了反斜杠，写出来的表达式也更直观。 3.3 compile编译正则表达式，返回一个正则表达式（ Pattern ） 对象。（可以把那些常用的正则表达式编译成正则表达式对象，这样可以提高效率。） 语法： compile(pattern,flags=0) pattern - 编译时用的表达式字符串 flags - 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。 123456&gt;&gt;&gt; import re&gt;&gt;&gt; data = "Tina is a good girl, she is cool, clever, and so on..."&gt;&gt;&gt; pattern = re.compile(r'\w*oo\w*', re.I)# 查找所有包含oo的单词，findall方法下面会讲到&gt;&gt;&gt; print(pattern.findall(data))['good', 'cool'] 3.4 match从字符串的起始位置匹配pattern，如果pattern结束时仍可匹配则返回一个macth object对象；如果起始位置匹配不成功的话，match()就返回none。 语法： match(pattern, string, flags=0) pattern - 匹配的正则表达式 string - 要匹配的字符串 flags - 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。 group(num=0) - 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组。 groups() - 返回一个包含所有小组字符串的元组，从 1 到 所含的小组号。 实例： 1234567891011121314151617181920# 起始位置匹配成功，返回一个match object对象&gt;&gt;&gt; print(re.match('www','www.runoob.com')) &lt;_sre.SRE_Match object; span=(0, 3), match='www'&gt; # match object对象使用span函数来返回一个元组包含匹配的位置&gt;&gt;&gt; print(re.match('www','www.runoob.com').span())(0, 3)# match object对象使用group()函数来返回匹配的字符串,下面会详细讲到&gt;&gt;&gt; print(re.match('www','www.runoob.com').group())www# match object对象使用groups()函数来返回以元组形式分组捕获的字符串&gt;&gt;&gt; print(re.match('(www)','www.runoob.com').groups())('www',)# 起始位置匹配不成功，返回None&gt;&gt;&gt; print(re.match('com','www.runoob.com')) None# 由于返回的不是match object对象，所以不能使用match object对象所包含的方法&gt;&gt;&gt; print(re.match('com','www.runoob.com').span())Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; 3.5 fullmatch从字符串的起始位置匹配pattern，如果完全匹配则返回一个macth object对象；如果从起始位置至结束位置不完全匹配的话，fullmatch()就返回none。 语法： fullmatch(pattern, string, flags=0) pattern - 匹配的正则表达式 string - 要匹配的字符串 flags - 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。 group(num=0) - 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组。 groups() - 返回一个包含所有小组字符串的元组，从 1 到 所含的小组号。 match和fullmatch的区别 match只考虑起始位置匹配成功即可；而fullmatch则是完全匹配即起始位置和结束位置都匹配成功 实例 1234&gt;&gt;&gt; print(re.fullmatch('www','www.baidu.com'))None&gt;&gt;&gt; print(re.fullmatch('www','www'))&lt;_sre.SRE_Match object; span=(0, 3), match='www'&gt; 3.6 search扫描整个字符串，只返回第一个成功匹配的macth object对象，如果字符串没有匹配则返回None。 语法： search(pattern, string, flags=0) pattern - 匹配的正则表达式 string - 要匹配的字符串 flags - 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。 group(num=0) 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组。 groups() 返回一个包含所有小组字符串的元组，从 1 到 所含的小组号。 match与search的区别 match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None； 而search匹配整个字符串，直到找到一个匹配。 实例： 1234567891011121314151617&gt;&gt;&gt; line = "Cats are smarter than dogs"# 匹配成功，返回一个match object对象&gt;&gt;&gt; print(re.search(r'(.*)[ ]are[ ](\b\w+?\b)', line, re.M|re.I))&lt;_sre.SRE_Match object; span=(0, 16), match='Cats are smarter'&gt;# 以元组形式返回全部分组截获的字符串。相当于调用group(1,2,…last)&gt;&gt;&gt; print(re.search(r'(.*)[ ]are[ ](\b\w+?\b)', line, re.M|re.I).groups())('Cats', 'smarter')# 我们编写正则表达式定义了组(...)，这时就可以添加编号或者组名（如果使用了命名分组）来返回一个或多个分组捕获的字符串# 编号0代表返回整个匹配的字符串，不填写参数时（group()）返回group(0)&gt;&gt;&gt; print(re.search(r'(.*)[ ]are[ ](\b\w+?\b)', line, re.M|re.I).group())Cats are smarter&gt;&gt;&gt; print(re.search(r'(.*)[ ]are[ ](\b\w+?\b)', line, re.M|re.I).group(1))Cats&gt;&gt;&gt; print(re.search(r'(.*)[ ]are[ ](\b\w+?\b)', line, re.M|re.I).group(2))smarter&gt;&gt;&gt; print(re.search(r'(.*)[ ]are[ ](\b\w+?\b)', line, re.M|re.I).group(1, 2))('Cats', 'smarter') 3.7 match object对象match和search一旦匹配成功，则返回一个match object对象。 match object对象时一次匹配的结果，包含了很多关于此次匹配的信息。可以使用match提供的可读属性或方法来读取这些信息。 属性： string - 匹配时使用的文本 re - 匹配时使用的pattern对象 pos - 文本中正则表达式开始搜索的索引 endpos - 文本中正则表达式结束搜索的索引 lastindex - 最后一个被捕获的分组的编号，如果没有被捕获的分组则返回None lastgroup - 最后一个被捕获的分组的别名，如果这个分组没有别名或者是没有被捕获的分组则返回None 方法： group(1, 2, …) 获得一个或多个分组捕获的字符串，指定多个参数时将以元组形式返回 参数可以使分组编号也可以是别名 编号0代表整个匹配的字符串，不填写参数时，返回group(0) 没有捕获字符串的组返回None，捕获了多次的组返回最后一次捕获的字符串 groups() 以元组形式返回全部分组捕获的字符串。 相当于group(1, 2, 3, …, last) groupdict() 返回以有别名的组的别名为键，以该组捕获的字符串为值的字典 没有别名的组不包含在内 start(n) 返回指定的组捕获的字符串在string中的起始索引 该索引为匹配的字符串的第一个字符的索引 没有参数时，默认为start(0) end(n) 返回指定的组捕获的字符串在string中的结束索引 该索引为匹配的字符串的最后一个字符的索引+1 没有参数时，默认为end(0) span(n) 以元组形式返回匹配的位置 相当于返回（start(n), end(n)） expand(template) 将匹配到的分组代入template中然后返回 template中可以使用\id或\g&lt; id&gt; \g&lt;name&gt;引用分组，但不能使用编号0 \id与\g&lt;id&gt;是等价的；但\10将被认为是第10个分组，如果你想表达\1之后是字符’0’，只能使用\g&lt;1&gt;0 实例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849&gt;&gt;&gt; line = "Cats are smarter than dogs"&gt;&gt;&gt; match_object = re.search(r'(.*)[ ]are[ ](?P&lt;sign&gt;\b\w+?\b)', line, re.M|re.I)# 返回匹配时使用的文本&gt;&gt;&gt; match_object.string'Cats are smarter than dogs'# 返回匹配时使用的pattern对象&gt;&gt;&gt; match_object.rere.compile('(.*)[ ]are[ ](?P&lt;sign&gt;\\b\\w+?\\b)', re.IGNORECASE|re.MULTILINE)# 文本中正则表达式开始搜索的索引&gt;&gt;&gt; match_object.pos0# 文本中正则表达式结束搜索的索引&gt;&gt;&gt; match_object.endpos26# 最后一个分组的编号&gt;&gt;&gt; match_object.lastindex2# 最后一个被捕获的分组的别名&gt;&gt;&gt; match_object.lastgroup'sign'# 以元组形式返回多个分组捕获的字符串&gt;&gt;&gt; match_object.group(1, 2)('Cats', 'smarter')# 返回分组1捕获的字符串&gt;&gt;&gt; match_object.group(1)'Cats'# 返回整个匹配的字符串&gt;&gt;&gt; match_object.group()'Cats are smarter'# 以元组形式返回全部分组捕获的字符串&gt;&gt;&gt; match_object.groups()('Cats', 'smarter')# 返回以有别名的组的别名为键，以该组捕获的字符串为值的字典&gt;&gt;&gt; match_object.groupdict()&#123;'sign': 'smarter'&#125;# 返回分组2捕获的字符串在string中的起始索引&gt;&gt;&gt; match_object.start(2)9# 返回分组2捕获的字符串在string中的结束索引&gt;&gt;&gt; match_object.end(2)16# 以元组形式返回分组2匹配的位置&gt;&gt;&gt; match_object.span(2)(9, 16)# 将分组1、分组&lt;sign&gt;带入r'\g&lt;sign&gt; \1'中返回。（就是把分组1和分组2对调）&gt;&gt;&gt; match_object.expand(r'\g&lt;sign&gt; \1')'smarter Cats' 3.8 findall在字符串中找到正则表达式所匹配的所有子串，并返回一个列表，如果没有找到匹配的，则返回空列表。 语法： findall(pattern, string, flags=0) pattern - 匹配的正则表达式 string - 要匹配的字符串 flags - 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 findall与match、search的区别 match 和 search 是匹配一次 ，findall 匹配所有 实例： 12345# 编译一个非贪婪匹配数字的正则表达式&gt;&gt;&gt; p = re.compile(r'\d+?')# 使用findall()函数将所有匹配的字符串返回到一个列表中&gt;&gt;&gt; print(p.findall('1fwer334nk432j5'))['1', '3', '3', '4', '4', '3', '2', '5'] 3.9 finditer在字符串中找到正则表达式所匹配的所有子串，并把它们作为一个迭代器返回。 语法： finditer(pattern, string, flags=0) pattern - 匹配的正则表达式 string - 要匹配的字符串 flags - 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 实例： 12345678910&gt;&gt;&gt; iter = re.finditer('\d+', '1fwer334nk432j5') &gt;&gt;&gt; iter &lt;callable_iterator object at 0x00000124117EBE48&gt; &gt;&gt;&gt; for i in iter: ... print(i.group()) ... 1 334 432 5 3.10 sub使用re替换string中每一个匹配的子串后返回替换后的字符串。 语法： sub(pattern, repl, string, count=0, flags=0) pattern - 匹配的正则表达式 repl - 替换的字符串，也可以是一个函数。当repl是一个字符串时，可以使用\id、\g&lt;id&gt;或\g&lt;name&gt;引用分组；当repl是一个函数时，这个函数只接受一个参数（macth object对象），并返回一个字符串用于替换。 string - 要匹配的字符串 count - 指定最多替换次数，默认全部替换 flags - 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 实例： 1234567891011121314151617181920212223242526272829# 当repl是一个字符串时，可以使用\g&lt;id&gt;引用分组&gt;&gt;&gt; phone = '03766227791'&gt;&gt;&gt; print(re.sub(r'(^\d&#123;4&#125;)', r'\g&lt;1&gt;-', phone))0376-6227791&gt;&gt;&gt; print(re.sub(r'(^\d&#123;4&#125;)', r'(\1)', phone))(0376)6227791# 这个例子虽然把dogs和cats互换了位置，但是还有一个问题，就是大小写的问题，我们用函数来处理它&gt;&gt;&gt; line = "Cats are smarter than dogs"&gt;&gt;&gt; p = re.compile(r'(\b\w+\b)(.*)(\b\w+\b)')&gt;&gt;&gt; p.sub(r'\3\2\1', line)'dogs are smarter than Cats'# 当repl是一个函数时，这个函数只接受一个match object对象作为参数，并返回一个字符串用于替换&gt;&gt;&gt; line = "Cats are smarter than dogs"&gt;&gt;&gt; p = re.compile(r'(\b\w+\b)(.*)(\b\w+\b)')&gt;&gt;&gt; def func(m):... return m.group(3).title() + m.group(2) + m.group(1).lower()...&gt;&gt;&gt; p.sub(func, line)'Dogs are smarter than cats'# 也可以用匿名函数&gt;&gt;&gt; p.sub(lambda m:m.group(3).title()+m.group(2)+m.group(1).lower(), line)'Dogs are smarter than cats'&gt;&gt;&gt; s = 'A23G4HFD567'&gt;&gt;&gt; re.sub(r'(?P&lt;value&gt;\d+?)', lambda m:str(int(m.group('value'))*2), s, 3)'A46G8HFD567' 3.11 subn 返回 (sub(repl, string[, count]), 替换次数） 语法： subn(pattern, repl, string, count=0, flags=0) pattern - 匹配的正则表达式 repl - 替换的字符串，也可以是一个函数。当repl是一个字符串时，可以使用\id、\g&lt;id&gt;或\g&lt;name&gt;引用分组；当repl是一个函数时，这个函数只接受一个参数（macth object对象），并返回一个字符串用于替换。 string - 要匹配的字符串 count - 指定最多替换次数，默认全部替换 flags - 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 实例： 123&gt;&gt;&gt; s = 'A23G4HFD567'&gt;&gt;&gt; re.subn(r'(?P&lt;value&gt;\d+?)', lambda m:str(int(m.group('value'))*2), s)('A46G8HFD101214', 6) 3.12 split按照能够匹配的子串将string分割后返回列表。 语法： split(pattern, string, maxsplit=0, flags=0) pattern - 匹配的正则表达式 string - 要匹配的字符串 maxsplit - 指定最大分割次数，默认全部分割 flags - 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等 实例： 12&gt;&gt;&gt; re.split(r'\d', 'one1two2three3four4five5')['one', 'two', 'three', 'four', 'five', ''] 3.13 purge清空缓存中的正则表达式 语法 purge() 实例： 1&gt;&gt;&gt; re.purge()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python模块</tag>
        <tag>regex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[抓包工具-Fiddler的使用（一）]]></title>
    <url>%2F2018%2F06%2F25%2F%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7-Fiddler%E7%9A%84%E4%BD%BF%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1. Fiddler简介 Fiddler是位于客户端和服务器端的HTTP代理。 它能够记录客户端和服务器之间的所有 HTTP请求，可以针对特定的HTTP请求，分析请求数据、设置断点、调试web应用、修改请求的数据，甚至可以修改服务器返回的数据，功能非常强大，是web调试的利器。 也就是说：客户端的所有请求都要先经过Fiddler，然后转发到相应的服务器，反之，服务器端的所有响应，也都会先经过Fiddler然后发送到客户端，基于这个原因，Fiddler支持所有可以设置http代理为127.0.0.1:8888的浏览器和应用程序。 流程如下所示： 2. Fiddler代理配置 2.1. Windows环境浏览器配置2.1.1. IEFiddler作为系统代理，当启动Fidder时，IE的proxy设定会变成127.0.0.1:8888。 修改端口： 如果8888端口已经被占用，可在fiddler中修改端口。 菜单栏 -&gt; Tools -&gt; Options -&gt; Connections -&gt; Fiddler listens on port: [9999] 如果你的浏览器在开启fiddler之后没有设置相应的代理，则fiddler是无法捕获到HTTP请求的。 当你的系统代理发生变化，Fiddler会有如下警告： 此时若想重新设置fiddler代理，只需点击一下黄色区域即可。 2.1.2. ChromeChrome如果没有使用代理插件，则会自动使用ie的代理配置，只需配置IE的代理配置即可。 在chrome配置中打开代理设置会自动弹出ie的代理配置。 Chrom如果使用了代理插件，就需要配置插件或者直接使用系统代理。 有三种方法： 在插件中选择使用系统代理 把代理插件关闭直接使用系统代理 自己在插件中配置代理： 2.1.3. FirefoxFirefox内置的有代理插件，同样有2种方法来配置： 直接使用系统代理 配置代理 2.1.4. 远程主机抓包还有一种情况，就是Fiddler想抓另一台电脑的包，就需要在另一台电脑上配置代理了。 开启远程链接功能：Fiddler菜单栏 -&gt; Tools -&gt; Options -&gt; connections -&gt; 勾选Allow remote computers to connect 在另一台电脑上配置host(Fiddler所在主机):port(Fiddler配置的端口)代理即可。 2.2. iphone抓包设置2.2.1. 配置Fiddler 开启远程链接功能：Fiddler菜单栏 -&gt; Tools -&gt; Options -&gt; connections -&gt; 勾选Allow remote computers to connect 开启捕获https流量：Fiddler菜单栏 -&gt; Tools -&gt; Options -&gt; HTTPS -&gt; 勾选 Capture HTTPSCONNECTs -&gt; 勾选 Decrypt HTTPS traffic -&gt; 选择from remote clients only -&gt; 勾选Ignore server certificate errors(unsafe) 获取Fiddler所在ip地址（Fiddler所在主机和手机链接同一路由）： 2.2.2. Iphone上配置Fiddler为代理设置 -&gt; 无线局域网 -&gt; 配置无线局域网 -&gt; 配置代理 -&gt; 勾选手动 -&gt; 配置服务器（即Fiddler所在主机）和端口号（Fiddler配置的端口号） 此时，Fiddler可以捕获iphone手机上的http协议的访问。 但是，iphone手机无法访问https协议的请求，接下来，我们需要配置描述文件并信任它。 2.2.3. Iphone上安装Fiddler描述文件 打开Safari浏览器，访问代理服务器ip:port。比如http://192.168.99.206:8888。 点击FiddlerRoot Certificate安装描述文件。 删除描述文件： 若想删除描述文件，设置 -&gt; 通用 -&gt; 描述文件 -&gt; 选中要删除的描述文件 -&gt; 移除描述文件 信任证书：设置 -&gt; 通用 -&gt; 关于本机 -&gt; 证书信任设置 -&gt; 开启对Fiddler的证书信任 此时就可以用Fiddler抓取iphone手机上Safari浏览器和APP的http/https协议的数据包了。 关于IOS版本 ios10.3之前版本的Iphone不需要信任证书。 ios10.3之后版本的Iphone需要信任证书。 2.3 Android抓包配置和iphone类似，只是少了一步安装描述文件。 2.3.1. 配置Fiddler参照iphone配置Fiddler 2.3.2 Android代理配置没有安卓机，下了一个模拟器试验，可能步骤有不同。 设置-&gt; WLAN -&gt; 点击已连接的网络 -&gt; 修改网络 -&gt; 勾选显示高级选项 -&gt; 选择代理为手动 -&gt; 手机的IP地址和Fiddler代理的主机ip地址必须在同一网段，模拟器使用桥接模式还需要下载插件，我懒得弄了，上面的配置手机是访问不了Fiddler主机ip的，故无法抓包，只是为了举例用。 此时，就可以用Fiddler抓取Android手机上浏览器和APP的http/https协议的数据包了。 3. Fiddler 界面简介Fiddler主界面的布局如下： 主界面中主要包括四个常用的块： Fiddler的菜单栏，上图绿色部分。 包括捕获http请求，停止捕获请求，保存http请求，载入本地session、设置捕获规则等功能。 Fiddler的工具栏,上图橙色部分。 包括Fiddler针对当前view的操作（暂停，清除session,decode模式、清除缓存等）。 web Session面板，上图红色区域。 主要是Fiddler抓取到的每条http请求（每一条称为一个session）,主要包含了请求的url，协议，状态码，body等信息，详细的字段含义如下图所示： 字段 信息 # HTTP Request的顺序，从1开始，按照页面加载请求的顺序递增 Result HTTP响应的状态码 Protocol 请求使用的协议，如HTTP/HTTPS/FTP Host 请求地址的域名 URL 请求的服务器路径和文件名，也包括GET参数 BODY 请求的大小，以byte为单位 Caching 请求的缓存过期时间或缓存控制header等值 Process 发出此请求的Winodws进程及进程ID（远程设备获取不到） Comments 用户通过脚本或右键菜单给此session增加的备注 Custom 用户可以通过脚本设置的自定义值 详情和数据统计面板，上图黑色区域。 针对每条http请求的具体统计（例如发送/接受字节数，发送/接收时间，还有粗略统计世界各地访问该服务器所花费的时间）和数据包分析。 下一篇在详细介绍Fiddler的常用功能。]]></content>
      <categories>
        <category>网络</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>抓包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-sorted模块]]></title>
    <url>%2F2018%2F06%2F25%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-sort%E4%B8%8Esorted%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[sorted(iterable，key=None,reverse=False) keykey指定一个接收一个参数的函数，这个函数用于从每个元素中提取一个用于比较的关键字。默认值为None 。 reverse是一个布尔值。如果设置为True，列表元素将被倒序排列，默认为False。 python3 sorted取消了对cmp的支持。 例1：按照学生的年龄排序123&gt;&gt;&gt; students = [('john', 'A', 15), ('jane', 'B', 12), ('dave','B', 10)]&gt;&gt;&gt; sorted(students,key=lambda x: x[2])[('dave', 'B', 10), ('jane', 'B', 12), ('john', 'A', 15)] 例2：正数在前负数在后；正数从小到大；负数从大到小123&gt;&gt;&gt; list_1 = [7, -8, 5, 4, 0, -2, -5]&gt;&gt;&gt; sorted(list_1, key=lambda x:(x&lt;0,abs(x)))[0, 4, 5, 7, -2, -5, -8] 例3：字符串长度排序123&gt;&gt;&gt; list_2 = ['dddd','a','bb','ccc']&gt;&gt;&gt; sorted(list_2, key=len)['a', 'bb', 'ccc', 'dddd'] 例3：小写 &lt; 大写 &lt; 奇数 &lt; 偶数123&gt;&gt;&gt; s = 'A1ab5C3c4D798'&gt;&gt;&gt; "".join(sorted(s, key=lambda x: (x.isdigit(),x.isdigit() and int(x) % 2 == 0,x.isupper(),x)))'abcACD1357948' 讲一下原理： Boolean 的排序会将 False 排在前，True排在后 。 先比较元组的第一个值，FALSE&lt;TRUE，如果相等就比较元组的下一个值，以此类推。 第一步比较：x.isdigit()的作用是把数字放在前边,字母放在后边。 第二步比较：x.isdigit() and int(x) % 2 == 0的作用是保证奇数在前，偶数在后。 第三步比较：x.isupper()的作用是在前面基础上,保证字母小写在前大写在后。 第四步比较：最后的x表示在前面基础上,对所有类别数字或字母排序 。 例4： 根据字符串中的内嵌数字排序1234l = [ 'ch9.txt', 'ch10.txt', 'ch1.txt', 'ch3.txt', 'ch11.txt' ]&gt;&gt;&gt; import re&gt;&gt;&gt; sorted(l, key = lambda x:int(re.match('\D+(\d+)\.txt',x).group(1)))['ch1.txt', 'ch3.txt', 'ch9.txt', 'ch10.txt', 'ch11.txt'] 例5：list里都是正整数，组合出来最大的数字这个在Python2里面用Sorted加cmp就很容易实现，但是在Python3中由于取消了内置对象__cmp__方法，所以sorted函数的传入比较函数的cmp参数也取消了。 我们可以使用cmp_to_key 来解决cmp的问题： 12345&gt;&gt;&gt; from functools import cmp_to_key&gt;&gt;&gt; list_3 = [3, 30, 34, 5, 9]&gt;&gt;&gt; cmp2key = cmp_to_key(lambda x,y: int(y+x)-int(x+y))&gt;&gt;&gt; print( ''.join(sorted(map(str, list_3), key=cmp2key)))9534330 例6：提取字典的某个key值并进行排序12from operator import itemgettersubmission_dicts = sorted(submission_dicts, key=itemgetter(1), reverse=True) 123456789101112131415161718192021222324&gt;&gt;&gt; help(itemgetter)class itemgetter(builtins.object) | itemgetter(item, ...) --&gt; itemgetter object | | Return a callable object that fetches the given item(s) from its operand. | After f = itemgetter(2), the call f(r) returns r[2]. | After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3]) | | Methods defined here: | | __call__(self, /, *args, **kwargs) | Call self as a function. | | __getattribute__(self, name, /) | Return getattr(self, name). | | __new__(*args, **kwargs) from builtins.type | Create and return a new object. See help(type) for accurate signature. | | __reduce__(...) | Return state information for pickling | | __repr__(self, /) | Return repr(self). sort用法与sorted相同。 sort对列表进行永久性排序。 sorted对列表进行临时排序，不改变原列表序列。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux学习笔记-进程后台运行]]></title>
    <url>%2F2018%2F06%2F21%2FLinux%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E8%BF%9B%E7%A8%8B%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[让进程在后台可靠运行的几种方法我们经常会碰到这样的问题，用 telnet/ssh 登录了远程的 Linux 服务器，运行了一些耗时较长的任务， 结果却由于网络的不稳定导致任务中途失败。如何让命令提交后不受本地关闭终端窗口/网络断开连接的干扰呢？下面举了一些例子， 您可以针对不同的场景选择不同的方式来处理这个问题。 nohup/setsid/&amp;场景：如果只是临时有一个命令需要长时间运行，什么方法能最简便的保证它在后台稳定运行呢？ 解决方法：我们知道，当用户注销（logout）或者网络断开时，终端会收到 HUP（hangup）信号从而关闭其所有子进程。因此，我们的解决办法就有两种途径：要么让进程忽略 HUP 信号，要么让进程运行在新的会话里从而成为不属于此终端的子进程。 hangup 名称的来由： ​ 在 Unix 的早期版本中，每个终端都会通过 modem 和系统通讯。当用户 logout 时，modem 就会挂断（hang up）电话。 同理，当 modem 断开连接时，就会给终端发送 hangup 信号来通知其关闭所有子进程。 1. nohup nohup 无疑是我们首先想到的办法。顾名思义，nohup 的用途就是让提交的命令忽略 hangup 信号。让我们先来看一下 nohup 的帮助信息： 12345678910111213141516NOHUP(1) User Commands NOHUP(1) NAME nohup - run a command immune to hangups, with output to a non-tty SYNOPSIS nohup COMMAND [ARG]... nohup OPTION DESCRIPTION Run COMMAND, ignoring hangup signals. --help display this help and exit --version output version information and exit 可见，nohup 的使用是十分方便的，只需在要处理的命令前加上 nohup 即可，标准输出和标准错误缺省会被重定向到 nohup.out 文件中。一般我们可在结尾加上“&amp;”来将命令同时放入后台运行，也可用&quot;&gt; filename 2&gt;&amp;1&quot;来更改缺省的重定向文件名。 nohup 示例1234567[root@pvcent107 ~]# nohup ping www.ibm.com &amp;[1] 3059nohup: appending output to `nohup.out'[root@pvcent107 ~]# ps -ef |grep 3059root 3059 984 0 21:06 pts/3 00:00:00 ping www.ibm.comroot 3067 984 0 21:06 pts/3 00:00:00 grep 3059[root@pvcent107 ~]# 2. setsid nohup 无疑能通过忽略 HUP 信号来使我们的进程避免中途被中断，但如果我们换个角度思考，如果我们的进程不属于接受 HUP 信号的终端的子进程，那么自然也就不会受到 HUP 信号的影响了。setsid 就能帮助我们做到这一点。让我们先来看一下 setsid 的帮助信息： 12345678910SETSID(8) Linux Programmer’s Manual SETSID(8) NAME setsid - run a program in a new session SYNOPSIS setsid program [ arg ... ] DESCRIPTION setsid runs a program in a new session. 可见 setsid 的使用也是非常方便的，也只需在要处理的命令前加上 setsid 即可。 setsid 示例12345[root@pvcent107 ~]# setsid ping www.ibm.com[root@pvcent107 ~]# ps -ef |grep www.ibm.comroot 31094 1 0 07:28 ? 00:00:00 ping www.ibm.comroot 31102 29217 0 07:29 pts/4 00:00:00 grep www.ibm.com[root@pvcent107 ~]# 值得注意的是，上例中我们的进程 ID(PID)为31094，而它的父 ID（PPID）为1（即为 init 进程 ID），并不是当前终端的进程 ID。请将此例与nohup 例中的父 ID 做比较。 3. &amp; 这里还有一个关于 subshell 的小技巧。我们知道，将一个或多个命名包含在“()”中就能让这些命令在子 shell 中运行中，从而扩展出很多有趣的功能，我们现在要讨论的就是其中之一。 当我们将”&amp;”也放入“()”内之后，我们就会发现所提交的作业并不在作业列表中，也就是说，是无法通过jobs来查看的。让我们来看看为什么这样就能躲过 HUP 信号的影响吧。 subshell 示例12345root@pvcent107 ~]# (ping www.ibm.com &amp;)[root@pvcent107 ~]# ps -ef |grep www.ibm.comroot 16270 1 0 14:13 pts/4 00:00:00 ping www.ibm.comroot 16278 15362 0 14:13 pts/4 00:00:00 grep www.ibm.com[root@pvcent107 ~]# 从上例中可以看出，新提交的进程的父 ID（PPID）为1（init 进程的 PID），并不是当前终端的进程 ID。因此并不属于当前终端的子进程，从而也就不会受到当前终端的 HUP 信号的影响了。 disown场景：我们已经知道，如果事先在命令前加上 nohup 或者 setsid 就可以避免 HUP 信号的影响。但是如果我们未加任何处理就已经提交了命令，该如何补救才能让它避免 HUP 信号的影响呢？ 解决方法：这时想加 nohup 或者 setsid 已经为时已晚，只能通过作业调度和 disown 来解决这个问题了。让我们来看一下 disown 的帮助信息： 1234567891011disown [-ar] [-h] [jobspec ...] Without options, each jobspec is removed from the table of active jobs. If the -h option is given, each jobspec is not removed from the table, but is marked so that SIGHUP is not sent to the job if the shell receives a SIGHUP. If no jobspec is present, and neither the -a nor the -r option is supplied, the current job is used. If no jobspec is supplied, the -a option means to remove or mark all jobs; the -r option without a jobspec argument restricts operation to running jobs. The return value is 0 unless a jobspec does not specify a valid job. 可以看出，我们可以用如下方式来达成我们的目的： 用disown -h jobspec来使某个作业忽略HUP信号。 用disown -ah来使所有的作业都忽略HUP信号。 用disown -rh来使正在运行的作业忽略HUP信号。 需要注意的是，当使用过 disown 之后，会将把目标作业从作业列表中移除，我们将不能再使用jobs来查看它，但是依然能够用ps -ef查找到它。 但是还有一个问题，这种方法的操作对象是作业，如果我们在运行命令时在结尾加了“&amp;”来使它成为一个作业并在后台运行，那么就万事大吉了，我们可以通过jobs命令来得到所有作业的列表。但是如果并没有把当前命令作为作业来运行，如何才能得到它的作业号呢？答案就是用 CTRL-z（按住Ctrl键的同时按住z键）了！ CTRL-z 的用途就是将当前进程挂起（Suspend），然后我们就可以用jobs命令来查询它的作业号，再用bg jobspec来将它放入后台并继续运行。需要注意的是，如果挂起会影响当前进程的运行结果，请慎用此方法。 灵活运用 CTRL-z在我们的日常工作中，我们可以用 CTRL-z 来将当前进程挂起到后台暂停运行，执行一些别的操作，然后再用 fg 来将挂起的进程重新放回前台（也可用 bg 来将挂起的进程放在后台）继续运行。这样我们就可以在一个终端内灵活切换运行多个任务，这一点在调试代码时尤为有用。因为将代码编辑器挂起到后台再重新放回时，光标定位仍然停留在上次挂起时的位置，避免了重新定位的麻烦。 disown 示例1（如果提交命令时已经用“&amp;”将命令放入后台运行，则可以直接使用“disown”）123456789[root@pvcent107 build]# cp -r testLargeFile largeFile &amp;[1] 4825[root@pvcent107 build]# jobs[1]+ Running cp -i -r testLargeFile largeFile &amp;[root@pvcent107 build]# disown -h %1[root@pvcent107 build]# ps -ef |grep largeFileroot 4825 968 1 09:46 pts/4 00:00:00 cp -i -r testLargeFile largeFileroot 4853 968 0 09:46 pts/4 00:00:00 grep largeFile[root@pvcent107 build]# logout disown 示例2（如果提交命令时未使用“&amp;”将命令放入后台运行，可使用 CTRL-z 和“bg”将其放入后台，再使用“disown”）123456789101112[root@pvcent107 build]# cp -r testLargeFile largeFile2 [1]+ Stopped cp -i -r testLargeFile largeFile2[root@pvcent107 build]# bg %1[1]+ cp -i -r testLargeFile largeFile2 &amp;[root@pvcent107 build]# jobs[1]+ Running cp -i -r testLargeFile largeFile2 &amp;[root@pvcent107 build]# disown -h %1[root@pvcent107 build]# ps -ef |grep largeFile2root 5790 5577 1 10:04 pts/3 00:00:00 cp -i -r testLargeFile largeFile2root 5824 5577 0 10:05 pts/3 00:00:00 grep largeFile2[root@pvcent107 build]# screen场景：我们已经知道了如何让进程免受 HUP 信号的影响，但是如果有大量这种命令需要在稳定的后台里运行，如何避免对每条命令都做这样的操作呢？ 解决方法：此时最方便的方法就是 screen 了。简单的说，screen 提供了 ANSI/VT100 的终端模拟器，使它能够在一个真实终端下运行多个全屏的伪终端。screen 的参数很多，具有很强大的功能，我们在此仅介绍其常用功能以及简要分析一下为什么使用 screen 能够避免 HUP 信号的影响。我们先看一下 screen 的帮助信息： 12345678910111213141516171819SCREEN(1) SCREEN(1) NAME screen - screen manager with VT100/ANSI terminal emulation SYNOPSIS screen [ -options ] [ cmd [ args ] ] screen -r [[pid.]tty[.host]] screen -r sessionowner/[[pid.]tty[.host]] DESCRIPTION Screen is a full-screen window manager that multiplexes a physical terminal between several processes (typically interactive shells). Each virtual terminal provides the functions of a DEC VT100 terminal and, in addition, several control functions from the ISO 6429 (ECMA 48, ANSI X3.64) and ISO 2022 standards (e.g. insert/delete line and support for multiple character sets). There is a scrollback history buffer for each virtual terminal and a copy-and-paste mechanism that allows moving text regions between windows. 使用 screen 很方便，有以下几个常用选项： 用screen -dmS session name来建立一个处于断开模式下的会话（并指定其会话名）。 用screen -list来列出所有会话。 用screen -r session name来重新连接指定会话。 用快捷键CTRL-a d来暂时断开当前会话。 screen 示例1234567[root@pvcent107 ~]# screen -dmS Urumchi[root@pvcent107 ~]# screen -listThere is a screen on: 12842.Urumchi (Detached)1 Socket in /tmp/screens/S-root. [root@pvcent107 ~]# screen -r Urumchi 当我们用“-r”连接到 screen 会话后，我们就可以在这个伪终端里面为所欲为，再也不用担心 HUP 信号会对我们的进程造成影响，也不用给每个命令前都加上“nohup”或者“setsid”了。这是为什么呢？让我来看一下下面两个例子吧。 1. 未使用 screen 时新进程的进程树123456789[root@pvcent107 ~]# ping www.google.com &amp;[1] 9499[root@pvcent107 ~]# pstree -H 9499init─┬─Xvnc ├─acpid ├─atd ├─2*[sendmail] ├─sshd─┬─sshd───bash───pstree │ └─sshd───bash───ping 我们可以看出，未使用 screen 时我们所处的 bash 是 sshd 的子进程，当 ssh 断开连接时，HUP 信号自然会影响到它下面的所有子进程（包括我们新建立的 ping 进程）。 2. 使用了 screen 后新进程的进程树123456789[root@pvcent107 ~]# screen -r Urumchi[root@pvcent107 ~]# ping www.ibm.com &amp;[1] 9488[root@pvcent107 ~]# pstree -H 9488init─┬─Xvnc ├─acpid ├─atd ├─screen───bash───ping ├─2*[sendmail] 而使用了 screen 后就不同了，此时 bash 是 screen 的子进程，而 screen 是 init（PID为1）的子进程。那么当 ssh 断开连接时，HUP 信号自然不会影响到 screen 下面的子进程了。 总结现在几种方法已经介绍完毕，我们可以根据不同的场景来选择不同的方案。nohup/setsid 无疑是临时需要时最方便的方法，disown 能帮助我们来事后补救当前已经在运行了的作业，而 screen 则是在大批量操作时不二的选择了。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL学习笔记-主从与主主]]></title>
    <url>%2F2018%2F06%2F21%2FMySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%B8%BB%E4%BB%8E%E4%B8%8E%E4%B8%BB%E4%B8%BB%2F</url>
    <content type="text"><![CDATA[MySQL主从与主主复制原理 MySQL主从是基于BinLog的，主上须开启BinLog才能进行主从。 主从过程大致有3个步骤： Master将更改操作记录到BinLog里。 Slave将Master的BinLog事件（SQL语句）同步到从本机上并记录在RelayLog里。 Slave根据RelayLog里面的SQL语句按顺序执行。 Master上有一个LogDump线程，用来和Slave的I/O线程传递BinLog。 Slave上有两个线程，其中I/O线程用来同步主的BinLog并生成RelayLog，另外一个SQL线程用来把RelayLog里面的SQL语句执行一遍。 主从大概流程Master 开启二进制日志。 配置唯一的server-id。 获得Master二进制日志文件名及位置。 创建一个用于Slave和Master通信的用户。 Slave 配置唯一的server-id。 使用Master分配的用户读取Master二进制日志。 启用Slave服务。 详细过程Master服务器 修改配置文件 123456789101112131415# /etc/my.cnf[mysqld]# 开启二进制日志log-bin = mysql-bin# 设置唯一的server-idserver-id = 1# 其他配置# 不同步的数据库 binlog-ignore-db = mysql binlog-ignore-db = test binlog-ignore-db = information_schema# 只同步哪些数据库，除此之外，其他不同步binlog-do-db = test 重启MySQL，创建同步用户 12345mysql&gt; grant replication slave on *.* to dbsync@'%' identified by '123456';Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 查看Master状态，记录二进制文件名和位置 1234567mysql&gt; SHOW MASTER STATUS;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000001 | 154 | | | |+------------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec) File：mysql-bin.000001 Position： 154 Slave服务器 修改配置文件 1234# /etc/my.cnf[mysqld]# 设置唯一的server-idserver-id=2 重启MySQL，执行同步SQL语句 123456mysql&gt; CHANGE MASTER TO -&gt; MASTER_HOST='192.168.198.22', -&gt; MASTER_USER='dbsync', -&gt; MASTER_PASSWORD='123456', -&gt; MASTER_LOG_FILE='mysql-bin.000001', -&gt; MASTER_LOG_POS=154; 启动Slave同步进程 12mysql&gt; start slave;Query OK, 0 rows affected (0.01 sec) 查看Slave状态 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960mysql&gt; show slave status\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.198.22 Master_User: dbsync Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 154 Relay_Log_File: localhost-relay-bin.000002 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 154 Relay_Log_Space: 531 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: dd329e6e-a06d-11e8-ab57-000c2902a227 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) Slave_IO_Running，Slave_SQL_Running 都为Yes时表示主从同步成功。 主主即两台MySQL服务器互为主从。 大概思路 两台MySQL服务器都可读写，互为主从，默认只使用一台负责数据的写入，另一台做备用。 两台主库之间做高可用，采用Keeplived或Haproxy使用VIP对外提供服务，实现热备与负载。 详细过程配置文件修改MySQL配置文件 mysql-1： 1234567891011121314151617181920212223242526272829303132333435363738[mysqld]# 设置唯一的server-idserver_id = 1# 开启二进制日志log-bin = mysql-bin# 二进制日志清单log-bin-index = mysql-bin.index# 指定mysql的binlog日志的格式，mixed是混合模式binlog_format = MIXED# 将复制事件写入binlog，一台服务器既做主库又做从库此选项必须要开启log-slave-updates = true# 开启中继日志功能relay-log = relay-bin# 中继日志清单relay-log-index = salve-relay-bin.index# 用来设定数据库中自动增长的起点(即初始值)，因为这两能服务器都设定了一次自动增长值2，所以它们的起点必须得不同，这样才能避免两台服务器数据同步时出现主键冲突auto_increment_offset = 1 # 表示自增长字段每次递增的量，其默认值是1。它的值应设为整个结构中服务器的总数，用到两台服务器，所以值设为2auto_increment_increment = 2 # 设置binlog日志老化时间expire_logs_days = 10# 设置binlog日志文件大小最大值max_binlog_size = 900M# 设置relaylog文件大小最大值max_relay_log_size = 900M# 过滤不同步的数据库binlog-ignore-db = sysbinlog-ignore-db = mysqlbinlog-ignore-db = information_schemabinlog-ignore-db = performance_schema# 跳过一些同步错误slave-skip-errors = ddl_exist_errors# 设置主从连接超时时间slave-net-timeout = 30 mysql-2： server_id和auto_increment_offset与mysql-1不同。 123456789101112131415161718192021222324# my.cnf# 设置唯一的server-idserver_id = 2log-bin = mysql-binbinlog_format = MIXEDlog-slave-updates = truerelay-log = relay-binrelay-log-index = salve-relay-bin.index# 用来设定数据库中自动增长的起点(即初始值)，因为这两能服务器都设定了一次自动增长值2，所以它们的起点必须得不同，这样才能避免两台服务器数据同步时出现主键冲突auto_increment_offset = 2auto_increment_increment = 2 expire_logs_days = 10max_binlog_size = 900Mmax_relay_log_size = 900Mbinlog-ignore-db = sysbinlog-ignore-db = mysqlbinlog-ignore-db = information_schemabinlog-ignore-db = performance_schemaslave-skip-errors = ddl_exist_errorsslave-net-timeout = 30 slave-skip-errors slave_skip_errors选项有四个可用值，分别为： off，all，ErorCode，ddl_exist_errors。 默认情况下该参数值是off，我们可以列出具体的error code，也可以选择all，mysql5.6及MySQL Cluster NDB 7.3以及后续版本增加了参数ddl_exist_errors，该参数包含一系列error code（1007,1008,1050,1051,1054,1060,1061,1068,1094,1146）。 一些error code代表的错误如下： 1007：数据库已存在，创建数据库失败 1008：数据库不存在，删除数据库失败 1050：数据表已存在，创建数据表失败 1051：数据表不存在，删除数据表失败 1054：字段不存在，或程序文件跟数据库有冲突 1060：字段重复，导致无法插入 1061：重复键名 1068：定义了多个主键 1094：位置线程ID 1146：数据表缺失，请恢复数据库 1053：复制过程中主服务器宕机 1062：主键冲突 配置完成后进行重启MySQL操作。 mysql-1设置为mysql-2的主服务器 mysql-1创建同步用户 12345mysql&gt; grant replication slave on *.* to dbsync@'%' identified by '123456';Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 查看mysql-1的binlog状态，记录二进制文件名和位置 1234567mysql&gt; SHOW MASTER STATUS;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000002 | 154 | | 此处省略不同步的库 | |+------------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec) 在mysql-2服务器上执行同步SQL语句 123456mysql&gt; CHANGE MASTER TO -&gt; MASTER_HOST='192.168.198.22', -&gt; MASTER_USER='dbsync', -&gt; MASTER_PASSWORD='123456', -&gt; MASTER_LOG_FILE='mysql-bin.000002', -&gt; MASTER_LOG_POS=154; 启动并查看Slave状态 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061mysql&gt; start slave;mysql&gt; show slave status\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.198.22 Master_User: dbsync Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000002 Read_Master_Log_Pos: 154 Relay_Log_File: relay-bin.000002 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000002 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 154 Relay_Log_Space: 521 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: dd329e6e-a06d-11e8-ab57-000c2902a227 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: mysql-2设置为mysql-1的主服务器步骤相同，反过来即可。 mysql-2创建同步用户 12345mysql&gt; grant replication slave on *.* to dbsync@'%' identified by '123456';Query OK, 0 rows affected, 1 warning (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec) 查看mysql-2的binlog状态，记录二进制文件名和位置 1234567mysql&gt; SHOW MASTER STATUS;+------------------+----------+--------------+------------------+-------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |+------------------+----------+--------------+------------------+-------------------+| mysql-bin.000003 | 850 | | 此处省略不同步的库 | |+------------------+----------+--------------+------------------+-------------------+1 row in set (0.00 sec) 在mysql-1服务器上执行同步SQL语句 123456mysql&gt; CHANGE MASTER TO -&gt; MASTER_HOST='192.168.198.23', -&gt; MASTER_USER='dbsync', -&gt; MASTER_PASSWORD='123456', -&gt; MASTER_LOG_FILE='mysql-bin.000003', -&gt; MASTER_LOG_POS=850; 启动并查看Slave状态 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061mysql&gt; start slave;mysql&gt; show slave status\G*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.198.23 Master_User: dbsync Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000003 Read_Master_Log_Pos: 850 Relay_Log_File: relay-bin.000002 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 850 Relay_Log_Space: 521 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 2 Master_UUID: 8d6b7632-a23d-11e8-bb9f-000c29420492 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version:]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-sys模块]]></title>
    <url>%2F2018%2F06%2F21%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-sys%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[sys 模块提供了许多函数和变量来处理 Python 运行时环境的不同部分 。 处理命令行参数 在解释器启动后, argv 列表包含了传递给脚本的所有参数, 列表的第一个元素为脚本自身的名称。 使用sys模块获得脚本的参数1234567print "script name is", sys.argv[0] # 使用sys.argv[0]采集脚本名称if len(sys.argv) &gt; 1: print "there are", len(sys.argv)-1, "arguments:" # 使用len(sys.argv)-1采集参数个数-1为减去[0]脚本名称 for arg in sys.argv[1:]: #输出除了[0]外所有参数 print argelse: print "there are no arguments!" 如果是从标准输入读入脚本 (比如 “python &lt; sys-argv-example-1.py”), 脚本的名称将被设置为空串。 如果把脚本作为字符串传递给python (使用 -c 选项), 脚本名会被设置为 “-c”。 处理模块 path 列表是一个由目录名构成的列表, Python 从中查找扩展模块( Python 源模块, 编译模块,或者二进制扩展). 启动 Python 时,这个列表从根据内建规则, PYTHONPATH 环境变量的内容, 以及注册表( Windows 系统)等进行初始化. 由于它只是一个普通的列表, 你可以在程序中对它进行操作。 使用sys模块操作模块搜索路径123456print "path has", len(sys.path), "members"sys.path.insert(0, "samples") #将路径插入到path,[0]中import samplesys.path = [] #删除path中所有路径import random 使用sys模块查找内建模块builtin_module_names 列表包含 Python 解释器中所有内建模块的名称。 123456789101112131415161718def dump(module): print module, "=&gt;", if module in sys.builtin_module_names: #查找内建模块是否存在 print "&lt;BUILTIN&gt;" else: module = _ _import_ _(module) #非内建模块输出模块路径 print module._ _file_ _dump("os")dump("sys")dump("string")dump("strop")dump("zlib")os =&gt; C:\python\lib\os.pycsys =&gt; &lt;BUILTIN&gt;string =&gt; C:\python\lib\string.pycstrop =&gt; &lt;BUILTIN&gt;zlib =&gt; C:\python\zlib.pyd 使用sys模块查找已导入的模块modules 字典包含所有加载的模块. import 语句在从磁盘导入内容之前会先检查这个字典。 Python 在处理你的脚本之前就已经导入了很多模块。 1234print sys.modules.keys()['os.path', 'os', 'exceptions', '_ _main_ _', 'ntpath', 'strop', 'nt','sys', '_ _builtin_ _', 'site', 'signal', 'UserDict', 'string', 'stat'] 使用sys模块获得当前平台sys.platform 返回当前平台 出现如： “win32” “linux2” 等 处理标准输出/输入标准输入和标准错误 (通常缩写为 stdout 和 stderr) 是内建在每一个 UNIX 系统中的管道。 当你 print 某些东西时，结果前往 stdout 管道；当你的程序崩溃并打印出调试信息 (例如 Python 中的 traceback (错误跟踪)) 的时候，信息前往 stderr 管道。 1234567891011121314&gt;&gt;&gt; for i in range(3):... print'Dive in'Dive inDive inDive in&gt;&gt;&gt; import sys&gt;&gt;&gt; for i in range(3):... sys.stdout.write('Dive in')Dive inDive inDive in&gt;&gt;&gt; for i in range(3):... sys.stderr.write('Dive in')Dive inDive inDive in stdout 是一个类文件对象；调用它的 write 函数可以打印出你给定的任何字符串。 实际上，这就是 print 函数真正做的事情；它在你打印的字符串后面加上一个硬回车，然后调用 sys.stdout.write 函数。 在最简单的例子中，stdout 和 stderr 把它们的输出发送到相同的地方 和 stdout 一样，stderr 并不为你添加硬回车；如果需要，要自己加上。 stdout 和 stderr 都是类文件对象，但是它们都是只写的。 它们都没有 read 方法，只有 write 方法。然而，它们仍然是类文件对象，因此你可以将其它任何 (类) 文件对象赋值给它们来重定向其输出。 使用sys重定向输出123456789print 'Dive in' # 标准输出saveout = sys.stdout # 终在重定向前保存stdout，这样的话之后你还可以将其设回正常fsock = open('out.log', 'w') # 打开一个新文件用于写入。如果文件不存在，将会被创建。如果文件存在，将被覆盖。sys.stdout = fsock # 所有后续的输出都会被重定向到刚才打开的新文件上。print 'This message will be logged instead of displayed' # 这样只会将输出结果“打印”到日志文件中；屏幕上不会看到输出sys.stdout = saveout # 在我们将 stdout 搞乱之前，让我们把它设回原来的方式。 fsock.close() # 关闭日志文件。 重定向错误信息 fsock = open(‘error.log’, ‘w’) # 打开你要存储调试信息的日志文件。sys.stderr = fsock # 将新打开的日志文件的文件对象赋值给stderr以重定向标准错误。raise Exception, ‘this error will be logged’ # 引发一个异常,没有在屏幕上打印出任何东西,所有正常的跟踪信息已经写进error.log 还要注意你既没有显式关闭日志文件，也没有将 stderr 设回最初的值。 这样挺好，因为一旦程序崩溃 (由于引发的异常)，Python 将替我们清理并关闭文件 打印到 stderr向标准错误写入错误信息是很常见的，所以有一种较快的语法可以立刻导出信息 。 12345&gt;&gt;&gt; print 'entering function'entering function&gt;&gt;&gt; import sys&gt;&gt;&gt; print &gt;&gt; sys.stderr, 'entering function'entering function print 语句的快捷语法可以用于写入任何打开的文件 (或者是类文件对象)。 在这里，你可以将单个print语句重定向到stderr而且不用影响后面的print语句。 使用sys模块退出程序12import syssys.exit(1) 注意 sys.exit 并不是立即退出. 而是引发一个 SystemExit 异常. 这意味着你可以在主程序中捕获对 sys.exit 的调用。 捕获sys.exit调用 12345678910import sysprint "hello"try: sys.exit(1)except SystemExit: # 捕获退出的异常 pass # 捕获后不做任何操作print "there"hellothere 如果准备在退出前自己清理一些东西(比如删除临时文件), 你可以配置一个 “退出处理函数”(exit handler), 它将在程序退出的时候自动被调用 。 另一种捕获sys.exit调用的方法 12345678910def exitfunc(): print "world"sys.exitfunc = exitfunc # 设置捕获时调用的函数print "hello"sys.exit(1) # 退出自动调用exitfunc()后，程序依然退出了print "there" # 不会被 printhelloworld 总结123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164# sys_demo.py sys模块相关函数# 该模块含有解释器的一些变量，与解释器交互的函数import sysdef sys_demo(): # 默认编码 print(sys.getdefaultencoding()) # Python版本 print(sys.version) # 添加模块路径到搜索路径 sys.path.append('./module') # (函数)打印异常信息 ''' try: 1 / 0 except: types, value, back = sys.exc_info() # 捕获异常 sys.excepthook(types, value, back) # 打印异常 ''' # 输入和输出 # 标准输出 sys.stdout.write("&gt;&gt; ") # 刷新输出(貌似3.6版本无论是Linux还是winodws加不加都一样) sys.stdout.flush() # 标准输入 strs = sys.stdin.readline()[:-1] # 错误输出 sys.stderr.write("输入内容为： &#123;&#125;".format(strs)) sys.stderr.flush()def sys_func(): # 传递给python脚本的命令行参数列表 # =&gt; python p.py -&gt; ['p.py'] / python p.py a 1 -&gt; ['p.py', 'a', '1'] / 程序内执行 -&gt; ['程序本身路径'] # sys.argv[0] 获取脚本名 # sys.argv[1] 获取第一个参数 # ... lists = sys.argv # 默认字符集名称 strs = sys.getdefaultencoding() # 系统文件名字符集名称 strs = sys.getfilesystemencoding() # 返回object的引用计数 num = sys.getrefcount(object) # 已加载的模块，可修改，但不能通过修改返回的字典进行修改 dicts = sys.modules # 模块搜索路径,初始化时使用PYTHONPATH环境变量的值 lists = sys.path # 动态添加模块搜索路径 sys.path.append('./test') # 平台标识符(系统身份进行详细的检查，推荐使用) strs = sys.platform # python解释器版本 strs = sys.version # 监视器C API版本 num = sys.api_version # 线程信息 lists = sys.thread_info # 捕获异常 types, value, back = sys.exc_info() # 打印异常 sys.excepthook(types, value, back) # 引发SystemExit异常退出Python(可以try), 范围[0,127], None==0, "string"==1 # sys.exit([arg]) # 正常退出 sys.exit(0) # 最大递归数(堆栈最大深度) num = sys.getrecursionlimit() # 修改最大递归数 sys.setrecursionlimit(5000) # 获取线程切换间隔 fnum = sys.getswitchinterval() # 设置线程切换间隔, 单位秒 sys.setswitchinterval(0.005) # 解释器的检查间隔 num = sys.getcheckinterval() # 设置解释器检查间隔, 执行(默认)100个虚拟指令执行一次检查, 值为&lt;=0时,检查每个虚拟指令 sys.setcheckinterval(100) # sys.stdin // 标准输入流 strs = sys.stdin.readline()[:-1] # sys.stdout // 标准出入输出 sys.stdout.write("&gt;&gt;") sys.stdout.flush() # sys.stderr // 标注错误流 sys.stderr.write("&gt;&gt;") # 所有模块 (注:非导入模块) lists = sys.builtin_module_names # Python安装路径 path = sys.base_exec_prefix # 同base_exec_prefix path = sys.base_prefix # 同base_exec_prefix path = sys.exec_prefix # 同base_exec_prefix path = sys.prefix # Python解释器的绝对路径 path = sys.executable # 本机字节顺序指示器, big-endian(最高有效字节在第一位)值为'big', little-endian(最低有效字节在第一位)值为'little' strs = ys.byteorder # python版权 strs = sys.copyright # 16进制版本号 num = sys.hexversion # 当前运行的解释器的信息 lists = sys.implementation # 解释器当前分配的内存块的数量 num = sys.getallocatedblocks() # 是否不会尝试导入源模块是写入.pyc文件 (False会写入.pyc文件) boolean = sys.dont_write_bytecode # 返回对象的大小bit, 只计算自身内存消耗, 不计算引用对象的内存消耗, 调用对象的__sizeof__(), default没有获取到默认返回值 # sys.getsizeof(object[, default]) num = sys.getsizeof(object) # 解释器是否正在被关机 boolean = sys.is_finalizing() # 最大整数值(2 ** 31 -1), 与系统有关 num = sys.maxsize # 最大Unicode值的整数 (1114111) num = sys.maxunicode # 解释器主提示符 strs = sys.ps1 # 解释器次提示符 strs = sys.ps2 # 调用函数 sys.call_tracing(func, ("arg",)) # 清除内部类型缓存 sys._clear_type_cache() # 打印CPython内存分配器状态的低级信息 sys._debugmallocstats() # 设置profile函数, 默认None sys.setprofile(profilefunc) # 获取profile函数 sys.getprofile() # 设置跟踪函数, def tracefunc(frame、event 和arg): sys.settrace(tracefunc) # 获取跟踪函数, 默认None sys.gettrace() # 设置包装 def wrapper(coro): sys.set_coroutine_wrapper(wrapper) # 包装, 默认None sys.get_coroutine_wrapper()if __name__ == '__main__': sys_demo() #sys_func()]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-timeit模块]]></title>
    <url>%2F2018%2F06%2F21%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-timeit%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[timeit模块 timeit.timeit(stmt=’pass’, setup=’pass’, timer=\&lt;defaulttimer>, number=1000000) 返回：返回执行stmt这段代码number遍所用的时间，单位为秒，float型参数： stmt：要执行的那段代码setup：执行代码的准备工作,初始化代码或构建环境导入语句,不计入时间，一般是import之类的timer：这个在win32下是time.clock()，linux下是time.time()，默认的，不用管number：要执行stmt多少遍 12345678910111213import timeitprint(timeit.timeit('sum(x)', 'x=(i for i in range(100))'))def test(): L = [i for i in range(100)]if __name__ == '__main__': import timeit print(timeit.timeit('test()', setup="from __main__ import test", number=100000)) 120.101190333638963780.321354159539091 repeat(stmt=’pass’, setup=’pass’, timer=\&lt;defaulttimer>, repeat=3, number=1000000) 这个函数比timeit函数多了一个repeat参数而已，表示重复执行timeit这个过程多少遍，返回一个列表，表示执行每遍的时间 当然，为了方便，python还用了一个Timer类，Timer类里面的函数跟上面介绍的两个函数是一样的。 12345678910111213import timeitprint(timeit.repeat('sum(x)', 'x=(i for i in range(100))'))def test(): L = [i for i in range(100)]if __name__ == '__main__': import timeit print(timeit.repeat('test()', setup="from __main__ import test", repeat=5, number=100000)) 12[0.106236657178438, 0.1073258932847974, 0.10616481121618107][0.29790928249832355, 0.3105618696372121, 0.30556985822396787, 0.30301376705486294, 0.30120607132593435] timeit.default_timer() 默认的计时器 123456789101112131415161718192021import timeit# 计时器1s1 = timeit.default_timer()print(timeit.repeat('sum(x)', 'x=(i for i in range(100))'))def test(): L = [i for i in range(100)]if __name__ == '__main__': import timeit print(timeit.repeat('test()', setup="from __main__ import test", repeat=5, number=100000))# 计时器2s2 = timeit.default_timer()print(s1, s2, s2 - s1) 123[0.10179332653647721, 0.10404193409330395, 0.10724164534096042][0.30232567090443807, 0.30226451630561224, 0.2942087877875549, 0.30265410958904115, 0.29502689091730194]0.0 1.8100781923555895 1.8100781923555895 Timer类，为了方便，python还用了一个Timer类，Timer类里面的函数跟上面介绍的两个函数是一样。 class timeit.Timer(stmt=’pass’, setup=’pass’,timer=\) Timer.timeit(number=1000000) Timer.repeat(repeat=3,number=1000000) 12345678910import timeitdef test(): L = [i for i in range(100)]time_obj = timeit.Timer(stmt="test()", setup="from __main__ import test")print(time_obj.timeit(number=20000))print(time_obj.repeat(repeat=2, number=20000))print(time_obj.repeat(repeat=5, number=20000)) 1230.06169643712451932[0.060732503797572286, 0.06019152080795917][0.05959408741942987, 0.05915531386421796, 0.06217412724262039, 0.06109216126339412, 0.06159337238104362] 命令行调用 python -m timeit [-n N][-r N][-s S][-t][-c][-h][statement…] -n N 执行指定语句的次数-r N 重复测量的次数(默认3次)-s S 指定初始化代码活构建环境的导入语句(默认pass)-t 使用time.time() (不推荐)-c 使用time.clock() (不推荐)-v 打印原始计时结果-h 帮助 123456$ python -m timeit '"-".join(str(n) for n in range(100))'10000 loops, best of 3: 40.3 usec per loop$ python -m timeit '"-".join([str(n) for n in range(100)])'10000 loops, best of 3: 33.4 usec per loop$ python -m timeit '"-".join(map(str, range(100)))'10000 loops, best of 3: 25.2 usec per loop]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-random模块]]></title>
    <url>%2F2018%2F06%2F21%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-random%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[1. random 生成伪随机数 伪随机数是可预测的，严格意义上不具有随机性质，通常用数学公式的方法（比如统计分布，平方取中等）获得 正如数列需要有首项，产生伪随机数需要一个初值用来计算整个序列，这个初值被称为“种子”。种子可以是一个固定的值，也可以是根据当前系统状态确定的值。 2. random方法1. seed()：改变随机数生成器的种子seed12345678910111213141516# seed() 方法改变随机数生成器的种子，可以在调用其他随机模块函数之前调用此函数# seed()是不能直接访问的，需要导入 random 模块，然后通过 random 静态对象调用该方法import randomrandom.seed()print ("使用默认种子生成随机数：", random.random())random.seed(10)print ("使用整数种子生成随机数：", random.random())random.seed('hello')print ("使用字符串种子生成随机数：", random.random())random.seed(10)print(random.random())random.seed('hello')print(random.random()) 执行后结果为： 12345使用默认种子生成随机数： 0.7446141652673619使用整数种子生成随机数： 0.5714025946899135使用字符串种子生成随机数： 0.35377544047307220.57140259468991350.3537754404730722 也就是说，seed( ) 用于指定随机数生成时所用算法开始的整数值，如果使用相同的seed( )值，则每次生成的随即数都相同，如果不设置这个值，则系统根据时间来自己选择这个值，此时每次生成的随机数因时间差异而不同。 2. getrandbits(k)：返回一个具有x位（bit）随机整数12345678910111213141516171819202122#最小值为0，最大值为15&gt;&gt;&gt; random.getrandbits(4)8&gt;&gt;&gt; random.getrandbits(4)14&gt;&gt;&gt; random.getrandbits(4)15&gt;&gt;&gt; random.getrandbits(4)12&gt;&gt;&gt; random.getrandbits(4)10&gt;&gt;&gt; random.getrandbits(4)7#最小值为0，最大值为255random.getrandbits(8)&gt;&gt;&gt; random.getrandbits(8)239&gt;&gt;&gt; random.getrandbits(8)109&gt;&gt;&gt; random.getrandbits(8)41 3. randrange([start],stop,[step])：从指定范围内，按指定基数递增的集合中获取一个随机数123456789&gt;&gt;&gt; random.randrange(100)52&gt;&gt;&gt; random.randrange(100)37&gt;&gt;&gt; random.randrange(1, 100, 2)25&gt;&gt;&gt; random.randrange(1, 100, 2)99 4. randint(a, b)：随机生成a，b之间的整数1234&gt;&gt;&gt; random.randint(10, 15)13&gt;&gt;&gt; random.randint(10, 15)15 5. choice(seq)：从非空序列中随机挑选一个元素123456789&gt;&gt;&gt; random.choice([1, 3, 6, [3, 6]])1&gt;&gt;&gt; random.choice([1, 3, 6, [3, 6]])[3, 6]&gt;&gt;&gt; random.choice('hello, python')'y'&gt;&gt;&gt; random.choice('hello, python')'n' 6. shuffle(seq, [random] )：将序列的所有元素随机排序1234&gt;&gt;&gt; l = list(range(10))&gt;&gt;&gt; random.shuffle(l)&gt;&gt;&gt; l[2, 6, 3, 4, 7, 5, 0, 8, 1, 9] 7. sample(seq, n)：从指定的序列中，选取n个随机且独立的元素组，不改变原序列12345&gt;&gt;&gt; l = list(range(10))&gt;&gt;&gt; l[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; random.sample(l, 5)[5, 3, 1, 7, 0] 8. random()：随机生成一个0到1之间的浮点数1234&gt;&gt;&gt; random.random()0.585264330791065&gt;&gt;&gt; random.random()0.4522431095454009 9. uniform(a, b)：随机生成一个a到b之间的浮点数1234&gt;&gt;&gt; random.uniform(1, 2)1.489053942575901&gt;&gt;&gt; random.uniform(1, 2)1.7829116037672397 常见的函数就介绍到这，还有一些以后需要用到在记录。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python模块</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python由列表组成的列表中注意的问题]]></title>
    <url>%2F2018%2F06%2F21%2F%E5%88%9D%E6%AD%A5%E4%BA%86%E8%A7%A3%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[今天学习python二分查找，遇见了时间复杂度，有点懵，先简单了解一下。 在计算机科学中，算法的时间复杂度是一个函数，它定性描述了该算法的运行时间。这是一个关于代表算法输入值的字符串的长度的函数。时间复杂度常用大O符号表述，不包括这个函数的低阶项和首项系数。 不懂。继续查资料。 时间频度： 一个算法执行所消耗的时间，从理论上是不能算出来的，必须上机运行测试才能知道。但我们不可能也没有必要对每个算法都上机测试，只需知道算法花费的时间是多少。 一个算法花费的时间与算法中语句的执行次数成正比，哪个算法中语句执行次数多，它花费的时间就多。 一个算法中的语句执行次数称为语句频度或时间频度。记为T(n)。 时间复杂度 n称为问题的规模，当n不断变化时，时间频度T(n)也会不断变化。但有时我们想知道它变化时呈现什么规律。因此我们引入时间复杂度概念。 一般情况下，算法中基本操作重复执行的次数是问题规模n的某个函数，用T(n)表示，若有某个辅助函数f(n)，使得当n趋近于无穷大时，T(n)/f(n)的极限值为不等于零的常数，则称f(n)是T(n)的同数量级函数。记作T(n)=O(f(n)) 称O(f(n))为算法的渐进时间复杂度，简称时间复杂度。 注意，时间频度与时间复杂度时不同的，时间频度不同但时间复杂度可能相同。 如：T(n)=n^2+3n+4与T(n)=4n2+2n+1它们的频度不同，但时间复杂度相同，都为O(n^2)。 常见的时间复杂度有： 常数阶O(1) &lt; 对数阶O(log2n) &lt; 线性阶O(n) &lt; 线性对数阶O(nlog2n) &lt; 平方阶O(n^2) &lt; 方阶O(n3) &lt; k次方阶O(n^k) &lt; 指数阶O(2^n) &lt; O(n!)&lt;O(n^n) 目前不想了解太深入，目前主要还是以语法学习为主，以后学习算法时再详细了解。 举几个常见的例子： 时间复杂度为O(1)，就是最低的时间复杂度了，也就是耗时/耗空间与输入数据大小无关，无论输入数据增大多少倍，耗时/空间都不变。哈希算法就是典型的O(1)时间复杂度，无论数据规模多大，都可以在一次计算后找到目标。 时间复杂度为O(n)，就代表数据量增大几倍，耗时也增大几倍。比如常见的遍历算法。 12for i in list: print(i) 时间复杂度为O(n^2)，就代表数据量增大n倍时，耗时增大n的平方倍，这是比线性更高的时间复杂度。比如冒泡排序，就是典型的O(n^2)的算法，对n个数排序，需要扫描n×n次。 123456def bubble_sort(numbers): for i in range(len(numbers) -1): for j in range(len(numbers) - i - 1): if numbers[j] &gt; numbers[j + 1]: numbers[j], numbers[j + 1] = numbers[j + 1], numbers[j] return numbers 时间复杂度为O(logn)，当数据量增大n倍时，耗时量增大logn倍（这里的log是以2为底的，比如，当数据量增大256倍时，耗时只增大8倍，是比线性还要低的时间复杂度）。二分查找就是O(logn)的算法。 123456789101112def binary_search(data_list, val): low = 0 high = len(data_list) - 1 while low &lt; high: mid = (low + high) // 2 if data_list[mid] == val: return mid elif data_list[mid] &gt; val: high = mid - 1 else: low = mid + 1 return O(nlogn)同理，就是n乘以logn，当数据增大256倍时，耗时增大256*8=2048倍。这个复杂度高于线性低于平方。归并排序就是O(nlogn)的时间复杂度。]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记-二分查找与bisect模块]]></title>
    <url>%2F2018%2F06%2F20%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE%E4%B8%8Ebisect%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[1. 二分查找 Python 的列表（list）内部实现是一个数组，也就是一个线性表。在列表中查找元素可以使用 list.index() 方法，其时间复杂度为O(n)。对于大数据量，则可以用二分查找进行优化。二分查找要求对象必须有序，其基本原理如下： 从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜素过程结束； 如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。 如果在某一步骤数组为空，则代表找不到。 二分查找也成为折半查找，算法每一次比较都使搜索范围缩小一半， 其时间复杂度为 O(logn)。 我们分别用递归和循环来实现二分查找： 123456789101112131415161718192021222324def binary_search_loop(data_list, val): low = 0 high = len(data_list) - 1 while low &lt; high: mid = (low + high) // 2 if data_list[mid] == val: return mid elif data_list[mid] &gt; val: high = mid - 1 else: low = mid + 1 returndef binary_search_recursion(data_list, val, low, high): if high &lt; low: return mid = (low + high) // 2 if data_list[mid] == val: return mid elif data_list[mid] &lt; val: return binary_search_recursion(data_list, val, mid + 1, high) else: return binary_search_recursion(data_list, val, low, mid - 1) 接着对这两种实现进行一下性能测试： 123456789101112131415161718192021if __name__ == '__main__': import random import timeit data_list = [random.randint(0, 100000) for _ in range(100000)] data_list.sort() def test_recursion(): binary_search_recursion(data_list, 999, 0, len(data_list) - 1) def test_loop(): binary_search(data_list, 999) t1 = timeit.Timer('test_recursion()', setup='from __main__ import test_recursion') t2 = timeit.Timer('test_loop()', setup='from __main__ import test_loop') print("Recursion: ", t1.timeit()) print("Loop: ", t2.timeit()) 执行结果如下： 12Recursion: 5.192518525994553Loop: 3.60416338798188 可以看出循环方式比递归效率高。 2. bisect模块 2.1 用bisect来搜索并插入bisect(haystack, needle)在haystack（干草垛）里搜索needle（针）的位置，该位置满足的条件是，把needle插入这个位置之后，haystack还能保持升序。 也就是说这个函数返回的位置前面的值，都小于或等于needle的值。 其中haystack必须是一个有序的序列。 你可以先用bisect(haystack, needle)查找位置index，再用haystack.inset(index, needle)来插入新值。但你也可以用insort来一步到位，并且后者速度更快一些。 举例： 123456789101112131415161718192021222324252627282930import bisectimport syshaystack = [1, 4, 5, 6, 8, 12, 15, 20, 21, 23, 23, 26, 29, 30]needles = [0, 1, 2, 5, 8, 10, 22, 23, 29, 30, 31]ROW_FMT = '&#123;0:2d&#125; @ &#123;1:2d&#125; &#123;2&#125;&#123;0:&lt;2d&#125;'def demo(bisect_fn): for needle in reversed(needles): # 用特定的bisect函数来计算元素应该出现的位置 position = bisect_fn(haystack, needle) # 利用该位置来算出需要几个分隔符号 offset = position * ' |' # 把元素和其应该出现的位置打印出来 print(ROW_FMT.format(needle, position, offset))if __name__ == '__main__': # 根据命令上最后一个参数来选用bisect函数 if sys.argv[-1] == 'left': bisect_fn = bisect.bisect_left else: bisect_fn = bisect.bisect # 把选定的函数在抬头打印出来 print('DEMO:', bisect_fn.__name__) print('haystack -&gt;', ' '.join('%2d' % n for n in haystack)) demo(bisect_fn) 1234567891011121314C:\Users\Peng.Gao\Desktop\fluency_python\c2&gt;python bisect_demo.pyDEMO: bisecthaystack -&gt; 1 4 5 6 8 12 15 20 21 23 23 26 29 3031 @ 14 | | | | | | | | | | | | | |3130 @ 14 | | | | | | | | | | | | | |3029 @ 13 | | | | | | | | | | | | |2923 @ 11 | | | | | | | | | | |2322 @ 9 | | | | | | | | |2210 @ 5 | | | | |10 8 @ 5 | | | | |8 5 @ 3 | | |5 2 @ 1 |2 1 @ 1 |1 0 @ 0 0 bisect函数其实是bisect_right函数的别名，后者还有个姊妹函数叫bisect_left。 他们的区别在于，bisect_left返回的插入位置是原序列中跟被插入元素相等的元素的位置，也就是新元素会被放置于与它相等的元素的前面，而bisect_right返回的则是跟它相等的元素之后的位置。 这个细微的差别可能对于整数序列来讲没什么用，但是对于那些值相等但是形式不同的数据类型来讲，结果就不一样了。 比如说虽然1 == 1.0的返回值是True，但1和1.0其实是两个不同的元素。 我们试试使用bisect_left来运行上面的模块： 1234567891011121314C:\Users\Peng.Gao\Desktop\fluency_python\c2&gt;python bisect_demo.py leftDEMO: bisect_lefthaystack -&gt; 1 4 5 6 8 12 15 20 21 23 23 26 29 3031 @ 14 | | | | | | | | | | | | | |3130 @ 13 | | | | | | | | | | | | |3029 @ 12 | | | | | | | | | | | |2923 @ 9 | | | | | | | | |2322 @ 9 | | | | | | | | |2210 @ 5 | | | | |10 8 @ 4 | | | |8 5 @ 2 | |5 2 @ 1 |2 1 @ 0 1 0 @ 0 0 对比可以发现，值1、8、23、29和30的插入位置变成了原序列中这些值的前面。 分析完后，改造一下函数，让它直接返回插入后的结果： 1234567891011121314151617181920import bisectimport syshaystack = [1, 4, 5, 6, 8.0, 12, 15, 20, 21, 23, 23, 26, 29, 30]needles = [0, 1, 2, 5, 8, 10, 22, 23, 29, 30, 31]def demo(bisect_fn): for needle in reversed(needles): position = bisect_fn(haystack, needle) haystack.insert(position, needle) return haystackif __name__ == '__main__': if sys.argv[-1] == 'left': bisect_fn = bisect.bisect_left else: bisect_fn = bisect.bisect print(demo(bisect_fn)) 执行： 1234567# 8在8.0后面C:\Users\Peng.Gao\Desktop\fluency_python\c2&gt;python bisect_demo.py[0, 1, 1, 2, 4, 5, 5, 6, 8.0, 8, 10, 12, 15, 20, 21, 22, 23, 23, 23, 26, 29, 29, 30, 30, 31]# 8在8.0前面C:\Users\Peng.Gao\Desktop\fluency_python\c2&gt;python bisect_demo.py left[0, 1, 1, 2, 4, 5, 5, 6, 8, 8.0, 10, 12, 15, 20, 21, 22, 23, 23, 23, 26, 29, 29, 30, 30, 31] bisect可以用来建立一个用数字作为索引的查询表格，比如说把分数和成绩对应起来： 1234567891011import bisectdef grade(score, breakpoints=list(range(60, 100, 10)), grades='FDCBA'): i = bisect.bisect(breakpoints, score) return grades[i]if __name__ == '__main__': r = [grade(score) for score in [22, 33, 66, 88, 50, 70, 90, 100]] print(r) 2.2 用bisect.insort插入新元素排序很耗时，因此在等到一个有序序列之后，我们最好能保持它的有序。 bisect.insort就是为了这个而存在的。 insort(seq, item)把变量item插入到序列seq中，并能保持seq的升序顺序。 123456789101112import bisectimport randomSIZE = 7random.seed(1729)my_list = []for i in range(SIZE): new_item = random.randrange(SIZE*2) bisect.insort(my_list, new_item) print('%2d -&gt;' % new_item, my_list) 输出结果如下： 123456710 -&gt; [10] 0 -&gt; [0, 10] 6 -&gt; [0, 6, 10] 8 -&gt; [0, 6, 8, 10] 7 -&gt; [0, 6, 7, 8, 10] 2 -&gt; [0, 2, 6, 7, 8, 10]10 -&gt; [0, 2, 6, 7, 8, 10, 10] insort和bisect一样也有个变体叫insort_left，这个变体在背后用的是bisect_left。 附上一个insort和sort的执行时间测试： 1234567891011121314151617181920212223242526272829303132333435363738import bisectimport randomimport timeitdef biscet_insort(list_1, size): random.seed(1729) for i in range(size): new_item = random.randrange(size * 2) bisect.insort(list_1, new_item) return list_1def list_sort(list_2, size): random.seed(1729) for i in range(size): new_item = random.randrange(size * 2) list_2.append(new_item) list_2.sort() return list_2if __name__ == '__main__': list_1, list_2 = [], [] size = 1000 def test_insort(): biscet_insort(list_1, size) def test_sort(): list_sort(list_2, size) t_1 = timeit.Timer('test_insort()', setup='from __main__ import test_insort') t_2 = timeit.Timer('test_sort()', setup='from __main__ import test_sort') print(t_1.timeit(number=100)) print(t_2.timeit(number=100)) 执行结果如下： 121.357789898457706685.78586909665677]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python模块</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python由列表组成的列表中注意的问题]]></title>
    <url>%2F2018%2F06%2F20%2FPython%E7%94%B1%E5%88%97%E8%A1%A8%E7%BB%84%E6%88%90%E7%9A%84%E5%88%97%E8%A1%A8%E4%B8%AD%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[有时候我们需要初始化一个嵌套着几个列表的列表。 最好的选择是使用列表推导： 12345&gt;&gt;&gt; board = [['-'] * 3 for i in range(3)][['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]&gt;&gt;&gt; board[1][2] = 'X'&gt;&gt;&gt; board[['-', '-', '-'], ['-', '-', 'X'], ['-', '-', '-']] 还有一种看上去是个有人的捷径，但实际上是错误的： 123456&gt;&gt;&gt; board = [['-'] * 3] * 3&gt;&gt;&gt; board[['-', '-', '-'], ['-', '-', '-'], ['-', '-', '-']]&gt;&gt;&gt; board[1][2] = 'X'&gt;&gt;&gt; board[['-', '-', 'X'], ['-', '-', 'X'], ['-', '-', 'X']] 错误的地方在于：第二种方法最终生成的列表其实包含3个指向用一个列表的引用。我们不做修改的时候，看起来还好，一旦我们试图修改其中包含的某一个列表中的元素时，就暴露了列表内的3个引用指向同一个对象的事实。 如图： 列表推导生成列表： 使用*便捷生成列表：]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中__name__ == '__main__'的作用]]></title>
    <url>%2F2018%2F06%2F20%2FPython%E4%B8%AD__name__%20%3D%20__main__%E7%9A%84%E4%BD%9C%E7%94%A8%2F</url>
    <content type="text"><![CDATA[原理 学习python的过程中遇见了if __name__ = = ‘__main__’条件判断。 不明白其作用，去网上查了一下。 网上有一句话概括了这段代码的含义： 让你写的脚本模块既可以导入到别的模块中用，另外该模块自己也可执行。 我们怎么去理解它呢？ 其实很简单，我们只需要弄清楚2点： 执行python模块时，也会执行其中所import的模块 __name__是python内置的全局变量，全局变量__name__存放的是模块的名字。 如果当前模块是主模块或者直接被执行时，此时模块的__name__变量就是__main__，如果当前模块被其他模块调用，则当前模块的__name__就是该模块的名字。 我们举个例子来说明： 情况一：直接执行当前模块时 新建一个test_name_1.py模块： 12# test_name_1.pyprint(__name__) 输出为： 1__main__ 继续扩展： 12345678# test_name_1.pydef test(): print('__name__=', __name__) print(__name__ == '__main__')if __name__ == '__main__': print("Test:") test() 执行test_name_1.py时，输出为： 123Test:__name__= __main__True 也就是说 当你直接执行test_name_1.py时，if __name__ == __main__成立，此时就会执行if语句里面的代码。 情况二： 被其他模块导入并执行时 我们新建一个test_name_2.py模块： 1234# test_name_2.pyimport test_name_1 as tn1tn1.test() 输出为： 12__name__= test_name_1False 注意，此时调用test_name_1模块时，__name__就发生了变化，变成了test_name_1的模块名，而不是’__main__了’。 所以执行test_name_2.py时，执行其中import导入的模块test_name_1.py时，if __name__ == __main__不成立，此时就不会执行if语句里面的代码。 应用场景 加上if __name__ == ‘__main__’ 应用于什么场景？ 我们想排查问题的时候通常会在模块下面加入一些我们调试的代码，我之前是这样做的： 12345678# test_name_1.pydef test(): print('__name__=', __name__) print(__name__ == '__main__')print('Test:')test() 直接执行test_name_1.py时，输出结果和加上if __name == ‘__main__’相同： 123Test:__name__= __main__True 但是这样做有个很大的弊端： 当你从另外一个模块导入此模块时，执行另外一个模块时，也会执行被导入的模块。 比如我们新建一个test_name_2.py 12# test_name_2.pyimport test_name_1 as tn1 执行test_name_2.py时，就会出现一些我在test_name_1.py加入的调试代码： 123test:__name__= test_name_1False 此时，如果不想删除调试代码方便以后的调试，就需要把调试代码放在if __name__ == __main__当中了。 通过这个特性可以在if语句里面添加测试代码，方便后续的测试。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python字符串格式化-占位符与format用法]]></title>
    <url>%2F2018%2F06%2F18%2FPython%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A0%BC%E5%BC%8F%E5%8C%96-%E5%8D%A0%E4%BD%8D%E7%AC%A6%E4%B8%8Eformat%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[字符串格式化方法分为两种：占位符（%）和str.format方式。 占位符方式在python2.x中用的比较广泛，随着python3.x的使用越来越广，str.format方式使用的更加广泛。 个人来讲我更喜欢str.format，但是大部分教程还使用%。 1 占位符（%） 格式 描述 %% 百分号标记 %c 字符及ASCII码 %s 字符串（str） %r 字符串（repr） %d 有符号整数（十进制） %u 无符号整数（十进制） %o 无符号整数（八进制） %x 无符号整数（十六进制） %X 无符号整数（十六进制大写字符） %e 浮点数字（科学计数法） %E 浮点数字（科学计数法，用E代替e） %f 浮点数字（用小数点符号） %g 浮点数字（用根据值的大小采用%e或%f） %G 浮点数字（类似于%g） %p 指针（用十六进制打印值的内存地址） %n 存储输出字符的数量放进参数列表的下一个变量中 1234567891011121314151617&gt;&gt;&gt; age = 25&gt;&gt;&gt; print("My age is %d" %age)My age is 25&gt;&gt;&gt; print("My name is %s" %name)My name is gao&gt;&gt;&gt; x, y = 2, 4&gt;&gt;&gt; print('Vector(%r, %r)' % (x, y))Vector(2, 4)&gt;&gt;&gt; print('%6.3f' % 2.3) 2.300&gt;&gt;&gt; print('%.3f' % 2.3)2.300&gt;&gt;&gt; print('%f' % 2.3)2.300000 2 format方法 2.1 映射 位置映射 123&gt;&gt;&gt; name = 'gao'&gt;&gt;&gt; print('&#123;&#125;,&#123;&#125;!'.format('hello', name))hello,gao! 关键字映射 12&gt;&gt;&gt; print('&#123;name&#125; is &#123;age&#125; years old.'.format(age=25, name='gao'))gao is 25 years old. 元素访问 123456&gt;&gt;&gt; metro_areas = ('Tokyo', 'JP', 36.933, (35.689722, 139.691667))&gt;&gt;&gt; '&#123;0[0]&#125; | &#123;0[3][0]&#125; | &#123;0[3][1]&#125;'.format(metro_areas)'Tokyo | 35.689722 | 139.691667'&gt;&gt;&gt; '&#123;1[0]&#125;:&#123;0[1]&#125;'.format((1, 2), [3, 4])'3:2' 2.2 指定格式结合’:’使用 指定精度 f 1234&gt;&gt;&gt; '&#123;:.4f&#125;'.format(40.808611)'40.8086'&gt;&gt;&gt; '&#123;:9.4f&#125;'.format(40.808611)' 40.8086' 指定进制 b、d、o、x、分别表示二进制、十进制、八进制、十六进制 1234567&gt;&gt;&gt; '&#123;:b&#125;'.format(25)'11001'&gt;&gt;&gt; '&#123;:d&#125;'.format(25)'25'&gt;&gt;&gt; '&#123;:x&#125;'.format(25)'19'&gt;&gt;&gt; '&#123;:o&#125;'.format(25) 指定对其方式^、&lt;、&gt;分别表示居中、左对齐、右对齐。后面带宽度，默认用’ ‘填充，可指定填充方式。 12&gt;&gt;&gt; print('&#123;:-&lt;15&#125; | &#123;:*^9&#125; | &#123;:&gt;9&#125;'.format('', 'lat', 'long'))--------------- | ***lat*** | long 注：填充必须和对其方式结合使用 2.3 定义对象属性1234567891011&gt;&gt;&gt; class Person:... def __init__(self, name, age):... self.name = name... self.age = age... def __str__(self):... return '&#123;self.name&#125; is &#123;self.age&#125;'.format(self=self)...&gt;&gt;&gt; Person('gao', 25)&lt;__main__.Person object at 0x000002708A0C24E0&gt;&gt;&gt;&gt; str(Person('gao', 25))'gao is 25']]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django学习笔记 - 4 API与后台管理]]></title>
    <url>%2F2018%2F06%2F17%2FDjango%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-API%E4%B8%8E%E5%90%8E%E5%8F%B0%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[1. API 1.1 初试API现在让我们进入交互式 Python 命令行，尝试一下 Django 为你创建的各种 API。通过以下命令打开 Python 命令行： 1python manage.py shell 我们使用这个命令而不是简单的使用 “Python” 是因为 manage.py 会设置 DJANGO_SETTINGS_MODULE 环境变量，这个变量会让 Django 根据 mysite/settings.py文件来设置 Python 包的导入路径。 1234567891011121314151617181920212223242526272829# 导入我们刚写的模型类&gt;&gt;&gt; from polls.models import Choice, Question# 系统中目前没有问题&gt;&gt;&gt; Question.objects.all()&lt;QuerySet []&gt;# 创建一个问题，就是通过类来创建对象# 如果你想设置成中国时区，在settings.py文件中把USE_TZ设置为False，TIME_ZONE设置为'Asia/Shanghai'&gt;&gt;&gt; from django.utils import timezone&gt;&gt;&gt; q = Question(question_text="What's new?", pub_date=timezone.now())# 将对象保存在数据库中，save()方法用于创建和更新对象。&gt;&gt;&gt; q.save()# 第一次保存对象后，Django会为Question实例创建一个ID&gt;&gt;&gt; q.id1# 我们还可以通过python属性来访问其他的模型字段值&gt;&gt;&gt; q.question_text"What's new?"&gt;&gt;&gt; q.pub_datedatetime.datetime(2018, 6, 11, 14, 33, 13, 469975)# 通过更改属性来更改字段值，并使用save()来更新&gt;&gt;&gt; q.question_text = "What's up?"&gt;&gt;&gt; q.save()&gt;&gt;&gt; Question.objects.all()&lt;QuerySet [&lt;Question: Question object (1)&gt;]&gt; 等等。\&lt;Question: Question object (1)> 对于我们了解这个对象的细节没什么帮助。让我们通过编辑 Question 模型的代码（位于 polls/models.py 中）来修复这个问题。给 Question 和 Choice 增加 str() 方法。 __str__方法是对象的字符串表示形式，我们可以使用描述属性来表示它。 12345678910111213# 创建一个问题类型class Question(models.Model): ... def __str__(self): return self.question_text # 创建一个解决方案类型class Choice(models.Model): ... def __str__(self): return self.choice_text 保存文件后再次打开python交互式命令行： 12345678910111213141516171819202122&gt;&gt;&gt; from polls.models import Choice, Question&gt;&gt;&gt; Question.objects.all()&lt;QuerySet [&lt;Question: What's up?&gt;]&gt; # Django提供了由关键字参数来查找数API&gt;&gt;&gt; Question.objects.filter(id=1)&lt;QuerySet [&lt;Question: What's up?&gt;]&gt;&gt;&gt;&gt; Question.objects.filter(question_text__startswith='What')&lt;QuerySet [&lt;Question: What's up?&gt;]&gt;# 获取今年发布的问题&gt;&gt;&gt; Question.objects.filter(question_text__startswith='What')&lt;QuerySet [&lt;Question: What's up?&gt;]&gt; # 当你请求一个不存在的ID时，会引发异常Traceback (most recent call last): ...polls.models.DoesNotExist: Question matching query does not exist.# 通过主键查找是最常见的情况，Django提供了主键精准查找的方法&gt;&gt;&gt; Question.objects.get(pk=1)&lt;Question: What's up?&gt; 我们给这个问题提供几个选项，调用create()来创建一个新的Choice对象。 12345678910111213141516171819202122232425262728# 通过字段标识获取单个对象&gt;&gt;&gt; q = Question.objects.get(pk=1)# 目前这个问题还没有选项&gt;&gt;&gt; q.choice_set.all()&lt;QuerySet []&gt;# 创建3个选项&gt;&gt;&gt; q.choice_set.create(choice_text='Not much', votes=0)&gt;&gt;&gt; q.choice_set.create(choice_text='The sky', votes=0)&gt;&gt;&gt; c = q.choice_set.create(choice_text='Just hacking again', votes=0)# Choice对象有和它相关联的Question对象的访问权限&gt;&gt;&gt; c.question&lt;Question: What's up?&gt;# 反之亦然，Question对象也可以访问Choice对象&gt;&gt;&gt; q.choice_set.all()&lt;QuerySet [&lt;Choice: Not much&gt;, &lt;Choice: The sky&gt;, &lt;Choice: Just hacking again&gt;]&gt;# 查找今年发布问题的所有选项，使用双下划线来分隔关系。# 我们会重用上面使用的current_year变量&gt;&gt;&gt; Choice.objects.filter(question__pub_date__year=current_year)&lt;QuerySet [&lt;Choice: Not much&gt;, &lt;Choice: The sky&gt;, &lt;Choice: Just hacking again&gt;]&gt;# 使用delete()删除其中一个选项。&gt;&gt;&gt; c = q.choice_set.filter(choice_text__startswith='Just hacking')&gt;&gt;&gt; c.delete() 1.2 数据库API整理1.2.1 创建对象123456789# 方法一&gt;&gt;&gt; Question.objects.create(question_text="What's up?", pub_date=timezone.now())# 方法二&gt;&gt;&gt; q = Question(question_text="What'up？", pub_date=timezone.now())&gt;&gt;&gt; q.save()# 方法三，先尝试获取，不存在就创建（返回True），存在时不创建(返回False)。&gt;&gt;&gt; Question.objects.get_or_create(question_text="What's up?", pub_date=timezone.now()) 当你创建有关联的对象时(ForeignKey、OneToOneField、ManyToManyField），我们需要先获取与之相关联的对象。我们这样来创建： 123456789101112# 方法一：&gt;&gt;&gt; q = Question.objects.get(pk=1)&gt;&gt;&gt; Choice.objects.create(question=q, Choice_text="Not much", votes=0)# 方法二：&gt;&gt;&gt; q = Question.objects.get(pk=1)&gt;&gt;&gt; c = Choice(question=q, Choice_text="Not much", votes=0)&gt;&gt;&gt; c.save()# 方法三：&gt;&gt;&gt; q = Question.objects.get(pk=1)&gt;&gt;&gt; Choice.objects.get_or_create(question=q, Choice_text="Not much", votes=0) Django 提供了一个更快捷的API来创建对象： 1234567# 方法一&gt;&gt;&gt; q = Question.objects.get(pk=1)&gt;&gt;&gt; q.choice_set.create(choice_text='Not much', votes=0)# 方法二,先尝试获取，不存在就创建（返回True），存在时不创建(返回False)。&gt;&gt;&gt; q = Question.objects.get(pk=1)&gt;&gt;&gt; q.choice_set.get_or_create(choice_text='Not much', votes=0 1.2.2 更改对象我们使用save()来保存已经存在于数据库中的对象的更改： 123&gt;&gt;&gt; q = Question.objects.get(pk=1)&gt;&gt;&gt; q.question_text = "What's new?"&gt;&gt;&gt; q.save 使用update()批量更改对象： 12# 将所有2018年6月12号发布的问题描述全部改为"What"&gt;&gt;&gt; Question.objects.filter(pub_date__year=2018, pub_date__month=6, pub_date__day=12).update(question_text='What') 批量更新对象不需要执行save()就已经保存至数据库，需谨慎操作。 1.2.3 获取对象1234567891011121314151617181920# 获取所有对象&gt;&gt;&gt; Question.objects.all()# 获取单个对象，pk代表主键，主键的字段是ID，所以下面2个语句是等价的&gt;&gt;&gt; Question.objects.get(pk=1)&gt;&gt;&gt; Question.objects.get(id=1)# 使用filter过滤器获取特定对象# filter(**kwargs)：返回匹配给定的查找参数的对象# 获取id等于1的问题&gt;&gt;&gt; Question.objects.filter(id=1)# 获取发布时间为2018年的问题&gt;&gt;&gt; Question.objects.filter(pub_date__year=2018)# 使用exclude过滤器获取特定对象# exclude(**kwargs)：返回与给定的查找参数不匹配的对象# 获取id不等于1之外的所有问题&gt;&gt;&gt; Question.objects.exclude(id=1)# 获取除2018年之外所有发布的问题&gt;&gt;&gt; Question.objects.exclude(pub_date__year=2018) 当你获取与其他对象有关联的对象时，我们还可以跨越关系查找。只需使用模型中相关字段的字段名称用双下划线分隔即可： 123456# 获取问题ID为6的所有选项，再次强调pk为主键，id是主键字段&gt;&gt;&gt; Choice.objects.filter(question__id=1)&gt;&gt;&gt; Choice.objects.filter(question__pk=1)# 反向查找，获取选项开头为"not"(不区分大小写)的所有问题&gt;&gt;&gt; Question.objects.filter(choice__choice_text__istartswith='not') 链接过滤器： 12345678# 获取同时满足发布时间为2018、内容开头为“What”并且不是已“?”结尾的的问题&gt;&gt;&gt; Question.objects.filter(... question_text__startswith='What'... ).filter(... pub_date__year=2018... ).exclude(... question_text__endswith='?'... ) QuerySet（查询集）是惰性执行的，创建查询集不会带来任何数据库的访问，直到查询集写完后需要求值时，才会去访问数据库。例如： 12345&gt;&gt;&gt; q = Question.objects.filter(question_text__startswith='What')&gt;&gt;&gt; q.filter(pub_date__year=2018)&gt;&gt;&gt; q.exclude(question_text__endswith='?')# 直到你需要获取值时，才会去访问数据库&gt;&gt;&gt; print(q) 常用筛选条件： 大于、大于等于：__gt ， __gte 小于、小于等于：__lt， __lte 精准匹配：__exact， __iexact 包含：__contains（不忽略大小写）, __icontains（忽略大小写） 以…开头：__startwith， __istartwith（匹配字符加i忽略大小写基本适用，之后不在描述） 以…结尾：__endswith，__iendswith 在…中：__in 在…范围（使用元组）：__range 是否为空值： __isnull=True，__isnull=False 日期类型： 年：__year 月：__month 日：__day 小时:__hour 分钟:__minut 秒：__second 1.2.4 删除对象我们使用delete()来删除对象： 12345678# 删除id等于1的问题&gt;&gt;&gt; Question.bojects.filter(id=1).delete()# 等同于&gt;&gt;&gt; q = Question.bojects.filter(id=1)&gt;&gt;&gt; q.delete()# 删除所有对象&gt;&gt;&gt; Question.objects.all().delete() 1.2.5 排序对象12345# 根据时间排序&gt;&gt;&gt; Question.objects.all().order_by('pub_date')# 参数前面加-号可以实现倒序&gt;&gt;&gt; Question.objects.all().order_by('-pub_date') 1.2.6 使用索引查询集不支持负索引，想获取最后几个值需要通过其他的方法，例如reverse()或者通过参数前面加-号的方式： 123456789101112# 获取所有问题的前5条&gt;&gt;&gt; Question.objects.all()[:5]# 使用负索引会报错&gt;&gt;&gt; Question.objects.all()[-5:]...AssertionError: Negative indexing is not supported.# 获取最后3个值&gt;&gt;&gt; Question.objects.all().order_by('id').reverse()[:3]# 等同于&gt;&gt;&gt; Question.objects.all().order_by('-id')[:3] 1.2.7 去重对象123# 去除重复的对象&gt;&gt;&gt; q = Question.objects.all()&gt;&gt;&gt; q.distinct() 1.2.8 获取元组形式的对象我们使用values_list()来获取元组形式的对象： 1234567891011# 以元组形式获取问题的描述与发布时间&gt;&gt;&gt; Question.objects.values_list('question_text', 'pub_date')&lt;QuerySet [('What', datetime.datetime(2018, 6, 11, 17, 46)), ('What', datetime.datetime(2018, 6, 11, 18, 47))]&gt;# 如果只需要一个字段，可以指定flat=True&gt;&gt;&gt; Question.objects.values_list('pub_date', flat=True)&lt;QuerySet ['What', 'What', 'What', 'What', 'What', 'What', 'What']&gt;# 转化为列表&gt;&gt;&gt; list(Question.objects.values_list('question_text', flat=True))['What', 'What', 'What', 'What', 'What', 'What', 'What'] 1.2.9 获取字典形式的对象我们使用values()来获取字典形式的对象： 1234567# 以字典形式获取问题的描述与发布时间&gt;&gt;&gt; Question.objects.values('question_text', 'pub_date')&lt;QuerySet [&#123;'question_text': 'What', 'pub_date': datetime.datetime(2018, 6, 11, 17, 46)&#125;]&gt;# 转化为列表&gt;&gt;&gt; list(Question.objects.values('question_text', 'pub_date'))[&#123;'question_text': 'What', 'pub_date': datetime.datetime(2018, 6, 11, 17, 46)&#125;] 2. Django管理页面 为你的员工或客户生成一个用户添加，修改和删除内容的后台是一项缺乏创造性和乏味的工作。因此，Django 全自动地根据模型创建后台界面。 Django 产生于一个公众页面和内容发布者页面完全分离的新闻类站点的开发过程中。站点管理人员使用管理系统来添加新闻、事件和体育时讯等，这些添加的内容被显示在公众页面上。Django 通过为站点管理人员创建统一的内容编辑界面解决了这个问题。 管理界面不是为了网站的访问者，而是为管理者准备的。 2.1 创建一个管理员账号123456789python manage.py createsuperuser# 输入管理用户名Username: admin# 输入管理员邮箱，可不填Email address:# 输入管理员密码Password:Password (again): 2.2 启动开发服务器Django 的管理界面默认就是启用的，它是由django.contrib.admin提供的，不需要添加应用。 直接启动开发服务器，看看它是什么样的。 1python manage.py runserver 现在，打开浏览器，转到你本地域名的 “/admin/“ 目录， 比如 “http://127.0.0.1:8000/admin/“ 。你应该会看见管理员登录界面： 如果你想使用中文，将settings.py中LANGUAGE_CODE设置为’zh-hans’。 123456789LANGUAGE_CODE = 'zh-hans'TIME_ZONE = 'Asia/Shanghai'USE_I18N = TrueUSE_L10N = TrueUSE_TZ = False 2.3 进入管理站点页面现在，试着使用你在上一步中创建的超级用户来登录。然后你将会看到 Django 管理页面的索引页： 你将会看到几种可编辑的内容：组和用户。它们是由 django.contrib.auth 提供的，这是 Django 开发的认证框架。 2.4 向管理页面中加入投票应用但是我们的投票应用在哪呢？它没在索引页面里显示。 只需要做一件事：我们得告诉管理页面，问题 Question 对象需要被管理。打开 polls/admin.py 文件，把它编辑成下面这样： 1234567from django.contrib import adminfrom .models import Choice, Question#注册模块应用admin.site.register(Question)admin.site.register(Choice) 现在我们向管理页面注册了问题 Question类和Choice类，接下来在后台管理页面上来添加问题和选项：]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django学习笔记 - 3 数据库配置与模型]]></title>
    <url>%2F2018%2F06%2F17%2FDjango%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-%E6%95%B0%E6%8D%AE%E5%BA%93%E9%85%8D%E7%BD%AE%E4%B8%8E%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[现在起，我们将建立数据库，并创建你的第一个模型。 1. 数据库配置 现在，打开 mysite/settings.py 。这是个包含了 Django 项目设置的 Python 模块。 通常，这个配置文件使用 SQLite 作为默认数据库。如果你不熟悉数据库，或者只是想尝试下 Django，这是最简单的选择。Python 内置 SQLite，所以你无需安装额外东西来使用它。当你开始一个真正的项目时，你可能更倾向使用一个更具扩展性的数据库，例如 MySQL，避免中途切换数据库这个令人头疼的问题。 如果你想使用其他数据库，你需要安装合适的 database bindings ，然后改变设置文件中 DATABASES &#39;default&#39; 项目中的一些键值： ENGINE – 数据库引擎配置 ，可选值有 ‘django.db.backends.sqlite3’，’django.db.backends.postgresql’，’django.db.backends.mysql’，或 ‘django.db.backends.oracle’。其它 可用后端。 NAME - 数据库的名称。如果使用的是 SQLite，数据库将是你电脑上的一个文件，在这种情况下， NAME 应该是此文件的绝对路径，包括文件名。默认值 os.path.join(BASE_DIR,’db.sqlite3’) 将会把数据库文件储存在项目的根目录。 如果你不使用 SQLite，则必须添加一些额外设置，比如 USER 、 PASSWORD 、 HOST 等等。想了解更多数据库设置方面的内容，请看文档：DATABASES 。 SQLite 以外的其它数据库 如果你使用了 SQLite 以外的数据库，请确认在使用前已经创建了数据库。你可以通过在你的数据库交互式命令行中使用 “CREATE DATABASE database_name;“ 命令来完成这件事。 另外，还要确保该数据库用户中提供 mysite/settings.py 具有 “create database” 权限。这使得自动创建的 test database 能被以后的教程使用。 如果你使用 SQLite，那么你不需要在使用前做任何事——数据库会在需要的时候自动创建。 本次学习中，我们将使用MySQL数据库来完成后续的操作。 1.1 安装MySQL数据库并配置首先在本地安装mysql数据库，本次是在winodws环境下安装的8.0版本。 安装完成后，为使用方便使用我设置了环境变量： MYSQLHOME D:\MySQL\MySQL Server 8.0 并在环境变量path中添加了 %MYSQL_HOME%\bin 前面我们提到，如果使用SQLite以外其他数据库，必须在使用前已经创建了数据库，并具有一定的权限。所以我们需要进入数据库并创建我们使用的数据库及用户，并赋予用户一些权限。 12345mysql -hlocalhost -uroot -p123456create database pydb character set utf8;create user py_user@&apos;%&apos; identified by &apos;123456&apos;;grant all privileges on pydb.* to &apos;py_user&apos;@&apos;%&apos;;flush privileges; 到这，我们mysql数据库环境已经准备完毕。 1.2 安装mysqlclient驱动并配置前面我们还提到，如果你想使用其他数据库，必须安装合适的 database bindings。所以你需要一个DB API Drivers，即数据库接口驱动，常见的有： mysqlclient - 是一个原生驱动。目前支持Python 2.7,3.4+ ，这是官网推荐的驱动。 MySQL Connector/Python - Oracle官网提供的纯驱动程序，它不需要MySQL客户端库或标准库之外的任何Python模块。 mysqldb - 是用于Python 的流行的MySQL数据库服务器的接口，目前支持Python-2.4到2.7 ，并不支持python3.6。 pymysql - 目标是成为MySQLdb的替代品，支持Python 2.7,3.4+ 。 除了DB API Driver之外，Django还需要一个适配器才能从其ORM访问数据库驱动程序。Django为mysqlclient提供了一个适配器，而MySQL Connector / Python包含他自己的适配器。 我们使用官网推荐的驱动mysqlclient。 1pip install mysqlclient 安装完成后，进行配置来链接数据库。打开mysite/mysite/settings.py配置文件，添加数据库连接配置信息： 1234567891011121314151617# 修改DATABASE配置DATABASES = &#123; 'default': &#123; # 数据库连接字符 'ENGINE': 'django.db.backends.mysql', # 数据库名称 'NAME': 'pydb', # 数据库登录用户 'USER': 'py_user', # 数据库登录密码 'PASSWORD': '123456', # 数据库所在主机IP，本地可以为空 'HOST': '', # 数据库连接端口 'PORT': '3306' &#125;&#125; 到这我们项目就可以连接到mysql数据库了。 使用其他的驱动： 1 MySQL Connector/Python 12&gt; pip install mysql-connector-python&gt; 123456789101112131415161718&gt; # 修改DATABASE配置&gt; DATABASES = &#123;&gt; 'default': &#123;&gt; # 数据库连接字符&gt; 'ENGINE': 'mysql.connector.django',&gt; # 数据库名称&gt; 'NAME': 'pydb',&gt; # 数据库登录用户&gt; 'USER': 'py_user',&gt; # 数据库登录密码&gt; 'PASSWORD': '123456',&gt; # 数据库所在主机IP，本地可以为空&gt; 'HOST': '',&gt; # 数据库连接端口&gt; 'PORT': '3306'&gt; &#125;&gt; &#125;&gt; 注意ENGINE发生了改变，因为mysql.connector.django为MySQL自己提供的Django后端模块。 2 pymysql 12&gt; pip install pimysql&gt; 1234567891011121314151617181920212223&gt; # 引入pymysql模块&gt; import pymysql&gt; # 指定按照mysqldb的方式使用&gt; pymysql.install_as_MySQLdb()&gt; &gt; # 修改DATABASE配置&gt; DATABASES = &#123;&gt; 'default': &#123;&gt; # 数据库连接字符&gt; 'ENGINE': 'django.db.backends.mysql',&gt; # 数据库名称&gt; 'NAME': 'pydb',&gt; # 数据库登录用户&gt; 'USER': 'py_user',&gt; # 数据库登录密码&gt; 'PASSWORD': '123456',&gt; # 数据库所在主机IP，本地可以为空&gt; 'HOST': '',&gt; # 数据库连接端口&gt; 'PORT': '3306'&gt; &#125;&gt; &#125;&gt; 2. 创建模型并和数据库交互 2.1 创建模型在 Django 里写一个数据库驱动的 Web 应用的第一步是定义模型 - 也就是数据库结构设计和附加的其它元数据。 设计哲学 模型是真实数据的简单明确的描述。它包含了储存的数据所必要的字段和行为。Django 遵循 DRY Principle 。它的目标是你只需要定义数据模型，然后其它的杂七杂八代码你都不用关心，它们会自动从模型生成。 来介绍一下迁移 - 举个例子，不像 Ruby On Rails，Django 的迁移代码是由你的模型文件自动生成的，它本质上只是个历史记录，Django 可以用它来进行数据库的滚动更新，通过这种方式使其能够和当前的模型匹配。 在这个简单的投票应用中，需要创建两个模型：问题 Question 和选项 Choice。Question模型有两个字段，问题描述和发布时间。Choice 模型有两个字段，选项描述和当前得票数。每个选项属于一个问题。 我们通过类图来表现这两个模型之间的关系： 我们可以这样来解读它们：一个choice必须与一个（1）question相关联，但是question下面可能与多个（0..*）choice关联。也就是说，一个选项必须属于一个问题，没有问题就没有选项，但是一个问题可能有多个选项，也可能没有选项。（多对一关系） 他们是怎么关联的？通过外键来关联，之后我们会详细讲到。 我们要做的就是来创建Django所表示的类。按照下面的例子来编辑 polls/models.py文件： 1234567891011121314151617181920# 引入django.db模块中的models模块from django.db import models# 创建一个问题类型class Question(models.Model): # 初始化问题描述属性，指定为字符串类型，最大长度200个字符 question_text = models.CharField(max_length=200) # 初始化发布时间属性，指定为日期时间类型 pub_date = models.DateTimeField('date published')# 创建一个解决方案类型class Choice(models.Model): # 将解决方案和问题关联起来，通过外键的形式 question = models.ForeignKey(Question, on_delete=models.CASCADE) # 解决方案的描述信息，字符串，最大长度 choice_text = models.CharField(max_length=200) # 解决方案的投票总数，整数类型，默认0 votes = models.IntegerField(default=0) 每个模型都是django.db.models.Model类的子类。每个类将被转换为数据库表，每个模型有一些类变量，它们都表示模型里的一个数据库字段。 每个字段都是 Field 类的实例 - 比如，字符字段被表示为 CharField ，日期时间字段被表示为 DateTimeField 。这将告诉 Django 每个字段要处理的数据类型。 每个 Field 类实例变量的名字（例如 question_text 或 pub_date ）也是字段名，所以最好使用对机器友好的格式。你将会在 Python 代码里使用它们，而数据库会将它们作为列名。 每个 Field 类实例变量的名字（例如 question_text 或 pub_date ）也是字段名，所以最好使用对机器友好的格式。你将会在 Python 代码里使用它们，而数据库会将它们作为列名。 你可以使用可选的选项来为 Field 定义一个人类可读的名字。这个功能在很多 Django 内部组成部分中都被使用了，而且作为文档的一部分。如果某个字段没有提供此名称，Django 将会使用对机器友好的名称，也就是变量名。在上面的例子中，我们只为 Question.pub_date 定义了对人类友好的名字。对于模型内的其它字段，它们的机器友好名也会被作为人类友好名使用。 定义某些 Field 类实例需要参数。例如 CharField 需要一个 max_length 参数。这个参数的用处不止于用来定义数据库结构，也用于验证数据。 Field 也能够接收多个可选参数；在上面的例子中：我们将 votes 的 default 也就是默认值，设为0。 我们使用 ForeignKey 定义了模型之间的关系。question字段是Question模型的ForeignKey，这将告诉 Django，每个 Choice 对象都关联到一个 Question 对象，并在数据库级别创建适当的关系，也就是外键关联。在创建多对一关系时，需要在ForeignKey的第二参数中加入on_delete=models.CASCADE，表示主外关系键中级联删除，也就是说当删除主表的数据时从表中的数据也一并删除。 2.2 激活模型上面的一小段用于创建模型的代码给了 Django 很多信息，通过这些信息，Django 可以： 为这个应用创建数据库 schema（生成 CREATE TABLE 语句）。 创建可以与 Question 和 Choice 对象进行交互的 Python 数据库 API。 但是首先得把 polls 应用安装到我们的项目里。 设计哲学 Django 应用是“可插拔”的。你可以在多个项目中使用同一个应用。除此之外，你还可以发布自己的应用，因为它们并不会被绑定到当前安装的 Django 上。 为了在我们的工程中包含这个应用，我们需要在配置类 INSTALLED_APPS 中添加设置。因为 PollsConfig 类写在文件 polls/apps.py 中，所以它的点式路径是 ‘polls.apps.PollsConfig’。在文件 mysite/settings.py 中 INSTALLED_APPS 子项添加点式路径后，它看起来像这样： 12345678910111213141516INSTALLED_APPS = [ # 投票模块应用,即我们需要添加的应用 'polls.apps.PollsConfig', # 默认提供的后台管理网站 'django.contrib.admin', # 权限认证模块 'django.contrib.auth', # 内容编码框架模块 'django.contrib.contenttypes', # session会话管理框架模块 'django.contrib.sessions', # 消息队列处理模块 'django.contrib.messages', # 项目静态文件管理模块 'django.contrib.staticfiles',] 现在你的 Django 项目会包含 polls 应用。接着运行下面的命令： 1python manage.py makemigrations polls 你将会看到类似于下面这样的输出： 12345Migrations for 'polls': polls/migrations/0001_initial.py: - Create model Choice - Create model Question - Add field question to choice 通过运行 makemigrations 命令，Django 会检测你对模型文件的修改（在这种情况下，你已经取得了新的），并且把修改的部分储存为一次 迁移。 迁移是 Django 对于模型定义（也就是你的数据库结构）的变化的储存形式 - 没那么玄乎，它们其实也只是一些你磁盘上的文件。如果你想的话，你可以阅读一下你模型的迁移数据，它被储存在 polls/migrations/0001_initial.py 里。别担心，你不需要每次都阅读迁移文件，但是它们被设计成人类可读的形式，这是为了便于你手动修改它们。 Django 有一个自动执行数据库迁移并同步管理你的数据库结构的命令 - 这个命令是 migrate，我们马上就会接触它 - 但是首先，让我们看看迁移命令会执行哪些 SQL 语句。sqlmigrate 命令接收一个迁移的名称，然后返回对应的 SQL： 1python manage.py sqlmigrate polls 0001 你将会看到类似下面这样的输出 ： 123456789101112131415BEGIN;---- Create model Choice--CREATE TABLE `polls_choice` (`id` integer AUTO_INCREMENT NOT NULL PRIMARY KEY, `choice_text` varchar(200) NOT NULL, `votes` integer NOT NULL);---- Create model Question--CREATE TABLE `polls_question` (`id` integer AUTO_INCREMENT NOT NULL PRIMARY KEY, `question_text` varchar(200) NOT NULL, `pub_date` datetime(6) NOT NULL);---- Add field question to choice--ALTER TABLE `polls_choice` ADD COLUMN `question_id` integer NOT NULL;ALTER TABLE `polls_choice` ADD CONSTRAINT `polls_choice_question_id_c5b4b260_fk_polls_question_id` FOREIGN KEY (`question_id`) REFERENCES `polls_question` (`id`);COMMIT; 请注意以下几点： 输出的内容和你使用的数据库有关，上面的输出示例使用的是 MySQL。 数据库的表名是由应用名(polls)和模型名的小写形式( question 和 choice)连接而来（可自定义）。 主键(IDs)会被自动创建（可自定义）。 默认的，Django 会在外键字段名后追加字符串 “_id” 。（可自定义） 外键关系由 FOREIGN KEY 生成。 生成的 SQL 语句是为你所用的数据库定制的，所以那些和数据库有关的字段类型，比如 auto_increment (MySQL)、 serial (PostgreSQL)和 integer primary keyautoincrement (SQLite)，Django 会帮你自动处理。那些和引号相关的事情 - 例如，是使用单引号还是双引号 - 也一样会被自动处理。 这个 sqlmigrate 命令并没有真正在你的数据库中的执行迁移 - 它只是把命令输出到屏幕上，让你看看 Django 认为需要执行哪些 SQL 语句。这在你想看看 Django 到底准备做什么，或者当你是数据库管理员，需要写脚本来批量处理数据库时会很有用。 如果你感兴趣，你也可以试试运行 python manage.py check ;这个命令帮助你检查项目中的问题，并且在检查过程中不会对数据库进行任何操作。 现在，再次运行 migrate 命令，在数据库里创建新定义的模型的数据表： 1python manage.py migrate 123456789101112131415161718Operations to perform: Apply all migrations: admin, auth, contenttypes, polls, sessionsRunning migrations: Applying contenttypes.0001_initial... OK Applying auth.0001_initial... OK Applying admin.0001_initial... OK Applying admin.0002_logentry_remove_auto_add... OK Applying contenttypes.0002_remove_content_type_name... OK Applying auth.0002_alter_permission_name_max_length... OK Applying auth.0003_alter_user_email_max_length... OK Applying auth.0004_alter_user_username_opts... OK Applying auth.0005_alter_user_last_login_null... OK Applying auth.0006_require_contenttypes_0002... OK Applying auth.0007_alter_validators_add_error_messages... OK Applying auth.0008_alter_user_username_max_length... OK Applying auth.0009_alter_user_last_name_max_length... OK Applying polls.0001_initial... OK Applying sessions.0001_initial... OK 因为是一次执行migrate命令，所以把自带的默认应用也都迁移了。 这个 migrate 命令选中所有还没有执行过的迁移（Django 通过在数据库中创建一个特殊的表 django_migrations 来跟踪执行过哪些迁移）并应用在数据库上 - 也就是将你对模型的更改同步到数据库结构上。 迁移是非常强大的功能，它能让你在开发过程中持续的改变数据库结构而不需要重新删除和创建表 - 它专注于使数据库平滑升级而不会丢失数据。我们会在后面的教程中更加深入的学习这部分内容，现在，你只需要记住，改变模型需要这三步： 编辑 models.py 文件，改变模型。 运行 python manage.py makemigrations 为模型的改变生成迁移文件。 运行 python manage.py migrate 来应用数据库迁移。 数据库迁移被分解成生成和应用两个命令是为了让你能够在代码控制系统上提交迁移数据并使其能在多个应用里使用；这不仅仅会让开发更加简单，也给别的开发者和生产环境中的使用带来方便。]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django学习笔记 - 2 项目与模块]]></title>
    <url>%2F2018%2F06%2F17%2FDjango%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-%E9%A1%B9%E7%9B%AE%E4%B8%8E%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[通过这个教程，我们将带着你创建一个基本的投票应用程序。 它将由两部分组成： 一个让人们查看和投票的公共站点。 一个让你能添加、修改和删除投票的管理站点。 1. 项目 1.1 Django创建项目的结构在使用django开始创建并开发项目之前，我们先简单了解一下Django对于项目的管理结构。 Django之所以可以方便快捷的开发大型WEB应用，最大的特点是它面向对象开发的基础上的模块化开发，将项目中需要的各个功能可以封装成或大或小的模块，这些模块在项目中是可插拔时的，非常有利于项目的更新和扩展 。 所以，Django框架在使用时，首先常见一个项目，然后在项目的基础上，创建各个应用的模块，将模块引入到我们的项目中进行使用。 1.2 Django创建项目首先，我们需要确认要开发Django项目的文件夹，并切换到此目录下 1cd vote_poll\ 然后启动一个新项目，执行下面的命令来创建一个新的 Django 项目： 1django-admin.exe startproject mysite 命令行工具django-admin会在安装Django的时候一起自动安装好。 执行了上面的命令以后，系统会为Django项目生成基础文件夹结构。 现在，我们的vote_poll目录结构如下所示： 123456789vote_poll/ &lt;-- 高级别的文件夹 |-- mysite/ &lt;-- Django项目文件夹 | |-- mysite/ &lt;-- 根模块 | | |-- __init__.py | | |-- settings.py | | |-- urls.py | | |-- wsgi.py | +-- manage.py +-- vote_poll_venv/ &lt;-- 虚拟环境文件夹 这些目录和文件的用处是： 最外层的:file:vote_poll/ 根目录只是你项目的容器， Django 不关心它的名字，你可以将它重命名为任何你喜欢的名字。 manage.py : 一个让你用各种方式管理 Django 项目的命令行工具。 里面一层的 mysite/ 目录包含你的项目，它是一个纯 Python 包。它的名字就是当你引用它内部任何东西时需要用到的 Python 包名。 (比如 mysite.urls)。 __init.py：一个空文件，这个空文件告诉python这个文件夹是一个python包。 settings.py：这个文件包含了所有的项目配置。 urls.py：Django 项目的 URL 声明，就像你网站的“目录”。 这个文件负责映射我们项目中的路由和路径。 wsgi.py：该文件是用于部署的简单网关接口，作为你的项目的运行在 WSGI 兼容的Web服务器上的入口。 1.3 运行及访问Django项目django自带了一个简单的网络服务器。在开发过程中非常方便，所以我们无需安装任何其他软件即可在本地运行项目。我们可以通过执行命令来测试一下它： 1python manage.py runserver 更换端口 默认情况下，runserver 命令会将服务器设置为监听本机内部 IP 的 8000 端口。 如果你想更换服务器的监听端口，请使用命令行参数。举个例子，下面的命令会使服务器监听 8080 端口： 12&gt; $ python manage.py runserver 8080&gt; 如果你想要修改服务器监听的IP，在端口之前输入新的。比如，为了监听所有服务器的公开IP（这你运行 Vagrant 或想要向网络上的其它电脑展示你的成果时很有用），使用： 12&gt; $ python manage.py runserver 0:8000&gt; 会自动重新加载的服务器 runserver 用于开发的服务器在需要的情况下会对每一次的访问请求重新载入一遍 Python 代码。所以你不需要为了让修改的代码生效而频繁的重新启动服务器。然而，一些动作，比如添加新文件，将不会触发自动重新加载，这时你得自己手动重启服务器。 2. 模块应用 2.1 创建模块应用现在你的开发环境——这个“项目” ——已经配置好了，你可以开始干活了。 在 Django 中，每一个应用都是一个 Python 包，并且遵循着相同的约定。Django 自带一个工具，可以帮你生成应用的基础目录结构，这样你就能专心写代码，而不是创建目录了。 项目 VS 应用： 项目和应用有啥区别？应用是一个专门做某件事的网络应用程序——比如博客系统，或者公共记录的数据库，或者简单的投票程序。项目则是一个网站使用的配置和应用的集合。项目可以包含很多个应用。应用可以被很多个项目使用。 你的应用可以存放在任何 Python path中定义的路径。在这个教程中，我们将在你的 manage.py 同级目录下创建投票应用。这样它就可以作为顶级模块导入，而不是 mysite 的子模块。 请确定你现在处于 manage.py 所在的目录下，然后运行这行命令来创建一个应用： 1python manage.py startapp polls 这将会创建一个 polls 目录，它的目录结构大致如下： 123456789polls/ __init__.py admin.py apps.py migrations/ __init__.py models.py tests.py views.py 这个目录结构包括了投票应用的全部内容。 2.2 创建模块应用的第一个视图模块中的视图对应的是views.py文件，打开polls/views.py文件，创建如下方法： 1234567# 引入需要的模块from django.http import HttpResponse# 定义一个处理方法def index(request): return HttpResponse("hello, world. you're at the polls index.") 视图创建完成后，如果想看见效果，我们需要一个URL映射到它，这就是我们需要URLconf的原因。 为了创建URLconfg，在polls目录里新建一个urls.py文件。 现在你的应用目录看起来应该是： 12345678910polls/ __init__.py admin.py apps.py migrations/ __init__.py models.py tests.py urls.py views.py 在polls/urls.pu中，输入如下代码： 1234567from django.urls import pathfrom . import viewsurlpatterns = [ path('', views.index, name='index'),] 下一步是要在根URLconf文件中指定我们创建的polls.urls模块，在mysite/urls.py文件的urlpatterns列表里插入一个include(),如下： 12345678from django.contrib import adminfrom django.urls import include, pathurlpatterns = [ path('admin/', admin.site.urls), path('polls/', include('polls.urls'))] 函数 include()允许引用其它 URLconfs。每当 Django 遇到 :func：~django.urls.include 时，它会截断与此项匹配的 URL 的部分，并将剩余的字符串发送到 URLconf 以供进一步处理。 我们设计 include() 的理念是使其可以即插即用。因为投票应用有它自己的 URLconf( polls/urls.py )，他们能够被放在 “/polls/“ ， “/fun_polls/“ ，”/content/polls/“，或者其他任何路径下，这个应用都能够正常工作。 何时使用 include()： 当包括其它 URL 模式时你应该总是使用 include() ， admin.site.urls 是唯一例外。 你现在把 index 视图添加进了 URLconf。可以验证是否正常工作，运行下面的命令: 1python manage.py runserver 用你的浏览器访问 http://localhost:8000/polls/，你应该能够看见 “Hello, world. You’re at the polls index.“ ，这是你在 index视图中定义的。 函数 path() 具有四个参数，两个必须参数：route 和 view，两个可选参数：kwargs 和 name。现在，是时候来研究这些参数的含义了。 path() 参数： route route 是一个匹配 URL 的准则（类似正则表达式）。当 Django 响应一个请求时，它会从 urlpatterns 的第一项开始，按顺序依次匹配列表中的项，直到找到匹配的项。 这些准则不会匹配 GET 和 POST 参数或域名。例如，URLconf 在处理请求 https://www.example.com/myapp/ 时，它会尝试匹配 myapp/ 。处理请求 https://www.example.com/myapp/?page=3 时，也只会尝试匹配 myapp/。 path() 参数： view 当 Django 找到了一个匹配的准则，就会调用这个特定的视图函数，并传入一个 HttpRequest对象作为第一个参数，被“捕获”的参数以关键字参数的形式传入。稍后，我们会给出一个例子。 path() 参数： kwargs 任意个关键字参数可以作为一个字典传递给目标视图函数。本教程中不会使用这一特性。 path() 参数： name 为你的 URL 取名能使你在 Django 的任意地方唯一地引用它，尤其是在模板中。这个有用的特性允许你只改一个文件就能全局地修改某个 URL 模式。 在上述配置中我们通过urlpatterns来关联映射关系，整体关系如下图所示： 客户端发起请求：http://localhost:8000/polls 项目接收到用户请求，url地址-&gt; polls/ 进入项目的映射文件urls.py中进行正则匹配：url(&#39;polls&#39;, include(&#39;polls/urls&#39;))，开始打开通过include包含的polls.urls子模块映射文件polls/urls.py 进入项目子模块polls的映射文件中进行匹配，由于polls/路径已经匹配通过，开始匹配polls/后面的路径-&gt; 路径为：’ ‘； 进入polls/urls.py中查找对应的路径-&gt;url(&#39;&#39;, view.index, name=&quot;index&quot;) 匹配到views.index，开始打开子模块的views.py视图文件，执行index视图方法 views.py中的视图方法index()返回了一条字符串，打印到我们浏览器的页面上]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django学习笔记 - 1 概述与安装]]></title>
    <url>%2F2018%2F06%2F17%2FDjango%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-%E6%A6%82%E8%BF%B0%E4%B8%8E%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[python程序web项目开发，是非常重要的一部分，Python为基础的web项目开发的框架有很多，django无疑是最强大web框架之一，也是我们必须掌握的框架之一 。 框架（framework），就是已经包含了项目结构和部分通用功能的自动化处理工具，主要用于进行项目的快捷和高效开发 Django是一个用 Python 编写的 Web 框架。Web 框架是一种软件，基于web框架可以开发动态网站，各种应用程序以及服务。它提供了一系列工具和功能，可以解决许多与Web开发相关的常见问题，比如：安全功能，数据库访问，会话，模板处理，URL路由，国际化，本地化，等等。 Django是用python编写的最流行的web框架之一。它绝对是最完整的，提供了各种各样的开箱即用的功能，比如用于开发和测试的独立Web服务器，缓存，中间件系统，ORM，模板引擎，表单处理，基于Python单元测试的工具接口。 Django还自带内部电池，提供内置应用程序，比如一个认证系统，一个可用于 CRUD(增删改查) 操作并且自动生成页面的后台管理界面，生成订阅文档（RSS/Atom）,站点地图等。甚至在django中内建了一个地理信息系统（GIS）框架。 1. 核心组件 Django被设计的核心组件主要包含： 对象关系映射（ORM）：以python类的形式定义数据模型，和数据库表关联 URL分配：使用正则表达式匹配URL，可以设计任意的URL没有特定限定，非常灵活 模板系统：强大并且可扩展的模板预言，分隔设计、内容和代码，并且可以继承 表单处理：可以方便生成各种表单模型，实现表单的有效性验证 解决和处理性能问题的缓存 2.Django能做什么 Django是一个python语言为基础的web框架 Django可以开发网站应用，如公司门户网站、学校官方网站、新闻动态网站、购物网站等等各种涉及小、中、大型的网站 Django可以用于开发各种B/S结构的系统平台项目，如公司内部数据管理平台、公司工作流程管理平台、内容管理系统平台、OA办公系统平台等等 总之，Diango是用来进行B/S结构的网络软件开发 C/S结构的软件： 主要是指client-server软件，也就是客户端服务器软件，这样的软件在使用的过程中，需要下载软件的安装包，安装到我们的个人PC上，然后打开联网使用，如QQ，DOTA2等等 B/S结构的软件： 主要是指browser-server软件，客户端只需要一个浏览器就可以，通过浏览器访问服务器上的数据，来完成软件的功能，如淘宝、京东类似的网站就是B/S结构的软件，我们在客户端PC上只需要打开浏览器访问指定的URL地址，就可以进行购物等功能的使用了。 3.安装 3.1 安装python3.6.5参考文档：Centos 7 编译安装Python3.6.5环境 ​ windwos环境下安装Python3.6.5环境 我们将使用windows环境下配合IDE来学习。 3.2 安装Virtualenv1pip3 install virtualenv 到目前为止，我们执行的安装都是在操作系统环境下运行的。从现在开始，我们安装的所有东西，包括django本身，都将安装在虚拟环境中。 这样想一下：对于你开始的每个Django项目，你首先会为它创建一个虚拟环境。这就像每个Django项目都有一个沙盒。所以你随意运行，安装软件包，卸载软件包而不会破坏任何东西。 先创建一个vote_poll目录： 12mkdir vote_pollcd vote_poll Cmder软件： 和cmd说拜拜 这个文件夹是级别较高的目录，将存储与我们的Django项目相关的所有文件和东西，包括它的虚拟环境。 所以让我们开始创建我们的第一个虚拟环境并安装django。 在vote_poll文件夹中创建虚拟环境并激活它： 12python -m venv vote_poll_venvvote_poll_venv\Scripts\activate.bat 如果你在命令行前面看到 (vote_poll_venv)，就代表激活成功了，就像这样： 让我们试着了解一下这里发生了什么。我们创建了一个名为venv的特殊文件夹。该文件夹内包含了一个python的副本。在我们激活了venv环境之后，当我们运行Python命令时，它将使用我们存储在venv里面的本地副本，而不是我们之前在操作系统中安装的那个。 另一个重要的事情是，pip程序也已经安装好了，当我们使用它来安装Python的软件包（比如Django）时，它将被安装在venv环境中。 请注意，当我们启用venv时，我们将使用命令python（而不是python3）来调用Python 3.6.2，并且仅使用pip（而不是pip3）来安装软件包。 顺便说一句，要想退出venv环境，运行下面的命令： 1deactivate 3.3 安装Django 2.0.6在虚拟环境中执行一下命令来安装django： 1pip install django 3.4 验证若要验证 Django 是否能被 Python 识别，可以在 shell 中输入 python。 然后在 Python 提示符下（必须在虚拟环境下），尝试导入 Django： 123&gt;&gt;&gt; import django&gt;&gt;&gt; print(django.get_version())2.0.6]]></content>
      <categories>
        <category>Django</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos 7 编译安装Python3.6.5环境]]></title>
    <url>%2F2018%2F06%2F17%2FCentos%207%20%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85Python3.6.5%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[一：编译安装123456789101112131415161718192021222324252627# 安装依赖包yum -y install gcc zlib zlib-devel sqlite-devel openssl-devel# 下载wget工具yum -y install wget# 下载源码包wget -c https://www.python.org/ftp/python/3.6.5/Python-3.6.5.tgz# 解压源码包tar zxf Python-3.6.5.tgz# 进入目录cd Python-3.6.5# 添加配置./configure \--prefix=/usr/python3.6 \ # 指定安装目录--with-ssl \ # 开启ssl协议--enable-shared # 启用构建共享python库# 编译安装make &amp;&amp; make install# 备份python2.7软连接mv /usr/bin/python /usr/bin/python.bak# 配置python3软连接，让系统默认使用Python3ln -s /usr/python3.6/bin/python3 /usr/bin/pythonln -s /usr/python3.6/bin/pip3 /usr/bin/pip# yum使用的是python2，替换python3之后导致无法正常工作，因此需要yum继续使用python2sed -i 's/python/python2/g' /usr/bin/yumsed -i 's/python/python2/g' /usr/libexec/urlgrabber-ext-down# 配置python3环境变量sed -i '$a PATH=$PATH:/usr/python/bin/' /etc/profile 注：如果系统提前没有安装openssl-devel，直接编译安装了python3，使用pip安装时会出现报错： pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available. 这时需要安装openssl-devel后重新配置Python3再编译安装才可使用pip安装： 1234yum -y install openssl-develcd Python-3.6.5./configure --prefix=/usr/python --with-sslmake &amp;&amp; make install 安装好python3后，运行出现下面错误： python3: error while loading shared libraries: libpython3.6m.so.1.0: cannot open shared object file: No such file or directory 解决方案：设置环境变量： 1export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/python/lib/ 注：其中/usr/python/为python3安装的目录 二：yum安装1234567# 安装epel源yum -y install epel-release# 安装python3.6yum -y install python36 python36-devel# 安装pipwget --no-check-certificate https://bootstrap.pypa.io/get-pip.pypython get-pip.py]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python学习笔记 - pip用法]]></title>
    <url>%2F2018%2F06%2F15%2FPython%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-pip%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Python学习笔记 - pip用法pip提供的子命令： 子命令 解释说明 install 安装软件包 download 下载软件包 uninstall 卸载软件包 freeze 按照requirements格式输出安装包，可以到其他服务器上执行pip install -r requirements.txt 直接安装软件 list 列出当前系统的安装包 show 查看安装包的信息，包括版本、依赖、许可证、作者、主页等信息 check 检查安装包的依赖是否完整 config 管理本地或全局配置文件 search 查找安装包 wheel 打包软件到whell格式 completion 生成命令补全配置 help 获取pip和子命令的帮助信息 下面以Flask为例，来看一下pip几个常用的子命令 1234567891011121314151617181920212223242526272829303132333435# 查找安装包：pip search flask# 安装特定的安装包版本：pip install flask==0.8# 删除安装包pip uninstall flask# 查看安装包的信息：pip show flask# 检查安装包的依赖是否完整：pip check flask# 查看已安装的安装包列表：pip list# 导出系统已安装的安装包列表到requirements文件pip freeze &gt; requirements.txt# 从requirements文件安装：pip install -r requirements.txt# 查看本地或全局配置：pip config list# 查看、修改、删除本地或全局配置：pip config set 'global.index-url' 'https://pypi.douban.com/simple/'pip config get 'global.index-url' pip config unset 'global.index-url'# 使用pip命令补全：pip completion --bash &gt;&gt; ~/.profilesource ~/.profile 使用豆瓣或阿里云的源加速软件安装123456789101112# 通过pip命令的-i选项指定镜像源即可，如下所示：pip install -i https://pypi.douban.com/simple/ flask# 每次都要指定镜像源的地址比较麻烦，可以修改Pip配置文件，将镜像源写入配置文件中。# 豆瓣源，可以换成其他的源pip config set 'global.index-url' 'https://pypi.douban.com/simple/'# 添加豆瓣源为可信主机，要不然可能报错pip config set 'trusted-host' 'pypi.douban.com'# 取消pip版本检查，排除每次都报最新的pippip config set 'disable-pip-version-check' 'true'# 设置超时时间pip config set 'timeout' '120' 注：pip源配置文件可以放置的位置： /etc/pip.conf ~/.pip/pip.conf ~/.config/pip/pip.conf(使用pip config set命令时会创建在此目录中) 将软件下载到本地部署12345# 下载到本地pip download -d 'pwd' -r requirements.txt # 本地安装pip install --no-index -f file://'pwd' -r requirements.txt ​ ​ ​ ​ ​ ​ ​]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
